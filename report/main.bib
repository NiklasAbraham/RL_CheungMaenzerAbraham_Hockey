% general repo https://github.com/NiklasAbraham/RL_CheungMaenzerAbraham_Hockey
@misc{rl_cheung_maenzer_abraham_hockey,
  author = {Ansel Cheung and Jannik Mänzer and Niklas Abraham},
  title = {RL-Course 2025/26: Final Project Report},
  year = {2026},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/NiklasAbraham/RL_CheungMaenzerAbraham_Hockey}},
}


@inproceedings{herbrich2006trueskill,
  author    = {Ralf Herbrich and Tom Minka and Thore Graepel},
  title     = {{TrueSkill\texttrademark}: A Bayesian Skill Rating System},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {19},
  year      = {2007},
  publisher = {MIT Press},
}

% Niklas Citations for Reinforcement Learning methods
@misc{hansen2024tdmpc2scalablerobustworld,
      title={TD-MPC2: Scalable, Robust World Models for Continuous Control}, 
      author={Nicklas Hansen and Hao Su and Xiaolong Wang},
      year={2024},
      eprint={2310.16828},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.16828}, 
}

@misc{williams2015modelpredictivepathintegral,
      title={Model Predictive Path Integral Control using Covariance Variable Importance Sampling}, 
      author={Grady Williams and Andrew Aldrich and Evangelos Theodorou},
      year={2015},
      eprint={1509.01149},
      archivePrefix={arXiv},
      primaryClass={cs.SY},
      url={https://arxiv.org/abs/1509.01149}, 
}

% Jannik
@article{AlphaStar,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Micha{\"e}l Mathieu and Andrew Joseph Dudzik and Junyoung Chung and David Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and L. Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander Sasha Vezhnevets and R{\'e}mi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom Le Paine and Caglar Gulcehre and Ziyun Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario W{\"u}nsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy P. Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
  journal={Nature},
  year={2019},
  volume={575},
  pages={350 - 354},
  url={https://api.semanticscholar.org/CorpusID:204972004}
}

@misc{sac,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05905}, 
}

@misc{td3,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.09477}, 
}

@misc{VAE,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1312.6114}, 
}

@inproceedings{pink,
  title = {Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning},
  author = {Eberhard, Onno and Hollenstein, Jakob and Pinneri, Cristina and Martius, Georg},
  booktitle = {Proceedings of the Eleventh International Conference on Learning Representations (ICLR 2023)},
  month = may,
  year = {2023},
  url = {https://openreview.net/forum?id=hQ9V5QN27eS}
}

% Hockey Environment
% https://github.com/martius-lab/hockey-env
@misc{hockeyenv_github,
  author = {Georg Martius},
  title = {Hockey Environment},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/martius-lab/hockey-env}},
}


@inproceedings{Hessel2018:Rainbow,
  title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
  author={Matteo Hessel and Joseph Modayil and H. V. Hasselt and T. Schaul and Georg Ostrovski and W. Dabney and Dan Horgan and B. Piot and Mohammad Gheshlaghi Azar and D. Silver},
  booktitle={AAAI},
  year={2018}
}

@InProceedings{wang2016:DDQN,
title = {Dueling Network Architectures for Deep Reinforcement Learning},
author = {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado Hasselt and Marc Lanctot and Nando Freitas},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
pages = {1995--2003},
year = 2016,
editor = {Maria Florina Balcan and Kilian Q. Weinberger},
volume = 48,
series = {Proceedings of Machine Learning Research},
address = {New York,
New York,
USA},
month = {20--22 Jun},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v48/wangf16.pdf},
url = {http://proceedings.mlr.press/v48/wangf16.html},
}

@Article{Sehnke2010:PGPE,
  Title                    = {Parameter-exploring policy gradients},
  Author                   = {Frank Sehnke and Christian Osendorfer and Thomas R{\"u}ckstie{\ss} and Alex Graves and Jan Peters and J{\"u}rgen Schmidhuber},
  Journal                  = {Neural Networks},
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {551-559},
  Volume                   = {23},
  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1016/j.neunet.2009.12.004},
}


@article{SchulmanEtAl2017:PPO,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
}

@Article{Peters08:NAC,
  Title                    = {Natural {A}ctor-{C}ritic},
  Author                   = {Peters, J. and Schaal, S.},
  Journal                  = {Neurocomputing},
  Year                     = {2008},
  Number                   = {7-9},
  Pages                    = {1180-1190},
  Volume                   = {71},

  Key                      = {reinforcement learning, policy gradient, natural actor-critic, natural gradients},
  Url                      = {http://www-clmc.usc.edu/publications//P/peters-NC2008.pdf}
}


@InProceedings{HaarnojaAbbeelLevine2018:SAC,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author = 	 {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}

@article{mnih2015humanlevel,
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}
