\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hockeyenv_github}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Environment Overview}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{1}{section.2}\protected@file@percent }
\newlabel{sec:Method}{{2}{1}{Method}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Self-Play Infrastructure - Jannik M채nzer}{1}{subsection.2.1}\protected@file@percent }
\newlabel{sec:selfplay_section}{{2.1}{1}{Self-Play Infrastructure - Jannik M채nzer}{subsection.2.1}{}}
\citation{herbrich2006trueskill}
\citation{AlphaStar}
\citation{hansen2024tdmpc2scalablerobustworld}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}TrueSkill Evaluation}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Opponent Selection}{2}{subsubsection.2.1.2}\protected@file@percent }
\citation{hansen2024tdmpc2scalablerobustworld}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}TD-MPC 2 - Niklas Abraham}{3}{subsection.2.2}\protected@file@percent }
\newlabel{sec:tdmpc2}{{2.2}{3}{TD-MPC 2 - Niklas Abraham}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces On the left in a) TD-MPC2 agent architecture and their individual components. b) shows the modified architecture with an additional opponent network to model the behavior of the adversarial agent in the Laser Hockey environment.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tdmpc2_flow}{{1}{3}{On the left in a) TD-MPC2 agent architecture and their individual components. b) shows the modified architecture with an additional opponent network to model the behavior of the adversarial agent in the Laser Hockey environment}{figure.caption.1}{}}
\citation{williams2015modelpredictivepathintegral}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Architecture and Training}{4}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Planning with MPPI}{4}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Modifying TD-MPC2: Opponent-Aware Dynamics}{4}{subsubsection.2.2.3}\protected@file@percent }
\citation{sac}
\citation{td3}
\citation{mnih2015humanlevel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}SAC - Jannik M채nzer}{5}{subsection.2.3}\protected@file@percent }
\newlabel{sec:sac}{{2.3}{5}{SAC - Jannik M채nzer}{subsection.2.3}{}}
\citation{VAE}
\citation{pink}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Automatic Entropy Tuning}{6}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Colored Action Noise}{6}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{eq:colored_noise}{{14}{6}{Colored Action Noise}{equation.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Experiments}{6}{subsubsection.2.3.3}\protected@file@percent }
\citation{td3}
\citation{td3}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {(top)} Cumulative reward and win rate across different exploration noise types. \textbf  {(bottom)} Q-value propagation over temporal distance to the goal in early vs. late training stages.}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:experiments}{{2}{7}{\textbf {(top)} Cumulative reward and win rate across different exploration noise types. \textbf {(bottom)} Q-value propagation over temporal distance to the goal in early vs. late training stages}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}TD3 and REDQ - Ansel Cheung}{7}{subsection.2.4}\protected@file@percent }
\newlabel{sec:td3}{{2.4}{7}{TD3 and REDQ - Ansel Cheung}{subsection.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}The Overestimation Problem}{7}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Twin Delayed Deep Deterministic Policy Gradient (TD3)}{7}{subsubsection.2.4.2}\protected@file@percent }
\citation{schaul2016prioritized}
\citation{chen2021randomized}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Prioritized Experience Replay (PER)}{8}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Randomized Ensembled Double Q-Learning (REDQ)}{9}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{9}{section.3}\protected@file@percent }
\newlabel{sec:Results}{{3}{9}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}TD-MPC2 Hyperparameters and Curriculum}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:tdmpc2_hyperparameters_and_curriculum}{{3.1}{9}{TD-MPC2 Hyperparameters and Curriculum}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces TrueSkill ratings of the TD-MPC2 agent with different horizons and different opponent aware dynamics.}}{9}{figure.caption.3}\protected@file@percent }
\newlabel{fig:tdmpc2_hyperparameters_and_curriculum}{{3}{9}{TrueSkill ratings of the TD-MPC2 agent with different horizons and different opponent aware dynamics}{figure.caption.3}{}}
\citation{schaul2016prioritized}
\citation{herbrich2006trueskill}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}REDQ-TD3, PER Curriculum}{10}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces TD3 vs REDQ rewards}}{10}{figure.4}\protected@file@percent }
\newlabel{fig:redq_td3_rewards}{{4}{10}{TD3 vs REDQ rewards}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces REDQ selfplay rewards trajectory}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:redq_selfplay}{{5}{10}{REDQ selfplay rewards trajectory}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Overall Results}{10}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces TrueSkill ratings ($\mu - 3\sigma $) for a selected set of agents (left) and win rates against the scripted weak and strong bots (right). Higher is better.}}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:agent_ratings_selected}{{6}{10}{TrueSkill ratings ($\mu - 3\sigma $) for a selected set of agents (left) and win rates against the scripted weak and strong bots (right). Higher is better}{figure.caption.4}{}}
\citation{rl_cheung_maenzer_abraham_hockey}
\@writefile{toc}{\contentsline {section}{\numberline {4}Acknowledgements \& Data Availability}{11}{section.4}\protected@file@percent }
\newlabel{sec:Acknowledgements}{{4}{11}{Acknowledgements \& Data Availability}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}AI Usage Disclosure}{11}{subsection.4.1}\protected@file@percent }
\bibstyle{abbrv}
\bibdata{main}
\bibcite{chen2021randomized}{1}
\bibcite{rl_cheung_maenzer_abraham_hockey}{2}
\bibcite{pink}{3}
\bibcite{td3}{4}
\bibcite{sac}{5}
\bibcite{hansen2024tdmpc2scalablerobustworld}{6}
\bibcite{herbrich2006trueskill}{7}
\bibcite{VAE}{8}
\bibcite{hockeyenv_github}{9}
\bibcite{mnih2015humanlevel}{10}
\bibcite{schaul2016prioritized}{11}
\bibcite{AlphaStar}{12}
\bibcite{williams2015modelpredictivepathintegral}{13}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{13}{appendix.A}\protected@file@percent }
\newlabel{sec:appendix}{{A}{13}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Episode Logs}{13}{subsection.A.1}\protected@file@percent }
\newlabel{sec:episode_logs}{{A.1}{13}{Episode Logs}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Sparse Reward Problem and Reward Backpropagation}{13}{subsection.A.2}\protected@file@percent }
\newlabel{sec:reward_backprop}{{A.2}{13}{Sparse Reward Problem and Reward Backpropagation}{subsection.A.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.1}Problem: Reward Stagnation at Zero}{13}{subsubsection.A.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.2}Solution: Reward Backpropagation}{13}{subsubsection.A.2.2}\protected@file@percent }
\newlabel{eq:backprop}{{22}{13}{Solution: Reward Backpropagation}{equation.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.3}Phase-Out During Training}{13}{subsubsection.A.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.4}Effect on TD-MPC2 Training}{14}{subsubsection.A.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Episode logs of the TD-MPC2 agent with the horizon 4 without the opponent aware dynamics.}}{15}{figure.caption.6}\protected@file@percent }
\newlabel{fig:episode_logs}{{7}{15}{Episode logs of the TD-MPC2 agent with the horizon 4 without the opponent aware dynamics}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Effect of reward backpropagation on winning episodes (bonus $b = 10$, discount $\gamma _b = 0.82$). \textbf  {Top:} per-step reward before and after backpropagation for a representative winning episode; the near-zero trace is replaced by a smoothly decaying bonus. \textbf  {Bottom:} scatter of episode total rewards before versus after backpropagation across all winning episodes; every point lies above the identity line, confirming a consistent upward shift in the training signal. }}{16}{figure.caption.7}\protected@file@percent }
\newlabel{fig:reward_backprop_analysis}{{8}{16}{Effect of reward backpropagation on winning episodes (bonus $b = 10$, discount $\gamma _b = 0.82$). \textbf {Top:} per-step reward before and after backpropagation for a representative winning episode; the near-zero trace is replaced by a smoothly decaying bonus. \textbf {Bottom:} scatter of episode total rewards before versus after backpropagation across all winning episodes; every point lies above the identity line, confirming a consistent upward shift in the training signal}{figure.caption.7}{}}
\gdef \@abspage@last{16}
