\documentclass[11pt]{article}

\usepackage{times}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}       % blackboard math symbols
\usepackage{amssymb}
\usetikzlibrary{arrows.meta,positioning,fit,backgrounds,shadows.blur,calc}

\usepackage{geometry}
\geometry{left=2.8cm,right=2.8cm,top=2.6cm,bottom=2.6cm}
\usepackage{fancyhdr}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\pagestyle{fancy}
\usepackage{hyperref}% should be the last package you include

\newcommand{\theteam}{}
\newcommand{\team}[1]{\def\theteam{#1}}


\fancyhead[L]{\theteam}
\fancyhead[R]{\thepage}
\cfoot{}

\setlength{\parindent}{0pt}

\team{Ansel Cheung, Jannik Mänzer, Niklas Abraham}
\title{RL-Course 2025/26: Final Project Report}
\author{\theteam}

\begin{document}
\maketitle

\begin{abstract}
    This report presents our application and comparative study of modern Reinforcement Learning (RL) algorithms specifically, Twin Delayed Deep Deterministic Policy Gradient (TD3), Soft Actor-Critic (SAC), and the model based TD-MPC2 within the challenging Laser Hockey environment. We discuss the environment's design, its state and action spaces, and the unique characteristics making it a compelling testbed for RL research. Our work details the methodological approaches taken with these algorithms, summarizes key experimental findings, and reflects on lessons learned in this multi agent, continuous control setting.
\end{abstract}

\section{Introduction}\label{sec:intro}

\subsection{Environment Overview}

The Laser Hockey environment \cite{hockeyenv_github} is a custom reinforcement learning benchmark built on the Gymnasium Python API and powered by the Box2D physics engine. In this multi agent setting, two agents each control a hockey stick and compete to score goals by striking a puck into the opponent's net. The environment is designed such that both agents receive identical but mirrored observations at each timestep, which eliminates the need for side specific learning strategies.

At each timestep, the agent receives an 18 dimensional continuous state vector that captures the complete game state. This observation includes the position $x_1, y_1$ and orientation $\theta_1$ of both players relative to the center of the field, along with their linear $v_{x,1}, v_{y,1}$ and angular $\omega_1$ velocities. The state also contains the puck's position $x_p, y_p$ and velocity $v_{x,p}, v_{y,p}$, as well as time remaining for each player's puck possession $t_{\mathrm{puck},1}, t_{\mathrm{puck},2}$ in keep mode, which ranges from 0 to 15 steps. Player 1 refers to the agent currently being controlled, while Player 2 represents the opponent. The observation structure ensures that when the viewpoint switches during training, the indices are automatically mirrored so that each agent always perceives itself as Player 1, maintaining a consistent learning perspective.

The action space consists of a 4 dimensional continuous vector that controls the agent's stick. The first two components specify forces applied for stick movement in the x and y directions, while the third component controls the torque applied to adjust the stick's angle. The fourth component is a thresholded scalar that determines whether to release the puck when the agent is in possession. Reward feedback in this environment is sparse and goal oriented. An agent receives a reward of +10 for scoring a goaland 0 for a draw. Additionally, there is a small reward signal for maintaining proximity to the puck, which helps guide exploration during early learning stages. Each episode begins with all entities positioned at the center of the field and terminates when a goal is scored or a timeout occurs. The environment supports both scripted opponents, such as the BasicOpponent, and learning agents as adversaries, making it a versatile and challenging benchmark for continuous control and multi agent reinforcement learning research.

\newpage
\section{Method}\label{sec:Method}

\subsection{Self-Play Infrastructure - Jannik Mänzer}

Training reinforcement learning agents in multi-agent environments like air hockey presents several challenges, including catastrophic forgetting and the difficulty of evaluating progress once standard baselines are consistently defeated. Relying solely on standard self-play often leads to policies that overfit to recent behaviors, while a constantly changing opponent makes true improvement hard to measure. To address these issues, our approach relies on a diverse archive of opponents, including periodic checkpoints of our training agents, deterministic baselines, and agents trained using different RL algorithms. Sampling opponents from this archive during training forces the agent to adapt to various playstyles while ensuring robustness against older strategies. At the same time, it provides a stable population of opponents to evaluate the agent's overall skill.

\subsubsection{TrueSkill Evaluation}

To compare different agents in the archive we utilize the TrueSkill rating system \cite{herbrich2006trueskill}. TrueSkill models an agent's skill as a Gaussian distribution with mean $\mu_i$ and variance $\sigma_i^2$. The probability that agent $i$ defeats agent $j$ is calculated as:

$$P(i \text{ beats } j) = \Phi\left(\frac{\mu_i - \mu_j}{\sqrt{2\beta^2 + \sigma_i^2 + \sigma_j^2}}\right),$$

where $\beta$ is the environment's inherent variance and $\Phi$ is the cumulative distribution function of the standard normal distribution. During training, these ratings are dynamically updated. This running rating serves as both the target for the skill-based matching and a continuous evaluation metric for the active agent. To prevent rating drift, the archive can be periodically recalibrated by executing comprehensive round-robin tournaments among the saved agents.

\subsubsection{Opponent Selection}

During training, opponents are selected using a mixture of different strategies: Prioritized Fictitious Self-Play (PFSP) \cite{AlphaStar} and skill-based matching ensure competetive matches, additionally random sampling and deterministic baselines are included to ensure fundamental competencies are maintained and older strategies are not forgotten. While skill-based matching and PFSP both aim to maximize learning signal by selecting opponents at a similar skill level, they differ in how that skill level is determined. Skill-based matching relies on a global skill rating to provide generally competitive games against opponents of similar strength. In contrast, PFSP tracks empirical, pairwise win rates, allowing it to also capture relative dynamics between different agents. We find PFSP to outperform skill-based matching in terms of sample efficiency.

For PFSP we track the empirical win rate $W_{i,j} \in [0, 1]$ of the active training agent $i$ against an archived opponent $j$ using an Exponential Moving Average (EMA). After each match, the win rate is updated as $W_{i,j} \leftarrow (1 - \alpha) W_{i,j} + \alpha r$, where $\alpha$ is a smoothing factor and $r$ is the current result, with $r \in \{0, 0.5, 1\}$ for a loss, draw, or win respectively.  

The probability of selecting opponent $j$ from the available archive pool $\mathcal{A}$ is calculated by normalizing a priority function $f$:

$$P(\text{select } j) = \frac{f(W_{i,j})}{\sum_{k \in \mathcal{A}} f(W_{i,k})}.$$

For our priority function, we utilize $f(x) = x(1 - x)$. This parabola peaks exactly at a 50\% win rate and approaches zero as the win rate nears 0\% or 100\%. Consequently, the matchmaker actively selects the opponents that provide the most informative learning signal. If the active agent completely masters an opponent, its selection priority naturally diminishes. Conversely, if the agent experiences catastrophic forgetting and begins losing to an older opponent, the empirical win rate decays back toward $0.5$, automatically pulling that opponent back into the active training curriculum until a counter-strategy is successfully relearned.

\subsection{TD-MPC 2 - Niklas Abraham}\label{sec:tdmpc2}

TD-MPC2 \cite{hansen2024tdmpc2scalablerobustworld} is a model-based reinforcement learning algorithm that learns a world model to predict future states and rewards, and uses this model to select actions through planning. The core idea is to learn a policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ in a Markov Decision Process with an infinite horizon. The policy is constructed to maximize the expected discounted return. In TD-MPC2, this is achieved by learning a world model and selecting actions by planning with the learned models.

For planning, TD-MPC2 employs the Model Predictive Control (MPC) framework, in which actions are optimized based on planning over action sequences of a finite horizon $H$:
\begin{equation}
    \pi(s_t) = \arg\max_{a_1, \ldots, a_H} \mathbb{E}_\pi\left[\sum_{\tau=0}^{H} \gamma^\tau r(s_{t+\tau}, a_{t+\tau})\right].
\end{equation}
The return of each trajectory is estimated by simulating action sequences through the learned world model. However, this approach often leads to only locally optimal policies. To address this limitation, TD-MPC2 additionally utilizes a value function to guide the planning process and improve the policy toward a more globally optimal solution.

Rather than predicting raw future observation states, TD-MPC2 learns to predict a maximally useful latent representation for accurately estimating the outcomes of action sequences. The algorithm is composed of five distinct neural network components that interact in a coordinated manner, as illustrated in Figure~\ref{fig:tdmpc2_flow} a).
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/tdmpc2_flow.pdf}
    \caption{On the left in a) TD-MPC2 agent architecture and their individual components. b) shows the modified architecture with an additional opponent network to model the behavior of the adversarial agent in the Laser Hockey environment.}
    \label{fig:tdmpc2_flow}
\end{figure}
\begin{itemize}
    \item \textbf{Encoder:} Maps the observed state $s$ to a 512-dimensional latent vector $\vec{z}=h(s)$.
    \item \textbf{Latent Dynamics:} Predicts the next latent $\vec{z}_{t+1}$ from current latent and action: $\vec{z}_{t+1} = d(\vec{z}_t, a)$.
    \item \textbf{Reward Head:} Estimates reward $r$ for a given $(\vec{z}, a)$ pair: $r = R(\vec{z}, a)$.
    \item \textbf{Termination Head:} Predicts early episode end, e.g., when a goal is imminent.
    \item \textbf{Q-Network Ensemble:} An ensemble (5 networks) of Q-functions estimating value $q = Q(\vec{z}, a)$. The minimum of two sampled networks reduces value overestimation.
    \item \textbf{Policy Network:} Guides action selection in planning: $p(\vec{z}, a) \to \hat{a}$.
\end{itemize}

\subsubsection{Architecture and Training}

All network components are multi-layer perceptrons (MLPs) with Mish activations. As in \cite{hansen2024tdmpc2scalablerobustworld}, the latent representation $\vec{z}$ is projected into $L$-dimensional simplices via a softmax to stabilize training and enforce sparsity.

Training uses an experience replay buffer $\mathcal{B}$ with full episode trajectories. Model parameters are optimized over sampled subsequences of length $H{+}1$ from $\mathcal{B}$ by minimizing a joint loss for dynamics, reward, and value prediction:
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{(s_t,a_t,r_t,s_{t+1})_{t=0}^{H} \sim \mathcal{B}}\left[\sum_{t=0}^{H} \lambda^t \left(\|\vec{z}_{t+1} - \text{sg}(h(s_{t+1}))\|_2^2 + \text{CE}(\hat{r}_t, r_t) + \text{CE}(\hat{q}_t, q_t)\right)\right],
\end{equation}
where $\text{sg}(\cdot)$ is stop-gradient, $\vec{z}_{t+1} = d(\vec{z}_t, a_t)$ is the predicted next latent, $\hat{r}_t = R(\vec{z}_t, a_t)$, $\hat{q}_t = Q(\vec{z}_t, a_t)$, and $\lambda$ is a temporal discount factor. The Q-value target is $q_t = r_t + \gamma \bar{Q}(\vec{z}_{t+1}, p(\vec{z}_{t+1}, a_{t+1}))$, using an EMA of Q-net parameters ($\bar{Q}$) for stability. Following TD-MPC2, reward and value predictions are regressed in a log-transformed space with cross-entropy loss and soft targets.

The policy $p$ is optimized according to a maximum entropy RL objective:
\begin{equation}
    \mathcal{L}_p(\theta) = \mathbb{E}_{(s_t,a_t)_{t=0}^{H} \sim \mathcal{B}}\left[\sum_{t=0}^{H} \lambda^t \left(\alpha Q(\vec{z}_t, p(\vec{z}_t, a_t)) - \beta \mathcal{H}(p(\cdot|\vec{z}_t))\right)\right],
\end{equation}
where $\vec{z}_{t+1} = d(\vec{z}_t, a_t)$ with $\vec{z}_0 = h(s_0)$, and $\mathcal{H}(p(\cdot|\vec{z}_t))$ is the policy entropy. Hyperparameters $\alpha$ and $\beta$ balance value maximization and entropy, preventing premature collapse to deterministic policies.

\subsubsection{Planning with MPPI}

For local planning, TD-MPC2 leverages Model Predictive Path Integral (MPPI) control \cite{williams2015modelpredictivepathintegral}, sampling action sequences with guidance from the policy network. At each step, it estimates $\mu^*, \sigma^* \in \mathbb{R}^{H \times m}$, the mean and standard deviation of a multivariate Gaussian that maximizes expected return:
\begin{equation}
    \mu^*, \sigma^* = \arg\max_{\mu, \sigma} \mathbb{E}_{a_{t:t+H} \sim \mathcal{N}(\mu, \sigma^2)}\left[\gamma^H Q(\vec{z}_{t+H}, a_{t+H}) + \sum_{\tau=t}^{H} \gamma^\tau R(\vec{z}_\tau, a_\tau)\right].
\end{equation}
This is optimized by iteratively sampling actions from $\mathcal{N}(\mu, \sigma^2)$, evaluating their returns, and updating $\mu$ and $\sigma$ based on weighted top samples. The termination model predicts early ends in sampled rollouts. To speed up convergence, a fraction of samples comes from the policy $p$, and $\mu$, $\sigma$ are initialized from the previous step.

\subsubsection{Modifying TD-MPC2: Opponent-Aware Dynamics}

In the classical TD-MPC2, the dynamics model is trained to predict the next latent state given the current latent state and action. However, in the multi agent setting, with an adversarial opponent, the dynamics model will only receive the current latent state and action, but not receive the action of the opponent. Thus the standard TD-MPC2 implicitly models
\begin{equation}
P(s_{t+1} \mid s_t, a_t^{\text{self}})
=
\mathbb{E}_{a_t^{\text{opp}} \sim \pi_{\text{opp}}(\cdot \mid s_t)}
P(s_{t+1} \mid s_t, a_t^{\text{self}}, a_t^{\text{opp}}),
\end{equation}
which corresponds to marginalizing over opponent behavior.
This leads to a mean-opponent model, producing biased long-horizon predictions
when the opponent policy is multimodal or strategic. To address this, it is not enough to vary the opponents during training or to use self-play, the network architecture needs to be changed.

To model the opponent's behavior, we extend the dynamics model so that it takes the opponent action as an additional input: $\vec{z}_{t+1} = d(\vec{z}_t, a_t^{\text{self}}, a_t^{\text{opp}})$, where $a_t^{\text{opp}}$ is the opponent's action. The same is done for the reward model and Q-value network.

With opponent-aware models, evaluating a candidate self-action sequence $a_t^{\text{self}}, \ldots, a_{t+H-1}^{\text{self}}$ uses the following planning objective and rollout. The return is
\begin{equation}
\sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} R(\vec{z}_\tau, a_\tau^{\text{self}}, \hat{a}_\tau^{\text{opp}}) + \gamma^H \tilde{V}(\vec{z}_{t+H}),
\end{equation}
where the latent trajectory and predicted opponent actions are obtained by the recursion
\begin{equation}
\hat{a}_\tau^{\text{opp}} = \pi_{\text{opp}}(\vec{z}_\tau), \qquad \vec{z}_{\tau+1} = d(\vec{z}_\tau, a_\tau^{\text{self}}, \hat{a}_\tau^{\text{opp}}),
\end{equation}
for $\tau = t, \ldots, t+H-1$, with $\vec{z}_t = h(s_t)$. The terminal value $\tilde{V}(\vec{z}_{t+H})$ is given by the Q-ensemble (e.g., $\min_k Q_k(\vec{z}_{t+H}, a)$ at the policy action $a$). MPPI then maximizes this return over sampled self-action sequences, with opponent actions fixed by the recursion above.

The action of the opponent needs to be predicted with a separate network, which receives as input the current latent state and outputs the action of the opponent. This is illustrated in Figure~\ref{fig:tdmpc2_flow} b). This requires the opponent network to be trained separately to imitate the opponent's behavior; the opponent's actions are available in the replay buffer from collected episodes. In the setting in this project, we could choose between different opponents: basic weak, basic strong, the TD3 agent \cite{sec:td3} or the SAC agent \cite{sec:sac} as well as the TD-MPC2 agent without the opponent-aware dynamics. During data collection we control both policies (self and opponent), therefore $a_t^{\text{opp}}$ is logged exactly in the replay buffer. The separate loss is given by
\begin{equation}
\mathcal{L}_{\text{opp}} = \| a_t^{\text{opp}} - \pi_{\text{opp}}(\vec{z}_t) \|^2,
\end{equation}
where $\pi_{\text{opp}}$ is the opponent network, trained with a frozen encoder in periodic intervals. With this MSE objective, $\pi_{\text{opp}}$ is a deterministic mean predictor: it outputs a single action estimate per latent state.

During training, opponent actions are taken from the replay buffer; during inference they must be predicted by the opponent network. We use the opponent action deterministically: $\hat{a}_t^{\text{opp}} = \pi_{\text{opp}}(\vec{z}_t)$. This predicted action is fed into the dynamics to obtain the next latent state (Figure~\ref{fig:tdmpc2_flow} b). When multiple opponent models are used (e.g., different cloned policies), each rollout can be assigned a fixed opponent model for the full horizon; diversity across rollouts then comes from varying which opponent model is used, not from sampling different actions from a stochastic $\pi_{\text{opp}}$.

\subsection{SAC - Jannik Mänzer}

Soft Actor-Critic (SAC) \cite{sac} is an off-policy actor-critic algorithm that extends standard reinforcement learning by optimizing a maximum entropy objective. Rather than seeking only the maximum cumulative reward, the agent aims to maximize a weighted objective of reward and the entropy of the policy $\mathcal{H}(\pi(\cdot|\mathbf{s}_t))$ over trajectories $\tau = (\mathbf{s}_0, \mathbf{a}_0, \mathbf{s}_1, \mathbf{a}_1, \ldots)$ generated by the policy $\pi$:
\begin{equation}
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{\tau \sim \pi} [r(\mathbf{s}_t, \mathbf{a}_t) + \alpha \mathcal{H}(\pi(\cdot|\mathbf{s}_t))].
\end{equation}
This entropy term $\mathcal{H}$ encourages the policy to assign non-zero probability to multiple actions where optimal, preventing premature convergence to deterministic behavior and improving exploration. The temperature parameter $\alpha$ controls the trade-off between the reward and the entropy.

Two soft Q-functions, $Q_1$ and $Q_2$, are trained to estimate the expected return plus the future entropy of the policy. To mitigate overestimation, Clipped Double-Q Learning \cite{td3} is used, where the target is calculated using the minimum of two target networks $\bar{Q}_{1,2}$, whose weights are obtained via an exponential moving average of the main Q-networks \cite{mnih2015humanlevel}. The target for the Q-function update is given by:
\begin{equation}
y_t = r(\mathbf{s}_t, \mathbf{a}_t) + \gamma \mathbb{E}_{\mathbf{a}_{t+1} \sim \pi} \left[ \min_{j=1,2} \bar{Q}_j(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}) - \alpha \log \pi(\mathbf{a}_{t+1}|\mathbf{s}_{t+1}) \right].
\end{equation}
The parameters $\theta$ are updated by minimizing the squared error between the prediction and this entropy-augmented target:
\begin{equation}
J_Q(\theta) = \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t) \sim \mathcal{D}} \left[ \frac{1}{2}\left( Q_i(\mathbf{s}_t, \mathbf{a}_t) - y_t \right)^2 \right] \quad \text{for } i \in \{1, 2\}.
\end{equation}

The policy $\pi_\phi$ is updated to maximize the value estimate provided by the Q-functions while maintaining high entropy. 
Instead of directly sampling actions from the stochastic policy $a_t \sim \pi_\phi(\cdot|s_t)$, the reparameterization trick \cite{VAE} allows to express the action as a deterministic function of an independent noise source $\epsilon_t \sim \mathcal{N}(0, I)$ and the state: $\mathbf{a}_t = f_\phi(\epsilon_t; \mathbf{s}_t) = \mu_\phi(\mathbf{s}_t) + \sigma_\phi(\mathbf{s}_t) \odot \epsilon_t$, effectively moving the stochasticity outside the network, thus allowing for backpropagation. The policy objective is then to maximize:
\begin{equation}
J_\pi(\phi) = \mathbb{E}_{\mathbf{s}_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}} \left[ \min_{j=1,2} Q_j(\mathbf{s}_t, f_\phi(\epsilon_t; \mathbf{s}_t)) - \alpha \log \pi_\phi(f_\phi(\epsilon_t; \mathbf{s}_t)|\mathbf{s}_t) \right].
\end{equation}

\subsubsection{Automatic Entropy Tuning}

Instead of choosing the temperature hyperparameter $\alpha$ manually, it can be tuned automatically to ensure the policy satisfies a minimum target entropy constraint $\bar{\mathcal{H}}$ (typically chosen as $-\dim(\mathcal{A})$, where $\dim(\mathcal{A})$ is the dimensionality of the action space). This continuously adapts the "exploration pressure" during training:
\begin{equation}
J(\alpha) = \mathbb{E}_{\mathbf{a}_t \sim \pi_t} [-\alpha (\log \pi_t(\mathbf{a}_t|\mathbf{s}_t) + \bar{\mathcal{H}})].
\end{equation}

\subsubsection{Colored Action Noise}

Standard SAC samples the reparameterization noise $\epsilon_t$ from an independent Gaussian distribution (white noise). While this uncorrelated sampling, characterized by a flat power spectral density ($S(f) \propto 1$), is standard practice, it may overly restrict the agent to local exploration. Because the noise fluctuates independently at each timestep, the agent rarely commits to a single direction for an extended period, which can hinder comprehensive state-space coverage.

To facilitate broader exploration, temporally correlated noise can replace the independent samples $\epsilon_t$. This noise is defined by a power spectral density inversely proportional to frequency:
\begin{equation}
    S(f) \propto \frac{1}{f^\beta}
    \label{eq:colored_noise}
\end{equation}
where $\beta$ dictates the noise color. Eberhard et al.\ \cite{pink} therefore propose using pink noise ($\beta = 1$) as a sensible default to balance local and global exploration.

\subsubsection{Design Choices}

\subsection{TD3 and REDQ - Ansel Cheung}\label{sec:td3}

\subsubsection{The Overestimation Problem}

In Standard Actor-Critic methods with continuous action spaces, the critic is responsible for approximating the Q function $Q_\theta(s, a)$. This means that they can overestimate or underestimate the true value \cite{fujimoto2018addressing}. Pairing with the actor, who is responsible for maximizing the critic's expected return, is sensitive to the critic's over/underestimation. However, the overestimation of value will cause the Actor to udpate its parameters to select the overestimated action. This in turn causes the critic to use this artificially inflated Q-value from the next state, and ultimately pulls the current Q-value upwards. This cyclical overestimation of actions and Q-value backpropagates and leads to divergent behavior or suboptimal policies in continuous control over a long training timeframe. The predicted Q-values completely deviate from the actual expected returns.

\subsubsection{Twin Delayed Deep Deterministic Policy Gradient (TD3)} 
TD3 \cite{fujimoto2018addressing} aims to fix the overestimation problem in Standard Actor-Critic methods using 3 tricks. The first trick of TD3 is the \textbf{Clipped Double-Q Learning}. TD3 maintains two independent critics, $Q_{\theta_1}$ and $Q_{\theta_2}$, and uses the minimum value to calculate the Bellman target:
    \begin{equation}
        y = r + \gamma \min_{i=1,2} Q_{\theta_{i, targ}}(s', \tilde{a})
    \end{equation}

These 2 Critics have the exact same architecture, but 2 independent sets of weights which have different (random) initializations. They start at different points of the loss landscape and are updated by stochastic gradient descent. This allows the 2 networks to have different and more varied Q-values. The min operator is used when predicting the Q-value. This acts as a pessimistic bound that should disrupt the aforementioned overestimation of Critics. The Actor is then given a much more conservative Q-value estimation. Underestimation might occur but this is much better than overestimation, since the Actor is already maximizing the expected critic's return, and will avoid picking an underestimated action. This means that underestimation errors do not propagate as dangerously as overestimation errors.

The second trick of TD3 is \textbf{Target Policy Smoothing}. In continuous action spaces, the Critics do not have a smooth Q-value landscape, it is usually laced with random sharp peaks. These peaks causes the Critics to overestimate the Q-value and the Actor exploits these peaks. Hence, TD3 injects small random noise into the Actor's predicted action when updating the Critics.
    \begin{equation}
        \tilde{a} = \text{clip}(\pi_{\phi_{targ}}(s') + \epsilon, a_{low}, a_{high}), \quad \epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)
    \end{equation}

Clipping is performed to ensure the noisy action does not exceed the envrironment's valid action minimum and maximum. Noise allows the Critics to evaluate the possibly exploited Actor's action off the peak, which in the case where it is a sharp peak, means that a little noise will bring the Critic to a much flatter part of the landscape. The Actor, similarly cannot rely on sharp pinpoints of overestimation and will learn to maximize actions in broad and stable regions of high Q-values.

The third and final trick of TD3 is \textbf{Delayed Policy Updates}. Instead of updating the Actor at every step, the Actor $\pi_\phi$ is updated less frequently than the critics (e.g., a $d=2$ delay). During training, especially the early parts, the Critics are highly inaccurate. Updating the Actor at every step drags down both the Critic and the Actors since the Actor updates its policy based on an unstable Critic, which updates its policy using the half-baked Actor. The Actor is trying to converge towards a target which is unstable. Delaying the updates to the Actor allows the Critics' Q-value landscape to converge and settle, before updating the Actor to a more stable target. 

\subsubsection{Prioritized Experience Replay (PER)}

In a normal buffer, the agent stores its experiences and samples them with a uniform distribution. All samples are equally likely to be sampled. However, as the training progresses, the agent sees more and more of the same kind of experiences due to uniform sampling. The agent would learn a lot faster if it sees catastrophic failures and surprises/interesting experiences \cite{schaul2016prioritized}. We can measure this surprise factor with TD error. The TD error ($\delta$) is the difference between the target value ($y$) and the Critic's current Q-value prediction:
\begin{equation}
    \delta_i = y_i - Q(s_i, a_i)
\end{equation}
A low TD error near zero means that the Critic has predicted the Q-value perfectly, and there is nothing much to learn. A high absolute TD error ($|\delta_i|$) means that the Critic has made a wildly wrong prediction. These are the experiences we want to sample more often. 

The priority $p_i$ of transition $i$ is defined by its TD error $|\delta_i|$. The sampling probability is given by:
\begin{equation}
    P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha} \quad \text{where }p_i = |\delta_i| + \epsilon
\end{equation}

$\epsilon$ is just a small constant to ensure the probabilities do not ever reach 0. $\alpha$ is the tunable hyperparameter of how much prioritization is used, $\alpha=0$ for uniform sampling, $\alpha=1$ for full prioritization of TD error.

The data distribution that the model sees is being altered by changing the sampling and this introduces bias. These bias affect the expected values calculated by neural networks and might intefere with the gradients. To correct the bias introduced by frequent sampling of high transitions with high TD errors, the loss is weighted by $w_i$:
\begin{equation}
    w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta
\end{equation}
where $\beta$ is annealed from $\beta_0 \to 1$ over the duration of training. This scaling with importance weights scales down the gradients for transitions with high probability of being sampled.

The implementation for PER utilizes a SumTree data structure to reduce the sampling and updating of priorities in $\mathcal{O}(\log n)$ time. 


\subsubsection{Randomized Ensembled Double Q-Learning (REDQ)}

REDQ \cite{chen2021randomized} can be used to take TD3 to the next level. If we can use 2 Critics in TD3, why not 10? REDQ also has a few tricks.

Trick 1: \textbf{Sample Efficiency via Update-toData (UTD)}. REDQ aims for a high Update-to-Data (UTD) ratio $G \gg 1$, allowing the agent to perform many gradient steps per environment interaction. In the paper, a UTD ratio of 20 was used. However, using. high UTD ratio on vanilla TD3 will cause the overestimation problem to return. The Actor and Critics will be learning from the same small batch of transitions aggressively, causing approximation errors to compound. 

Trick 2: \textbf{Ensemble Minimum}. In order to continue to fix the overestimation problem since a high UTD is used, REDQ uses an even more pessimistic estimation. REDQ maintains an ensemble of $N$ Q-functions (Critics). For each update, using a minimum of all Critics might cause large overestimation bias. Instead, a random subset $\mathcal{M}$ of size $M$ is sampled to compute the target:
    \begin{equation}
        y = r + \gamma \left( \min_{j \in \mathcal{M}} Q_{\theta_{j, targ}}(s', a') - \alpha \log \pi_\phi(a'|s') \right)
    \end{equation}

This single target is used to update all $N$ critics. Using $N$ Critics with different initializations allows the approximation error to be even more uncorelated compared to TD3 with 2 Critics. The pessimisim is further intensified, and is more dynamic, by using the minimum of a random subset of $M$ critics.

Trick 3: \textbf{Policy Update}. The actor is updated to maximize the mean Q-value across all $N$ Critics. The mean of all $N$ Critics provides a much smoother and reliable signal for the Actor to follow.
    \begin{equation}
        \nabla_\phi J(\phi) = \nabla_\phi \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^N Q_{\theta_i}(s, \pi_\phi(s)) - \alpha \log \pi_\phi(a|s) \right]
    \end{equation}

\section{Results}\label{sec:Results}

\subsection{TD-MPC2 Hyperparameters and Curriculum}\label{sec:tdmpc2_hyperparameters_and_curriculum}

To determine the optimal horizon for TD-MPC2, we trained a TD-MPC2 agent with different horizons and evaluated the performance. The horizons tested were 4, 6, 8, 10, and 12. The runs were done with the following hyperparameters: learning rate 0.0003, batch size 512, network size of three layers with 256 units each, a latent dimension of 256, 5 Q-networks, a gamma of 0.99, a temperature of 0.5, a vmin of -10, a vmax of 10, a win reward bonus of 10, and a win reward discount of 0.92. The runs were done with the following curriculum: 4000 episodes of full competency, with a basic strong opponent. The results are shown in Figure~\ref{fig:tdmpc2_hyperparameters_and_curriculum}. Addtionally with the same hyperparameters, the modified opponent aware dynamics was tested, for this three internal opponent models were used: the TD-MPC2 agent without the opponent-aware dynamics, the SAC agent and the DECOYPOLICY agent, which was trained to mimick the basic strong opponent. The results are shown in the same figure, but with the label "opponent aware dynamics".

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/horizon_ratings.png}
    \caption{TrueSkill ratings of the TD-MPC2 agent with different horizons and different opponent aware dynamics.}
    \label{fig:tdmpc2_hyperparameters_and_curriculum}
\end{figure}

\subsection{REDQ-TD3, PER Curriculum}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/redq_td3_rewards.png}
    \caption{Comparison of TD3 and REDQ with and without PER.}
    \label{fig:redq_td3_per}
\end{figure}

We used the default paper hyperparameters for REDQ and TD3. Figure~\ref{fig:redq_td3_per} shows the comparison of REDQ, TD3 with and without PER. PER seems to degrade performance of both REDQ and TD3, while REDQ consistently improves the performance of TD3. PER was designed for discrete action spaces \cite{schaul2016prioritized} while air hockey environment is a continuous action space. Retuning of hyperparameters is needed to make PER work. We attempted to tune the hyperparameters but ultimately was unable to achieve a good set of hyperparameters for REDQ/TD3 for 2 reasons: (1) PER increases training time from $\mathcal{O}(1)$ to $\mathcal{O}(\log n)$ and (2) REDQ increases training time massively due to having 10 Critics and high UTD ratio. Ultimately we decided to continue with REDQ-TD3 with PER. 

\subsubsection{Overall Results}

To compare all trained agents on a common scale, we evaluated them in a round-robin tournament within the archive matchmaking system using TrueSkill \cite{herbrich2006trueskill}, a Bayesian rating algorithm that updates the belief over each agent's skill level after every match outcome. The rating reported is $\mu - 3\sigma$, a conservative lower-bound estimate that accounts for residual uncertainty. Figure~\ref{fig:agent_ratings_selected} shows the TrueSkill ratings for a representative selection of agents: the two scripted baselines (weak and strong bot), the SAC agent, and the two best-performing TD-MPC2 variants -- one trained with internal opponent modelling at planning horizon $H{=}8$ and one at $H{=}6$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/agent_ratings_selected.png}
    \caption{TrueSkill ratings (mean $\pm$ $3\sigma$) for a selected set of agents. Higher is better.}
    \label{fig:agent_ratings_selected}
\end{figure}

All learned agents clearly surpass both scripted baselines. The basic strong bot achieves the lowest rating ($15.76$), which is counterintuitive given its name but reflects the fact that its deterministic, aggressive strategy is highly exploitable by learned policies; it was not designed as a competitive opponent against gradient-based agents. The basic weak bot achieves a higher rating ($25.24$) because its more conservative behavior generates fewer opportunities for the opponents to score, making it harder to accumulate decisive wins against.

Among the learned agents, SAC reaches a rating of $28.34$, demonstrating that a well-tuned model-free actor-critic is already a strong baseline in this environment. The TD-MPC2 agent trained with internal opponent modelling at $H{=}6$ achieves a comparable rating of $28.53$, while the variant at $H{=}8$ reaches $31.41$, the highest rating overall. The gain from $H{=}6$ to $H{=}8$ suggests that a longer planning horizon provides a meaningful advantage, allowing the agent to anticipate multi-step game dynamics more accurately. The overlap in confidence intervals between SAC and TD-MPC2 at $H{=}6$ indicates that, at this horizon, the model-based approach does not yet offer a statistically significant benefit over the model-free baseline. The TD-MPC2 variant at $H{=}8$ with more training steps (16\,000 vs.\ 4\,000) does, however, emerge as the clearly strongest agent.


\section{Acknowledgements \& Data Availability}\label{sec:Acknowledgements}
We would like to thank the instructors and the staff of the Reinforcement Learning course for their help and support.
All of our code can be found on our GitHub repository \cite{rl_cheung_maenzer_abraham_hockey}.

\newpage

\bibliographystyle{abbrv}
\bibliography{main}

% appendix
\appendix
\section{Appendix}\label{sec:appendix}

\subsection{Episode Logs}\label{sec:episode_logs}
The episode logs of the TD-MPC2 agent with the horizon 4 without the opponent aware dynamics are shown in Figure~\ref{fig:episode_logs}. These logs were plotted fro all runs periodically, and this example is representative for the other runs.
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/episode_logs.png}
    \caption{Episode logs of the TD-MPC2 agent with the horizon 4 without the opponent aware dynamics.}
    \label{fig:episode_logs}
\end{figure*}

\end{document}
