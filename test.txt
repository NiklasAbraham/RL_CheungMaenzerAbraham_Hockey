diff --git a/.gitattributes b/.gitattributes
deleted file mode 100644
index aabd017..0000000
--- a/.gitattributes
+++ /dev/null
@@ -1,6 +0,0 @@
-# Force LF (Linux style) for bash and sbatch scripts
-*.sh      text eol=lf
-*.sbatch  text eol=lf
-
-# Optional: Keep everything else as "auto" (Git decides based on OS)
-* text=auto
diff --git a/.gitignore b/.gitignore
index 3bdef11..9b2ec12 100644
--- a/.gitignore
+++ b/.gitignore
@@ -10,7 +10,6 @@ __pycache__/
 
 Output/
 data/
-old/
 old_reports/
 results/
 
@@ -68,9 +67,6 @@ cover/
 *.mo
 *.pot
 
-src/rl_hockey/scripts/figures/
-
-
 # Django stuff:
 *.log
 local_settings.py
@@ -221,6 +217,3 @@ cython_debug/
 marimo/_static/
 marimo/_lsp/
 __marimo__/
-
-# Reward analysis figures
-src/rl_hockey/scripts/figures/
diff --git a/cluster_setup.sh b/cluster_setup.sh
new file mode 100755
index 0000000..92b5806
--- /dev/null
+++ b/cluster_setup.sh
@@ -0,0 +1,88 @@
+#!/bin/bash
+# Setup script to run on the TCML cluster after syncing files
+# Run this script on the cluster: bash cluster_setup.sh
+
+set -e
+
+PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
+cd "$PROJECT_DIR"
+
+echo "Setting up RL Hockey project on TCML cluster..."
+echo ""
+
+# Create singularity build directory
+echo "Creating singularity build directory..."
+mkdir -p singularity_build
+cd singularity_build
+
+# Copy container definition
+echo "Copying container definition..."
+cp ../resources/container/container.def ./rl_hockey.def
+
+# Embed requirements.txt directly into container.def by replacing the placeholder
+echo "Embedding requirements.txt into container definition..."
+python3 << 'PYEOF'
+import sys
+import re
+
+# Read the container definition
+with open('rl_hockey.def', 'r') as f:
+    content = f.read()
+
+# Read requirements
+with open('../requirements.txt', 'r') as f:
+    requirements = f.read()
+
+# Find and replace the heredoc section
+# Pattern: everything between '# Requirements will be embedded...' and 'REQUIREMENTS_EMBEDDED'
+pattern = r'(# Requirements will be embedded here by cluster_setup.sh\n)(.*?)(REQUIREMENTS_EMBEDDED)'
+
+def replace_func(match):
+    return match.group(1) + requirements + '\n' + match.group(3)
+
+new_content = re.sub(pattern, replace_func, content, flags=re.DOTALL)
+
+if new_content != content:
+    # Write back
+    with open('rl_hockey.def', 'w') as f:
+        f.write(new_content)
+    print(f"✓ Successfully embedded requirements.txt ({len(requirements.split(chr(10)))} lines)")
+else:
+    print("ERROR: Could not find placeholder in container.def")
+    sys.exit(1)
+PYEOF
+
+# Verify files exist before building
+echo ""
+echo "Verifying files are in place for build:"
+if [ ! -f "./rl_hockey.def" ]; then
+    echo "ERROR: rl_hockey.def not found!"
+    exit 1
+fi
+echo "✓ rl_hockey.def found"
+
+if [ ! -f "./requirements.txt" ]; then
+    echo "ERROR: requirements.txt not found in build directory!"
+    exit 1
+fi
+echo "✓ requirements.txt found ($(wc -l < ./requirements.txt) lines)"
+
+# Show that requirements.txt is in the same directory as the .def file
+echo "Files in build directory:"
+ls -lh *.def *.txt 2>/dev/null || ls -lh
+
+# Build the container
+echo ""
+echo "Building Singularity container (this may take 10-30 minutes)..."
+echo "This will install all packages from requirements.txt into the container."
+singularity build --fakeroot rl_hockey.simg rl_hockey.def
+
+echo ""
+echo "Container built successfully!"
+echo ""
+echo "Container location: $PROJECT_DIR/singularity_build/rl_hockey.simg"
+echo ""
+echo "Next steps:"
+echo "1. Update train_single_run.sbatch if needed (check paths and email)"
+echo "2. Submit the job: sbatch train_single_run.sbatch"
+echo "3. Monitor with: squeue -u \$USER"
diff --git a/configs/curriculum_sac.json b/configs/curriculum_sac.json
index 3fd0905..0b21460 100644
--- a/configs/curriculum_sac.json
+++ b/configs/curriculum_sac.json
@@ -2,43 +2,56 @@
   "curriculum": {
     "phases": [
       {
-        "name": "easy",
-        "episodes": 20000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
+        "name": "shooting_basics",
+        "episodes": 400,
+        "environment": {
+          "mode": "TRAIN_SHOOTING",
+          "keep_mode": true
+        },
+        "opponent": {
+          "type": "none",
+          "weight": 1.0
+        },
+        "reward_shaping": null
+      },
+      {
+        "name": "defense_basics",
+        "episodes": 300,
+        "environment": {
+          "mode": "TRAIN_DEFENSE",
+          "keep_mode": true
+        },
         "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.80},
-            {"type": "basic_strong", "weight": 0.20}
-          ]
+          "type": "basic_weak",
+          "weight": 1.0
         },
         "reward_shaping": null
       },
       {
-        "name": "easy",
-        "episodes": 20000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
+        "name": "normal_game",
+        "episodes": 500,
+        "environment": {
+          "mode": "NORMAL",
+          "keep_mode": true
+        },
         "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.10},
-            {"type": "basic_strong", "weight": 0.90}
-          ]
+          "type": "basic_weak",
+          "weight": 1.0
         },
         "reward_shaping": null
       }
     ]
   },
   "hyperparameters": {
-    "learning_rate": 0.001,
+    "learning_rate": 0.0003,
     "batch_size": 256
   },
   "training": {
     "max_episode_steps": 500,
     "updates_per_step": 1,
-    "warmup_steps": 80000,
+    "warmup_steps": 20000,
     "reward_scale": 0.5,
-    "checkpoint_save_freq": 5000
+    "checkpoint_save_freq": 100
   },
   "agent": {
     "type": "SAC",
diff --git a/configs/curriculum_simple.json b/configs/curriculum_simple.json
index a0aa310..b367adc 100644
--- a/configs/curriculum_simple.json
+++ b/configs/curriculum_simple.json
@@ -3,14 +3,14 @@
     "phases": [
       {
         "name": "shooting_basics",
-        "episodes": 10000,
+        "episodes": 8000,
         "environment": {"mode": "TRAIN_SHOOTING", "keep_mode": true},
         "opponent": {"type": "none", "weight": 1.0},
         "reward_shaping": null
       },
       {
         "name": "defense_basics",
-        "episodes": 10000,
+        "episodes": 8000,
         "environment": {"mode": "TRAIN_DEFENSE", "keep_mode": true},
         "opponent": {"type": "basic_weak", "weight": 1.0},
         "reward_shaping": null
@@ -54,7 +54,7 @@
       "hidden_dim": [256, 512, 256],
       "target_update_freq": 2000,
       "eps": 1,
-      "action_fineness": 7,
+      "action_fineness": 4,
       "eps_min": 0.1,
       "eps_decay": 0.9995,
       "use_huber_loss": true
diff --git a/configs/curriculum_td3 copy.json b/configs/curriculum_td3 copy.json
deleted file mode 100644
index 407c18c..0000000
--- a/configs/curriculum_td3 copy.json	
+++ /dev/null
@@ -1,65 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "shooting_basics",
-        "episodes": 8000,
-        "environment": {"mode": "TRAIN_SHOOTING", "keep_mode": true},
-        "opponent": {"type": "none", "weight": 1.0},
-        "reward_shaping": null
-      },
-      {
-        "name": "defense_basics",
-        "episodes": 8000,
-        "environment": {"mode": "TRAIN_DEFENSE", "keep_mode": true},
-        "opponent": {"type": "basic_weak", "weight": 1.0},
-        "reward_shaping": null
-      },
-      {
-        "name": "integration_phase",
-        "episodes": 20000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.55},
-            {"type": "basic_strong", "weight": 0.45}
-          ]
-        },
-        "reward_shaping": null
-      },
-      {
-        "name": "full_competency",
-        "episodes": 40000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {"type": "basic_strong", "weight": 1.0},
-        "reward_shaping": null
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0005,
-    "batch_size": 1024
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 80000,
-    "reward_scale": 0.5,
-    "checkpoint_save_freq": 5000
-  },
-  "agent": {
-    "type": "TD3",
-    "hyperparameters": {
-            "learning_rate": 3e-4,
-            "max_action": 1.0,
-            "discount": 0.99,
-            "tau": 0.005,
-            "expl_noise": 0.1,
-            "policy_noise": 0.2,
-            "noise_clip": 0.5,
-            "policy_freq": 2,
-            "batch_size": 256
-        }
-  }
-}
diff --git a/configs/curriculum_td3.json b/configs/curriculum_td3.json
deleted file mode 100644
index ad111ed..0000000
--- a/configs/curriculum_td3.json
+++ /dev/null
@@ -1,50 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "integration_phase",
-        "episodes": 100000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "weight": 1.0,
-          "checkpoint": null,
-          "deterministic": true,
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.5},
-            {"type": "basic_strong", "weight": 0.5}
-          ]
-        },
-        "reward_shaping": null
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0005,
-    "batch_size": 1024
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 80000,
-    "reward_scale": 1,
-    "checkpoint_save_freq": 5000
-  },
-    "agent": {
-      "type": "TD3",
-      "hyperparameters": {
-            "learning_rate": 5e-4,
-            "max_action": 1.0,
-            "discount": 0.99999,
-            "tau": 0.005,
-            "expl_noise": 0.1,
-            "policy_noise": 0.2,
-            "noise_clip": 0.5,
-            "policy_freq": 2,
-            "batch_size": 1024,
-            "priority_replay": true,
-            "normalize_obs": true
-      }
-    }
-  }
-  
\ No newline at end of file
diff --git a/configs/curriculum_td3_reference.json b/configs/curriculum_td3_reference.json
deleted file mode 100644
index 4287455..0000000
--- a/configs/curriculum_td3_reference.json
+++ /dev/null
@@ -1,73 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "shooting_basics",
-        "episodes": 8000,
-        "environment": {"mode": "TRAIN_SHOOTING", "keep_mode": true},
-        "opponent": {"type": "none", "weight": 1.0},
-        "reward_shaping": null
-      },
-      {
-        "name": "defense_basics",
-        "episodes": 8000,
-        "environment": {"mode": "TRAIN_DEFENSE", "keep_mode": true},
-        "opponent": {"type": "basic_weak", "weight": 1.0},
-        "reward_shaping": null
-      },
-      {
-        "name": "integration_phase",
-        "episodes": 20000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.55},
-            {"type": "basic_strong", "weight": 0.45}
-          ]
-        },
-        "reward_shaping": null
-      },
-      {
-        "name": "full_competency",
-        "episodes": 40000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {"type": "basic_strong", "weight": 1.0},
-        "reward_shaping": {
-          "N": 150,
-          "K": 150,
-          "CLOSENESS_START": 20,
-          "TOUCH_START": 15,
-          "CLOSENESS_FINAL": 15,
-          "TOUCH_FINAL": 1,
-          "DIRECTION_FINAL": 2
-        }
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0005,
-    "batch_size": 1024
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 80000,
-    "reward_scale": 0.5,
-    "checkpoint_save_freq": 5000
-  },
-  "agent": {
-    "type": "TD3",
-    "hyperparameters": {
-            "learning_rate": 3e-4,
-            "max_action": 1.0,
-            "discount": 0.99,
-            "tau": 0.005,
-            "expl_noise": 0.1,
-            "policy_noise": 0.2,
-            "noise_clip": 0.5,
-            "policy_freq": 2,
-            "batch_size": 256
-        }
-  }
-}
diff --git a/configs/curriculum_tdmpc2.json b/configs/curriculum_tdmpc2.json
deleted file mode 100644
index a4c8076..0000000
--- a/configs/curriculum_tdmpc2.json
+++ /dev/null
@@ -1,71 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "integration_phase_shaping",
-        "episodes": 300,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.55},
-            {"type": "basic_strong", "weight": 0.45}
-          ]
-        }
-      },
-      {
-        "name": "integration_phase",
-        "episodes": 2700,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.55},
-            {"type": "basic_strong", "weight": 0.45}
-          ]
-        }
-      },
-      {
-        "name": "full_competency",
-        "episodes": 10000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {"type": "basic_strong", "weight": 1.0}
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0003,
-    "batch_size": 512
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 5000,
-    "reward_scale": 1.0,
-    "checkpoint_save_freq": 200
-  },
-  "agent": {
-    "type": "TDMPC2",
-    "hyperparameters": {
-      "latent_dim": 256,
-      "hidden_dim": {
-        "encoder": [256, 256, 256],
-        "dynamics": [256, 256, 256],
-        "reward": [256, 256, 256],
-        "termination": [256, 256],
-        "q_function": [256, 256, 256],
-        "policy": [256, 256, 256]
-      },
-      "num_q": 2,
-      "gamma": 0.99,
-      "horizon": 8,
-      "num_samples": 256,
-      "num_iterations": 6,
-      "temperature": 0.5,
-      "vmin": -10.0,
-      "vmax": 10.0,
-      "win_reward_bonus": 10.0,
-      "win_reward_discount": 0.92
-    }
-  }
-}
diff --git a/configs/curriculum_tdmpc2_bonus_decay.json b/configs/curriculum_tdmpc2_bonus_decay.json
deleted file mode 100644
index 5406750..0000000
--- a/configs/curriculum_tdmpc2_bonus_decay.json
+++ /dev/null
@@ -1,79 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "integration_phase_shaping",
-        "episodes": 300,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.55},
-            {"type": "basic_strong", "weight": 0.45}
-          ]
-        }
-      },
-      {
-        "name": "integration_phase",
-        "episodes": 2700,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.55},
-            {"type": "basic_strong", "weight": 0.45}
-          ]
-        }
-      },
-      {
-        "name": "full_competency_bonus_decay",
-        "episodes": 10000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {"type": "basic_strong", "weight": 1.0},
-        "reward_bonus": {
-          "N": 0,
-          "K": 2000,
-          "WIN_BONUS_START": 10.0,
-          "WIN_BONUS_FINAL": 1.0,
-          "WIN_DISCOUNT_START": 0.92,
-          "WIN_DISCOUNT_FINAL": 0.92
-        }
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0003,
-    "batch_size": 512
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 5000,
-    "reward_scale": 1.0,
-    "checkpoint_save_freq": 200
-  },
-  "agent": {
-    "type": "TDMPC2",
-    "hyperparameters": {
-      "latent_dim": 256,
-      "hidden_dim": {
-        "encoder": [256, 256, 256],
-        "dynamics": [256, 256, 256],
-        "reward": [256, 256, 256],
-        "termination": [256, 256],
-        "q_function": [256, 256, 256],
-        "policy": [256, 256, 256]
-      },
-      "num_q": 2,
-      "gamma": 0.99,
-      "horizon": 8,
-      "num_samples": 256,
-      "num_iterations": 6,
-      "temperature": 0.5,
-      "vmin": -10.0,
-      "vmax": 10.0,
-      "win_reward_bonus": 10.0,
-      "win_reward_discount": 0.92
-    }
-  }
-}
diff --git a/configs/curriculum_tdmpc2_repo.json b/configs/curriculum_tdmpc2_repo.json
deleted file mode 100644
index 0b2c2b4..0000000
--- a/configs/curriculum_tdmpc2_repo.json
+++ /dev/null
@@ -1,104 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "warmup_phase",
-        "episodes": 500,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {"type": "basic_weak", "weight": 1.0}
-      },
-      {
-        "name": "integration_phase_shaping",
-        "episodes": 500,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.6},
-            {"type": "basic_strong", "weight": 0.4}
-          ]
-        }
-      },
-      {
-        "name": "integration_phase",
-        "episodes": 2000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.5},
-            {"type": "basic_strong", "weight": 0.5}
-          ]
-        }
-      },
-      {
-        "name": "competitive_phase",
-        "episodes": 5000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {
-          "type": "weighted_mixture",
-          "opponents": [
-            {"type": "basic_weak", "weight": 0.3},
-            {"type": "basic_strong", "weight": 0.7}
-          ]
-        }
-      },
-      {
-        "name": "full_competency",
-        "episodes": 10000,
-        "environment": {"mode": "NORMAL", "keep_mode": true},
-        "opponent": {"type": "basic_strong", "weight": 1.0}
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0003,
-    "batch_size": 512
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 2,
-    "warmup_steps": 5000,
-    "reward_scale": 2.0,
-    "checkpoint_save_freq": 100
-  },
-  "agent": {
-    "type": "TDMPC2_REPO",
-    "hyperparameters": {
-      "latent_dim": 512,
-      "num_q": 5,
-      "gamma": 0.99,
-      "horizon": 5,
-      "num_samples": 512,
-      "num_iterations": 6,
-      "num_elites": 64,
-      "num_pi_trajs": 24,
-      "temperature": 0.5,
-      "capacity": 1000000,
-      "vmin": -10.0,
-      "vmax": 10.0,
-      "tau": 0.01,
-      "grad_clip_norm": 20.0,
-      "consistency_coef": 20.0,
-      "reward_coef": 0.1,
-      "value_coef": 0.1,
-      "termination_coef": 1.0,
-      "rho": 0.5,
-      "entropy_coef": 0.0001,
-      "log_std_min": -10.0,
-      "log_std_max": 2.0,
-      "min_std": 0.05,
-      "max_std": 2.0,
-      "discount_denom": 5.0,
-      "discount_min": 0.95,
-      "discount_max": 0.995,
-      "episodic": true,
-      "mpc": true,
-      "compile": false,
-      "episode_length": 500,
-      "enc_lr_scale": 0.3,
-      "win_reward_bonus": 0.0,
-      "win_reward_discount": 1.0
-    }
-  }
-}
diff --git a/configs/curriculum_test.json b/configs/curriculum_test.json
deleted file mode 100644
index 373c268..0000000
--- a/configs/curriculum_test.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "test_phase",
-        "episodes": 100,
-        "environment": {"mode": "TRAIN_SHOOTING", "keep_mode": true},
-        "opponent": {"type": "none", "weight": 1.0},
-        "reward_shaping": null
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0005,
-    "batch_size": 256
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 1000,
-    "reward_scale": 0.5,
-    "checkpoint_save_freq": 1000
-  },
-  "agent": {
-    "type": "DDDQN",
-    "hyperparameters": {
-      "hidden_dim": [256, 256],
-      "target_update_freq": 50,
-      "eps": 1,
-      "eps_min": 0.1,
-      "eps_decay": 0.9995
-    }
-  }
-}
diff --git a/configs/curriculum_test_per.json b/configs/curriculum_test_per.json
deleted file mode 100644
index 60a21ee..0000000
--- a/configs/curriculum_test_per.json
+++ /dev/null
@@ -1,37 +0,0 @@
-{
-  "curriculum": {
-    "phases": [
-      {
-        "name": "test_phase",
-        "episodes": 100,
-        "environment": {"mode": "TRAIN_SHOOTING", "keep_mode": true},
-        "opponent": {"type": "none", "weight": 1.0},
-        "reward_shaping": null
-      }
-    ]
-  },
-  "hyperparameters": {
-    "learning_rate": 0.0005,
-    "batch_size": 256
-  },
-  "training": {
-    "max_episode_steps": 500,
-    "updates_per_step": 1,
-    "warmup_steps": 1000,
-    "reward_scale": 0.5,
-    "checkpoint_save_freq": 1000
-  },
-  "agent": {
-    "type": "DDQN_PER",
-    "hyperparameters": {
-      "hidden_dim": [256, 256],
-      "target_update_freq": 50,
-      "eps": 1,
-      "eps_min": 0.1,
-      "eps_decay": 0.9995,
-      "use_per": true,
-      "per_alpha": 0.6,
-      "per_beta": 0.4
-    }
-  }
-}
diff --git a/docs/TDMPC2_PROFILING_ANALYSIS.md b/docs/TDMPC2_PROFILING_ANALYSIS.md
deleted file mode 100644
index 09a6536..0000000
--- a/docs/TDMPC2_PROFILING_ANALYSIS.md
+++ /dev/null
@@ -1,586 +0,0 @@
-# TD-MPC2 Profiling Analysis & Training Optimization
-
-## Optimization Results Summary
-
-### ✅ Implemented Optimizations (Reference Repo Strategy)
-
-We implemented the same optimization strategy used by the official TD-MPC2 repository. The results show a **~2x speedup** in training:
-
-| Metric | Before | After | Improvement |
-|--------|--------|-------|-------------|
-| **CPU time per step** | 138.3ms | 67.4ms | **2.05× faster** |
-| **CUDA time per step** | 18.8ms | 9.7ms | **1.94× faster** |
-| **Total CPU time (25 steps)** | 3.457s | 1.686s | **2.05× faster** |
-| **Total CUDA time (25 steps)** | 469.7ms | 242.7ms | **1.93× faster** |
-| **`aten::linear` calls** | 5,450 | 900 | **6× fewer** |
-| **`aten::layer_norm` calls** | 3,750 | 600 | **6.25× fewer** |
-| **`CompiledFunctionBackward` calls** | 675 | 325 | **2.08× fewer** |
-
-### What Was Changed
-
-#### 1. `capturable=True` in Optimizers
-**File**: [tdmpc2.py#L244-L261](../src/rl_hockey/TD_MPC2/tdmpc2.py#L244-L261)
-
-```python
-# BEFORE
-self.optimizer = torch.optim.Adam([...], lr=self.lr)
-self.pi_optimizer = torch.optim.Adam([...], lr=self.lr, eps=1e-5)
-
-# AFTER
-self.optimizer = torch.optim.Adam([...], lr=self.lr, capturable=True)
-self.pi_optimizer = torch.optim.Adam([...], lr=self.lr, eps=1e-5, capturable=True)
-```
-
-**Why**: Allows the optimizer to be captured in CUDAGraphs, enabling the GPU to execute the entire optimization step as a single graph instead of launching individual kernels.
-
-#### 2. `cudagraph_mark_step_begin()` at Training Iteration Start
-**File**: [tdmpc2.py#L632-L635](../src/rl_hockey/TD_MPC2/tdmpc2.py#L632-L635)
-
-```python
-for _ in range(steps):
-    # Mark step boundary for CUDAGraphs (like reference repo)
-    if hasattr(torch.compiler, "cudagraph_mark_step_begin"):
-        torch.compiler.cudagraph_mark_step_begin()
-```
-
-**Why**: Explicitly marks iteration boundaries for torch.compile's CUDAGraph capture. This allows the compiler to know where one training step ends and the next begins, enabling better graph optimization.
-
-#### 3. Pre-compute TD Targets Before the Dynamics Loop
-**File**: [tdmpc2.py#L671-L678](../src/rl_hockey/TD_MPC2/tdmpc2.py#L671-L678)
-
-```python
-# BEFORE: TD targets computed INSIDE the loop for each timestep
-for t in range(horizon):
-    # ... compute TD target for this timestep
-    z_bootstrap = z_seq[:, t + n].detach()
-    next_action, _, _, _ = self.policy.sample(z_bootstrap)
-    target_q = self.target_q_ensemble.min(z_bootstrap, next_action)
-    # ...
-
-# AFTER: TD targets computed ONCE before the loop
-with torch.no_grad():
-    next_z = z_seq[:, 1:].detach()  # (batch, horizon, latent)
-    td_targets = self._compute_td_targets(next_z, rewards_seq, dones_seq, horizon)
-```
-
-**Why**: Instead of computing policy actions and target Q-values H times (once per horizon step), we compute them all at once in a single batched operation. This reduces:
-- Policy forward passes: 8 → 1
-- Target Q-ensemble forward passes: 8 → 1
-
-#### 4. Pre-compute Lambda Weights
-**File**: [tdmpc2.py#L692-L695](../src/rl_hockey/TD_MPC2/tdmpc2.py#L692-L695)
-
-```python
-# BEFORE: Computed inside loop
-for t in range(horizon):
-    weight = self.lambda_coef**t  # Recomputed every iteration
-
-# AFTER: Pre-computed tensor
-lambda_weights = self.lambda_coef ** torch.arange(
-    horizon, device=self.device, dtype=torch.float32
-)
-# Then use lambda_weights[t] in loop
-```
-
-**Why**: Avoids Python overhead of computing `lambda_coef**t` on each iteration. Minor improvement but contributes to overall efficiency.
-
-#### 5. Batched Reward and Q-Value Predictions After Dynamics Rollout
-**File**: [tdmpc2.py#L715-L748](../src/rl_hockey/TD_MPC2/tdmpc2.py#L715-L748)
-
-```python
-# BEFORE: Predictions INSIDE the loop
-for t in range(horizon):
-    r_pred_logits = self.reward(z_pred, a_t)  # Called 8 times
-    q_preds_logits = self.q_ensemble(z_pred, a_t)  # Called 8 times
-    # ... compute losses
-
-# AFTER: Predictions OUTSIDE the loop (single batched call)
-_zs = zs[:-1]  # (horizon, batch, latent)
-_zs_flat = _zs.reshape(-1, self.latent_dim)  # (horizon*batch, latent)
-_actions_flat = actions_seq.permute(1, 0, 2).reshape(-1, self.action_dim)
-
-# SINGLE forward pass for all timesteps
-reward_preds = self.reward(_zs_flat, _actions_flat)
-q_preds_all = self.q_ensemble(_zs_flat, _actions_flat)
-```
-
-**Why**: This is the **most impactful optimization**. Instead of calling `reward()` and `q_ensemble()` H times (8 times for horizon=8), we call them once with a batch size of `horizon * batch_size`. This:
-- Reduces CUDA kernel launches from 16 (8 reward + 8 Q) to 2 (1 reward + 1 Q)
-- Amortizes kernel launch overhead over more data
-- Enables better GPU utilization with larger batch sizes
-
-#### 6. New `_compute_td_targets()` Helper Method
-**File**: [tdmpc2.py#L908-L964](../src/rl_hockey/TD_MPC2/tdmpc2.py#L908-L964)
-
-This new method computes all TD targets in a single batched operation:
-
-```python
-@torch.no_grad()
-def _compute_td_targets(self, next_z, rewards_seq, dones_seq, horizon):
-    """Compute TD targets for all timesteps at once (like reference repo)."""
-    batch_size = next_z.shape[0]
-    
-    if self.n_step == 1:
-        # Flatten for batched computation
-        next_z_flat = next_z.reshape(-1, self.latent_dim)
-        
-        # Get policy actions for ALL next states at once
-        next_actions, _, _, _ = self.policy.sample(next_z_flat)
-        
-        # Get target Q-values for ALL next states at once
-        target_q_flat = self.target_q_ensemble.min(next_z_flat, next_actions)
-        target_q = target_q_flat.reshape(batch_size, horizon, 1)
-        
-        # TD target: r + γ(1-d)Q(s',a')
-        td_targets = rewards_seq + self.gamma * (1.0 - dones_seq) * target_q
-    # ...
-    return td_targets
-```
-
-### Why The Dynamics Loop Remains Sequential
-
-The dynamics rollout **must remain sequential** because each prediction depends on the previous one:
-
-```
-z₁ = dynamics(z₀, a₀)
-z₂ = dynamics(z₁, a₁)  ← depends on z₁!
-z₃ = dynamics(z₂, a₂)  ← depends on z₂!
-```
-
-The reference TD-MPC2 repo uses the same approach: **sequential dynamics, batched everything else**.
-
-The `zs` tensor (predicted latent states) is needed for **policy training** - the policy learns to maximize Q-values in the predicted future states, not just ground-truth states.
-
-### Profiling Evidence
-
-The kernel call reduction proves the optimization worked:
-
-| Kernel Type | Before | After | Reduction |
-|-------------|--------|-------|-----------|
-| `aten::linear` | 5,450 calls | 900 calls | **6× fewer** |
-| `aten::layer_norm` | 3,750 calls | 600 calls | **6.25× fewer** |
-| `CompiledFunctionBackward` | 675 calls | 325 calls | **2.08× fewer** |
-| `aten::copy_` | 15,700 calls | 4,350 calls | **3.6× fewer** |
-
-With `horizon=8`, we'd expect roughly a 6-8× reduction in forward pass calls (from inside-loop to batched), which matches the observed ~6× reduction in `aten::linear` calls.
-
----
-
-## Original Profiling Results (BEFORE Optimization)
-
-```
-================================================================================
-TD-MPC2 PROFILING SUMMARY
-================================================================================
-
-Device: cuda
-GPU 0: NVIDIA GeForce RTX 2080 Ti
-GPU Memory: 11.3 GB
-Config: /home/stud421/RL_CheungMaenzerAbraham_Hockey/configs/curriculum_tdmpc2.json
-Iterations: 50
-
-Agent configuration:
-  obs_dim: 18
-  action_dim: 8
-  latent_dim: 256
-  horizon: 8
-  num_samples: 256
-  num_iterations: 6
-
-================================================================================
-TRAINING STEP (Most Relevant for Training Speedup)
-================================================================================
-
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
-                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
-                                             train_step         0.00%       0.000us         0.00%       0.000us       0.000us        2.718s       578.63%        2.718s     108.714ms            25
-                                         training_total         0.01%     447.509us        82.29%        2.844s        2.844s       0.000us         0.00%     233.014ms     233.014ms             1
-                                             train_step        33.62%        1.162s        82.27%        2.844s     113.751ms       0.000us         0.00%     233.014ms       9.321ms            25
-autograd::engine::evaluate_function: CompiledFunctio...         1.13%      38.935ms        11.02%     380.809ms     564.162us       0.000us         0.00%     208.957ms     309.565us           675
-                               CompiledFunctionBackward         4.02%     139.107ms         6.75%     233.469ms     345.880us     185.804ms        39.56%     189.164ms     280.244us           675
-                               Optimizer.step#Adam.step         0.00%       0.000us         0.00%       0.000us       0.000us      72.810ms        15.50%      72.810ms       1.456ms            50
-                                       CompiledFunction         3.18%     109.849ms         6.10%     210.871ms     312.401us      61.002ms        12.99%      64.184ms      95.088us           675
-                                           aten::linear         0.60%      20.731ms        13.44%     464.646ms      85.256us       0.000us         0.00%      49.233ms       9.034us          5450
-                                            aten::copy_         2.97%     102.633ms         6.52%     225.223ms      14.345us      37.810ms         8.05%      37.825ms       2.409us         15700
-triton_red_fused__to_copy_add_fill_mish_mul_native_l...         0.00%       0.000us         0.00%       0.000us       0.000us      35.259ms         7.51%      35.259ms      25.185us          1400
-                                               aten::to         0.35%      12.107ms         7.98%     275.968ms      14.080us       0.000us         0.00%      30.427ms       1.552us         19600
-                                         aten::_to_copy         1.05%      36.339ms         7.63%     263.861ms      20.736us       0.000us         0.00%      30.427ms       2.391us         12725
-                                       aten::layer_norm         0.26%       8.845ms         5.16%     178.321ms      47.552us       0.000us         0.00%      29.293ms       7.811us          3750
-void cutlass::Kernel2<cutlass_75_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us      29.095ms         6.19%      29.095ms       6.064us          4798
-                             Torch-Compiled Region: 3/0         0.37%      12.731ms         2.89%      99.912ms     499.562us       0.000us         0.00%      25.218ms     126.092us           200
-void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      23.949ms         5.10%      23.949ms       1.797us         13325
-                                             aten::add_         2.48%      85.892ms         3.90%     134.794ms       9.509us      22.355ms         4.76%      22.355ms       1.577us         14175
-                               Optimizer.step#Adam.step         1.11%      38.319ms         2.46%      85.192ms       1.704ms       0.000us         0.00%      19.183ms     383.663us            50
-triton_red_fused__to_copy_add_fill_mish_mul_native_l...         0.00%       0.000us         0.00%       0.000us       0.000us      17.105ms         3.64%      17.105ms      28.509us           600
-                                            aten::addmm         2.10%      72.608ms         3.52%     121.652ms      44.643us      16.799ms         3.58%      16.811ms       6.169us          2725
-                                              aten::mul         1.86%      64.121ms         3.00%     103.808ms      15.210us      16.795ms         3.58%      16.807ms       2.463us          6825
-void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      16.354ms         3.48%      16.354ms       1.874us          8725
-triton_per_fused__to_copy_add_fill_mish_mul_native_l...         0.00%       0.000us         0.00%       0.000us       0.000us      14.556ms         3.10%      14.556ms       2.426us          6000
-                             Torch-Compiled Region: 1/1         0.54%      18.515ms         2.23%      77.059ms     385.294us       0.000us         0.00%      13.550ms      67.750us           200
-void cutlass::Kernel2<cutlass_75_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us      12.702ms         2.70%      12.702ms       7.057us          1800
-                             Torch-Compiled Region: 2/1         0.47%      16.349ms         1.73%      59.935ms     299.674us       0.000us         0.00%      12.435ms      62.175us           200
-void cutlass::Kernel2<cutlass_75_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us      12.277ms         2.61%      12.277ms       6.820us          1800
-void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      10.974ms         2.34%      10.974ms       4.027us          2725
-                                aten::native_layer_norm         0.65%      22.559ms         1.66%      57.312ms      30.567us      10.853ms         2.31%      10.853ms       5.788us          1875
-void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us      10.853ms         2.31%      10.853ms       5.788us          1875
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
-Self CPU time total: 3.457s
-Self CUDA time total: 469.702ms
-```
-
----
-
-## Profiling Results AFTER Optimization
-
-```
-================================================================================
-TRAINING STEP (AFTER OPTIMIZATION)
-================================================================================
-
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
-                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
-                                             train_step         0.00%       0.000us         0.00%       0.000us       0.000us        1.211s       498.84%        1.211s      48.431ms            25  
-                               Optimizer.step#Adam.step         0.00%       0.000us         0.00%       0.000us       0.000us     154.580ms        63.69%     154.580ms       3.092ms            50  
-                                         training_total         0.03%     461.066us        79.74%        1.344s        1.344s       0.000us         0.00%     122.513ms     122.513ms             1  
-                                             train_step        37.29%     628.626ms        79.71%        1.344s      53.747ms       0.000us         0.00%     122.513ms       4.901ms            25  
-autograd::engine::evaluate_function: CompiledFunctio...         0.81%      13.591ms        10.47%     176.418ms     542.825us       0.000us         0.00%      97.689ms     300.580us           325  
-                               CompiledFunctionBackward         3.96%      66.682ms         7.05%     118.771ms     365.450us      88.632ms        36.52%      92.305ms     284.014us           325  
-                                       CompiledFunction         3.37%      56.744ms         5.36%      90.428ms     278.240us      34.535ms        14.23%      37.291ms     114.742us           325  
-                               Optimizer.step#Adam.step         2.27%      38.346ms         9.93%     167.366ms       3.347ms       0.000us         0.00%      29.924ms     598.477us            50  
-                                           aten::linear         0.20%       3.431ms         4.04%      68.083ms      75.648us       0.000us         0.00%      18.578ms      20.642us           900  
-triton_red_fused__to_copy_add_fill_mish_mul_native_l...         0.00%       0.000us         0.00%       0.000us       0.000us      17.147ms         7.06%      17.147ms      28.579us           600  
-                                            aten::copy_         1.56%      26.355ms         3.63%      61.259ms      14.082us      14.859ms         6.12%      14.859ms       3.416us          4350  
-turing_fp16_s1688gemm_fp16_128x128_ldg8_relu_f2f_sta...         0.00%       0.000us         0.00%       0.000us       0.000us      14.680ms         6.05%      14.680ms      20.333us           722  
-                             Torch-Compiled Region: 1/2         1.10%      18.524ms         4.32%      72.845ms     364.223us       0.000us         0.00%      12.984ms      64.919us           200  
-                                       aten::layer_norm         0.08%       1.429ms         1.62%      27.333ms      45.555us       0.000us         0.00%      12.539ms      20.899us           600  
-                                    aten::_foreach_div_         0.41%       6.836ms         3.49%      58.862ms     130.805us     328.408us         0.14%      10.182ms      22.626us           450  
-void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       9.959ms         4.10%       9.959ms       2.108us          4725  
-                                             aten::div_         1.71%      28.896ms         3.05%      51.451ms      10.889us       9.888ms         4.07%       9.892ms       2.094us          4725  
-                                               aten::to         0.17%       2.907ms         3.27%      55.062ms       6.403us       0.000us         0.00%       9.576ms       1.114us          8600  
-                                         aten::_to_copy         0.51%       8.605ms         3.09%      52.155ms      18.965us       0.000us         0.00%       9.576ms       3.482us          2750  
-void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.247ms         3.40%       8.247ms       1.874us          4400  
-                             Torch-Compiled Region: 3/1         0.14%       2.350ms         0.68%      11.496ms     459.847us       0.000us         0.00%       7.994ms     319.747us            25  
-turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us       7.983ms         3.29%       7.983ms      18.783us           425  
-                                             aten::add_         2.32%      39.134ms         3.87%      65.224ms      16.618us       7.727ms         3.18%       7.732ms       1.970us          3925  
-                                            aten::addmm         0.75%      12.637ms         1.03%      17.281ms      38.403us       7.571ms         3.12%       7.571ms      16.825us           450  
-                             Torch-Compiled Region: 3/0         0.09%       1.554ms         0.56%       9.493ms     379.735us       0.000us         0.00%       7.368ms     294.731us            25  
-                                   aten::_foreach_copy_         0.37%       6.294ms         1.06%      17.916ms      27.563us       5.765ms         2.38%       6.298ms       9.690us           650  
-                                              aten::mul         2.16%      36.393ms         3.01%      50.689ms      20.480us       5.698ms         2.35%       5.698ms       2.302us          2475  
-void cutlass::Kernel2<cutlass_75_wmma_tensorop_f16_s...         0.00%       0.000us         0.00%       0.000us       0.000us       5.395ms         2.22%       5.395ms       6.347us           850  
-    autograd::engine::evaluate_function: AddmmBackward0         0.09%       1.454ms         0.85%      14.288ms     114.301us       0.000us         0.00%       5.300ms      42.396us           125  
-triton_per_fused__to_copy_native_layer_norm_native_l...         0.00%       0.000us         0.00%       0.000us       0.000us       5.180ms         2.13%       5.180ms       2.158us          2400  
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
-Self CPU time total: 1.686s
-Self CUDA time total: 242.718ms
-```
-
-### Side-by-Side Comparison
-
-| Metric | BEFORE | AFTER | Change |
-|--------|--------|-------|--------|
-| **Self CPU time total** | 3.457s | 1.686s | **-51%** |
-| **Self CUDA time total** | 469.7ms | 242.7ms | **-48%** |
-| **train_step CPU avg** | 113.75ms | 53.75ms | **-53%** |
-| **train_step CUDA avg** | 9.32ms | 4.90ms | **-47%** |
-| **`aten::linear` calls** | 5,450 | 900 | **-83%** |
-| **`aten::layer_norm` calls** | 3,750 | 600 | **-84%** |
-| **`aten::copy_` calls** | 15,700 | 4,350 | **-72%** |
-| **`CompiledFunctionBackward` calls** | 675 | 325 | **-52%** |
-
----
-
-## Original Single Action Selection (For Reference)
-
-```
-================================================================================
-SINGLE ACTION SELECTION (For Reference)
-================================================================================
-
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
-                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
-                                              act_total         0.00%       0.000us         0.00%       0.000us       0.000us        6.455s       888.58%        6.455s        6.455s             1
-                                              act_total        20.67%        1.336s       100.00%        6.462s        6.462s       0.000us         0.00%     726.627ms     726.627ms             1
-                                           aten::linear         2.78%     179.440ms        51.82%        3.349s      53.409us       0.000us         0.00%     422.392ms       6.737us         62700
-                                       aten::layer_norm         1.53%      98.727ms        27.80%        1.797s      40.651us       0.000us         0.00%     305.008ms       6.901us         44200
-                                            aten::addmm        13.00%     840.333ms        18.19%        1.175s      34.875us     193.282ms        26.61%     193.379ms       5.738us         33700
-                                            aten::copy_         5.25%     339.400ms        10.90%     704.298ms      11.314us     154.239ms        21.23%     154.275ms       2.478us         62250
--------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
-Self CPU time total: 6.462s
-Self CUDA time total: 726.444ms
-
-================================================================================
-PROFILING COMPLETE
-================================================================================
-```
-
----
-
-## Understanding the BEFORE Profiling Results
-
-This section explains the profiling data that motivated our optimizations.
-
-### Training Step Breakdown (Before Optimization)
-
-The training step profile over 25 iterations reveals critical bottlenecks:
-
-| Metric | Value | Meaning |
-|--------|-------|---------|
-| **Total CPU Time** | 3.457s | Time the CPU spent orchestrating operations |
-| **Total CUDA Time** | 469.7ms | Actual GPU compute time |
-| **CPU/CUDA Ratio** | **7.4x** | CPU takes 7.4x longer than GPU — severe bottleneck |
-| **Time per train step** | 113.75ms CPU / 9.32ms CUDA | Per-step overhead |
-
-#### Top Time Consumers (Training)
-
-1. **`CompiledFunctionBackward` (39.6% CUDA, 6.75% CPU total)**
-   - This is the backward pass through torch.compile'd functions
-   - 185.8ms of CUDA time across 675 calls
-   - torch.compile adds overhead but also provides some fusion benefits
-   - The high CPU overhead (233ms total) suggests compilation/dispatch costs
-
-2. **`Optimizer.step#Adam.step` (15.5% CUDA)**
-   - Adam optimizer updates take 72.8ms of GPU time across 50 calls (2 optimizers × 25 steps)
-   - This is ~1.46ms per optimizer step — relatively efficient
-
-3. **`aten::linear` (13.4% CPU total, but only 49.2ms CUDA)**
-   - 5,450 linear layer calls
-   - CPU takes 464ms to dispatch these operations, but GPU only computes for 49ms
-   - **This is the CPU bottleneck** — Python/PyTorch dispatch overhead dominates
-
-4. **`aten::copy_` / `aten::to` / `aten::_to_copy` (~14.5% CPU total)**
-   - 15,700 copy operations consuming 225ms CPU time
-   - 19,600 `to()` calls consuming 276ms CPU time  
-   - These are dtype conversions and device transfers
-   - Only 30-38ms of actual CUDA time — the rest is CPU overhead
-
-5. **`aten::layer_norm` (5.16% CPU total)**
-   - 3,750 layer norm calls taking 178ms CPU but only 29ms CUDA
-   - 6x overhead ratio due to kernel launch costs
-
-6. **Triton Fused Kernels (positive sign)**
-   - `triton_red_fused_*` kernels show torch.compile IS fusing some operations
-   - ~52ms of CUDA time in fused operations — this is actually working
-
-### The Core Problem (Solved): CPU Dispatch Overhead
-
-Looking at the BEFORE numbers:
-- **CPU time**: 3.457s for 25 training steps = **138ms per step**
-- **CUDA time**: 469.7ms for 25 training steps = **18.8ms per step**
-
-The GPU finished in 18.8ms but had to wait **119.2ms** for the CPU to prepare the next batch of operations. This was caused by:
-
-1. **Too many kernel launches**: 8× reward predictions, 8× Q predictions per training step
-2. **Python loop overhead**: Sequential Python `for` loop couldn't be optimized by torch.compile
-3. **Kernel launch latency**: ~5-10μs per CUDA kernel × thousands of kernels
-
-**After optimization**: CPU time dropped to 67.4ms (2.05× faster) by batching predictions and enabling CUDAGraph optimizations.
-
-### Why torch.compile Alone Wasn't Enough
-
-The BEFORE profile shows `Torch-Compiled Region` entries:
-- Region 3/0: 99.9ms CPU, 25.2ms CUDA
-- Region 1/1: 77ms CPU, 13.5ms CUDA  
-- Region 2/1: 60ms CPU, 12.4ms CUDA
-
-torch.compile fuses operations and generates Triton kernels, but it couldn't fully optimize our code because:
-- **Control flow breaks compilation**: The `for t in range(horizon)` loop prevented full graph capture
-- **Many small kernel calls**: 8× reward and Q predictions launched separate kernels
-
-**Solution implemented**: We kept the dynamics loop sequential (necessary for autoregressive rollout) but moved reward/Q predictions OUTSIDE the loop into single batched calls. Combined with `capturable=True` and `cudagraph_mark_step_begin()`, this allowed better CUDAGraph capture.
-
----
-
-## Optimization Status Summary
-
-### ✅ Implemented Optimizations (Achieved ~2x Speedup)
-
-| Optimization | Status | Actual Impact |
-|--------------|--------|---------------|
-| `capturable=True` in optimizers | ✅ **Done** | Enables CUDAGraph capture |
-| `cudagraph_mark_step_begin()` | ✅ **Done** | Better graph optimization |
-| Pre-compute TD targets | ✅ **Done** | 8→1 policy/Q-target forward passes |
-| Pre-compute lambda weights | ✅ **Done** | Minor CPU overhead reduction |
-| Batched reward predictions | ✅ **Done** | 8→1 reward forward passes |
-| Batched Q-value predictions | ✅ **Done** | 8→1 Q-ensemble forward passes |
-
-**Result**: ~2x speedup achieved (138ms → 67ms per train step)
-
-### 🔄 Remaining Optimization Opportunities
-
-| Optimization | Status | Expected Impact |
-|--------------|--------|-----------------|
-| Batch `two_hot_inv` in policy update | ❌ Not done | 1.3-1.5x policy update |
-| `torch.inference_mode()` for inference | ❌ Not done | 5-15% inference speedup |
-| Optimize gradient norm computation | ❌ Not done | 10-20% gradient time |
-| Buffer sampling optimization | ❌ Not done | 1.2-1.5x sampling |
-| Fused Adam optimizer | ❌ Not done | 10-20% optimizer step |
-| Reduce horizon (8→5) | ⚙️ Config option | ~1.6x (trade-off with planning) |
-| Larger batch size | ⚙️ Config option | 1.1-1.3x (memory trade-off) |
-
----
-
-## Technical Background: Why the Dynamics Loop Remains Sequential
-
-### The Problem: Autoregressive Dependencies
-
-The training loop has an **autoregressive structure** where each dynamics prediction depends on the previous one:
-
-```
-Step 0: z₁_pred = dynamics(z₀, a₀)           # z₀ from encoder
-Step 1: z₂_pred = dynamics(z₁_pred, a₁)     # depends on z₁_pred!
-Step 2: z₃_pred = dynamics(z₂_pred, a₂)     # depends on z₂_pred!
-...
-Step H-1: zₕ_pred = dynamics(zₕ₋₁_pred, aₕ₋₁)
-```
-
-This creates a **sequential dependency chain** that cannot be fully parallelized.
-
-### The Reference Repo Strategy (What We Implemented)
-
-The official TD-MPC2 repository uses a **hybrid strategy**:
-
-1. **Sequential dynamics rollout** (necessary for autoregressive `zs` used in policy update)
-2. **Batched predictions AFTER rollout** (Q, reward predictions for all timesteps at once)
-
-```python
-# Build zs sequentially (must be autoregressive)
-zs = [z_seq[:, 0]]
-for t in range(horizon):
-    z_pred = self.dynamics(z_pred, actions_seq[:, t])
-    zs.append(z_pred)
-
-# THEN batch all predictions (this is what we optimized)
-zs_flat = torch.stack(zs[:-1]).reshape(-1, latent_dim)
-actions_flat = actions_seq.permute(1, 0, 2).reshape(-1, action_dim)
-reward_preds = self.reward(zs_flat, actions_flat)      # 1 call instead of 8
-q_preds = self.q_ensemble(zs_flat, actions_flat)       # 1 call instead of 8
-```
-
-### Why `zs` Must Be Autoregressive
-
-The predicted latent states `zs` are needed for **policy training**:
-
-```python
-def _update_policy(self, zs, ...):
-    actions = self.policy(zs)           # Policy actions from PREDICTED states
-    q_values = self.q_ensemble(zs, actions)  # Q-values in predicted trajectory
-```
-
-The policy learns to maximize Q-values **in the predicted future states**, not just ground-truth states. This is critical for model-based RL.
-
----
-
-## Future Optimization Opportunities
-
-The following optimizations were identified but not yet implemented. They could provide additional speedups beyond the ~2x already achieved.
-
-### Priority 1: Batch `two_hot_inv` in Policy Update
-
-**Problem**: In the policy update, `two_hot_inv` is called separately for each Q-network:
-
-```python
-q_values = torch.stack(
-    [two_hot_inv(q_logits, self.num_bins, self.vmin, self.vmax)
-     for q_logits in q_logits_all],
-    dim=0,
-)
-```
-
-This creates `num_q` (default 5) separate calls.
-
-**Solution**: Modify `two_hot_inv` to accept batched input:
-
-```python
-q_logits_stacked = torch.stack(q_logits_all, dim=0)  # (num_q, batch, num_bins)
-q_values = two_hot_inv_batched(q_logits_stacked, self.num_bins, self.vmin, self.vmax)
-```
-
-**Expected speedup**: 1.3-1.5x for policy update portion.
-
----
-
-### Priority 2: Use `torch.inference_mode()` for Non-Training Code
-
-**Problem**: `torch.no_grad()` has more overhead than `torch.inference_mode()`.
-
-**Solution**: Replace in `act()`, `act_batch()`, and target network computation:
-
-```python
-@torch.inference_mode()  # Instead of @torch.no_grad()
-def act(self, obs, deterministic=False, t0=False):
-```
-
-**Expected speedup**: 5-15% for inference paths.
-
----
-
-### Priority 3: Optimize Gradient Norm Computation
-
-**Problem**: Computing gradient norms 6 times per training step is wasteful when using `max_norm=float("inf")`:
-
-```python
-grad_norm_encoder = torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), max_norm=float("inf"))
-grad_norm_dynamics = torch.nn.utils.clip_grad_norm_(self.dynamics.parameters(), max_norm=float("inf"))
-# ... 4 more times
-```
-
-**Solution**: Compute norm once for all parameters:
-
-```python
-torch.nn.utils.clip_grad_norm_(self._model_params, max_norm=self.grad_clip_norm)
-```
-
-**Expected speedup**: 10-20% of gradient computation time.
-
----
-
-### Priority 4: Buffer Sampling Optimization
-
-**Problem**: `sample()` uses Python loops to build batches.
-
-**Solution**: Pre-allocate tensors and use advanced indexing.
-
-**Expected speedup**: 1.2-1.5x for sampling.
-
----
-
-### Priority 5: Fused Adam Optimizer
-
-**Problem**: Standard Adam has separate kernels for each parameter update.
-
-**Solution**: Use `torch.optim.AdamW` with `fused=True` (PyTorch 2.0+):
-
-```python
-self.optimizer = torch.optim.AdamW(param_groups, lr=self.lr, fused=True)
-```
-
-**Expected speedup**: 10-20% faster optimizer step.
-
----
-
-### Configuration Tuning Options
-
-These don't require code changes:
-
-| Config Change | Trade-off | Expected Impact |
-|---------------|-----------|-----------------|
-| `horizon: 5` (from 8) | Less planning depth | ~1.6x faster |
-| `batch_size: 512` (from 256) | More GPU memory | 1.1-1.3x faster |
-
----
-
-## Estimated Additional Speedup Potential
-
-| Optimization | Expected Speedup |
-|--------------|------------------|
-| Batch two_hot_inv | 1.1-1.2x |
-| torch.inference_mode() | 1.05-1.1x |
-| Gradient norm optimization | 1.05-1.1x |
-| Buffer sampling | 1.1-1.2x |
-| Fused Adam | 1.05-1.1x |
-| **Combined (multiplicative)** | **~1.3-1.5x additional** |
-
-With the current ~2x speedup already achieved, implementing these remaining optimizations could potentially yield **~2.5-3x total speedup** compared to the original baseline.
diff --git a/environment.yml b/environment.yml
index 8d40220..e1e2096 100644
--- a/environment.yml
+++ b/environment.yml
@@ -76,6 +76,7 @@ dependencies:
       - glob2==0.7
       - gymnasium==1.2.2
       - h11==0.16.0
+      - hockey==2.6
       - httpcore==1.0.9
       - idna==3.11
       - ipykernel==7.1.0
@@ -161,6 +162,7 @@ dependencies:
       - rfc3339-validator==0.1.4
       - rfc3986-validator==0.1.1
       - rfc3987-syntax==1.1.0
+      - rl-hockey==0.1.0
       - rpds-py==0.29.0
       - send2trash==1.8.3
       - sentry-sdk==2.46.0
diff --git a/resources/cluster/niklas/hyperparameter_training.sbatch b/hyperparameter_training.sbatch
similarity index 93%
rename from resources/cluster/niklas/hyperparameter_training.sbatch
rename to hyperparameter_training.sbatch
index dfacc13..627d40e 100644
--- a/resources/cluster/niklas/hyperparameter_training.sbatch
+++ b/hyperparameter_training.sbatch
@@ -33,8 +33,8 @@
 # your email - CHANGE THIS!
 
 # Activate the singularity container
-# Path to the Singularity container image
-SINGULARITY_IMAGE="$HOME/RL_CheungMaenzerAbraham_Hockey/singularity_build/rl_hockey.simg"
+# Adjust the path to your container image
+SINGULARITY_IMAGE="/path/to/your/container.sif"
 
 # Set working directory to your project directory
 # This should be the root of your RL_CheungMaenzerAbraham_Hockey project
diff --git a/requirements.txt b/requirements.txt
index d5ee4ce..5988a02 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,11 +1,3 @@
-Farama-Notifications==0.0.4
-GitPython==3.1.45
-Jinja2==3.1.6
-MarkupSafe==3.0.3
-PyYAML==6.0.3
-Pygments==2.19.2
-RapidFuzz==3.12.2
-Send2Trash==1.8.3
 annotated-types==0.7.0
 anyio==4.11.0
 argon2-cffi==25.1.0
@@ -35,22 +27,23 @@ defusedxml==0.7.1
 delegator.py==0.1.1
 exceptiongroup==1.3.1
 executing==2.2.1
+Farama-Notifications==0.0.4
 fastjsonschema==2.21.2
 filelock==3.20.0
 fonttools==4.60.1
 fqdn==1.5.1
 fsspec==2025.10.0
 gitdb==4.0.12
+GitPython==3.1.45
 glob2==0.7
 gymnasium==1.2.2
 h11==0.16.0
-hockey==2.6
+hockey @ git+https://github.com/martius-lab/laser-hockey-env.git@59dbc689d0e79fea30814389a2414f1c9bce5617
 httpcore==1.0.9
 httpx==0.28.1
 idna==3.11
 imageio==2.37.0
 imageio-ffmpeg==0.6.0
-importlib_metadata==8.7.1
 ipykernel==7.1.0
 ipympl==0.9.8
 ipython==8.37.0
@@ -58,6 +51,7 @@ ipywidgets==8.1.8
 isoduration==20.11.0
 isort==7.0.0
 jedi==0.19.2
+Jinja2==3.1.6
 json5==0.12.1
 jsonpointer==3.0.0
 jsonschema==4.25.1
@@ -76,6 +70,7 @@ jupyterlab_server==2.28.0
 jupyterlab_widgets==3.0.16
 kiwisolver==1.4.9
 lark==1.3.1
+MarkupSafe==3.0.3
 matplotlib==3.10.7
 matplotlib-inline==0.2.1
 mistune==3.1.4
@@ -105,7 +100,6 @@ nvidia-nccl-cu12==2.27.5
 nvidia-nvjitlink-cu12==12.8.93
 nvidia-nvshmem-cu12==3.3.20
 nvidia-nvtx-cu12==12.8.90
-orjson==3.11.5
 overrides==7.7.0
 packaging==25.0
 pandas==2.2.3
@@ -113,9 +107,8 @@ pandocfilters==1.5.1
 parso==0.8.5
 pathspec==0.12.1
 pexpect==4.9.0
-pillow==12.0.0
+pillow>=9.2.0,<12.0
 pink-noise-rl==2.0.1
-pip==25.1.1
 platformdirs==4.5.0
 proglog==0.1.12
 prometheus_client==0.23.1
@@ -129,6 +122,7 @@ pydantic==2.12.5
 pydantic_core==2.41.5
 pygame==2.6.1
 pyglet==1.5.0
+Pygments==2.19.2
 pyparsing==3.2.5
 pypdfium2==4.30.1
 python-dateutil==2.9.0.post0
@@ -136,18 +130,20 @@ python-dotenv==1.1.0
 python-json-logger==4.0.0
 pytokens==0.3.0
 pyttsx3==2.98
-pyvers==0.1.0
+PyYAML==6.0.3
 pyzmq==27.1.0
+RapidFuzz==3.12.2
 referencing==0.37.0
 requests==2.32.5
 requests-cache==1.2.1
 rfc3339-validator==0.1.4
 rfc3986-validator==0.1.1
 rfc3987-syntax==1.1.0
+-e git+ssh://git@github.com/NiklasAbraham/RL_CheungMaenzerAbraham_Hockey.git@4e121bd08e75daece5a67891c9344046ffe7339a#egg=rl_hockey
 rpds-py==0.29.0
 scipy==1.15.2
+Send2Trash==1.8.3
 sentry-sdk==2.46.0
-setuptools==79.0.1
 six==1.17.0
 smmap==5.0.2
 sniffio==1.3.1
@@ -155,18 +151,17 @@ soupsieve==2.8
 stack-data==0.6.3
 sympy==1.14.0
 tensorboardX==2.6.4
-tensordict==0.10.0
 terminado==0.18.1
 timple==0.1.8
 tinycss2==1.4.0
 tomli==2.3.0
-torch==2.9.1
-torchaudio==2.9.1
-torchvision==0.24.1
+torch==2.7.1
+torchaudio==2.7.1
+torchvision==0.22.1
 tornado==6.5.2
 tqdm==4.67.1
 traitlets==5.14.3
-triton==3.5.1
+triton==3.3.1
 typing-inspection==0.4.2
 typing_extensions==4.15.0
 tzdata==2025.2
@@ -179,6 +174,4 @@ webcolors==25.10.0
 webencodings==0.5.1
 websocket-client==1.9.0
 websockets==13.1
-wheel==0.45.1
 widgetsnbextension==4.0.15
-zipp==3.23.0
diff --git a/resources/info/TCML_Documentation_2025-10.28.pdf b/resources/TCML_Documentation_2025-10.28.pdf
similarity index 100%
rename from resources/info/TCML_Documentation_2025-10.28.pdf
rename to resources/TCML_Documentation_2025-10.28.pdf
diff --git a/resources/info/TCML_SETUP_GUIDE.md b/resources/TCML_SETUP_GUIDE.md
similarity index 100%
rename from resources/info/TCML_SETUP_GUIDE.md
rename to resources/TCML_SETUP_GUIDE.md
diff --git a/resources/cluster/jannik/cluster_setup.sh b/resources/cluster/jannik/cluster_setup.sh
deleted file mode 100644
index c08ae85..0000000
--- a/resources/cluster/jannik/cluster_setup.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/bin/bash
-# Setup script to run on the TCML cluster after syncing files
-# Run this script on the cluster: bash resources/cluster_setup.sh
-
-set -e
-
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-echo "Setting up RL Hockey project on TCML cluster..."
-echo ""
-
-# Create singularity build directory
-echo "Creating singularity build directory..."
-mkdir -p singularity_build
-cd singularity_build
-
-# Copy container definition
-echo "Copying container definition..."
-cp ../resources/container/container.def ./rl_hockey.def
-
-# Copy requirements.txt to build directory
-echo "Copying requirements.txt..."
-cp ../requirements.txt ./requirements.txt
-
-# Verify files exist before building
-echo ""
-echo "Verifying files are in place for build:"
-if [ ! -f "./rl_hockey.def" ]; then
-    echo "ERROR: rl_hockey.def not found!"
-    exit 1
-fi
-echo "✓ rl_hockey.def found"
-
-if [ ! -f "./requirements.txt" ]; then
-    echo "ERROR: requirements.txt not found!"
-    exit 1
-fi
-echo "✓ requirements.txt found in project root ($(wc -l < ../requirements.txt) lines)"
-
-# Show files in build directory
-echo "Files in build directory:"
-ls -lh
-
-# Build the container
-echo ""
-echo "Building Singularity container (this may take 10-30 minutes)..."
-echo "This will install all packages from requirements.txt into the container."
-singularity build --fakeroot rl_hockey.simg rl_hockey.def
-
-echo ""
-echo "Container built successfully!"
-echo ""
-echo "Container location: $PROJECT_DIR/singularity_build/rl_hockey.simg"
-echo ""
-echo "Next steps:"
-echo "1. Update resources/train_single_run.sbatch if needed (check paths and email)"
-echo "2. Submit the job: sbatch resources/train_single_run.sbatch"
-echo "3. Monitor with: squeue -u \$USER"
diff --git a/resources/cluster/jannik/container.def b/resources/cluster/jannik/container.def
deleted file mode 100644
index 67915c4..0000000
--- a/resources/cluster/jannik/container.def
+++ /dev/null
@@ -1,62 +0,0 @@
-bootstrap: docker
-from: ubuntu:24.04
-
-%files
-    requirements.txt /opt/requirements.txt
-
-%post
-    set -e
-    export DEBIAN_FRONTEND=noninteractive
-
-    echo "deb http://archive.ubuntu.com/ubuntu noble universe" >> /etc/apt/sources.list
-    apt-get update
-
-    apt-get install -y python3-dev python3-pip python3-full swig git
-
-    python3 -m venv /venv
-    . /venv/bin/activate
-
-    # Install setuptools and wheel first
-    pip install --upgrade pip setuptools wheel
-
-    # Install PyTorch with CUDA 12.6 (supports compute capability 6.1 for GTX 1080 Ti)
-    pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126
-
-    # Install hockey environment (must be done separately as it's from git)
-    pip install "hockey @ git+https://git@github.com/martius-lab/hockey-env.git"
-
-    # Install requirements from /opt/requirements.txt (copied via %files section)
-    echo "Checking for requirements.txt..."
-    
-    if [ -f /opt/requirements.txt ]; then
-        echo "✓ Found requirements.txt at /opt/requirements.txt"
-        echo "Total lines in requirements.txt: $(wc -l < /opt/requirements.txt)"
-        echo "Filtering out rl_hockey, hockey, torch packages, triton, and CUDA packages..."
-        
-        # Filter requirements and save to /opt/filtered_requirements.txt
-        grep -v "rl_hockey" /opt/requirements.txt | \
-        grep -v "^hockey" | \
-        grep -v "^torch" | \
-        grep -v "^triton" | \
-        grep -v "^nvidia-" > /opt/filtered_requirements.txt
-        
-        echo "Packages to install: $(wc -l < /opt/filtered_requirements.txt)"
-        echo "First few packages to install:"
-        head -5 /opt/filtered_requirements.txt
-        echo ""
-        echo "Installing packages from requirements.txt..."
-        pip install -r /opt/filtered_requirements.txt
-        echo "✓ Finished installing requirements.txt packages"
-        rm -f /opt/filtered_requirements.txt
-    else
-        echo "ERROR: requirements.txt not found at /opt/requirements.txt"
-        exit 1
-    fi
-
-    # cleanup
-    apt-get clean
-    pip cache purge
-
-%runscript
-    . /venv/bin/activate
-    "$@"
diff --git a/resources/cluster/jannik/hyperparameter_training.sbatch b/resources/cluster/jannik/hyperparameter_training.sbatch
deleted file mode 100644
index 2d36a15..0000000
--- a/resources/cluster/jannik/hyperparameter_training.sbatch
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/bin/bash
-
-#SBATCH --job-name=hockey_hyperparam
-# give it any name you want
-
-#SBATCH --cpus-per-task=24
-# max 24 per node, using maximum for best performance
-
-#SBATCH --partition=week
-# choose out of day, week, month depending on job duration
-# This job tests 180 configurations, each with 3000 episodes, so week partition is recommended
-
-#SBATCH --mem-per-cpu=2G
-# max 251GB per node, using 48GB total (24 CPUs * 2GB)
-
-# GPU not requested - focusing on maximum CPU cores
-
-#SBATCH --time=7-00:00:00
-# job length: 7 days (week partition limit)
-# The job will run either until completion or until this timer runs out
-
-#SBATCH --error=job.%J.err
-# %J is the job ID, errors will be written to this file
-
-#SBATCH --output=job.%J.out
-# the output will be written in this file
-
-#SBATCH --mail-type=ALL
-# write a mail if a job begins, ends, fails, gets requeued or stages out
-# options: NONE, BEGIN, END, FAIL, REQUEUE, ALL
-
-#SBATCH --mail-user=jannik.maenzer@uni-tuebingen.de
-# your email - CHANGE THIS!
-
-# Activate the singularity container
-# Path to the Singularity container image
-SINGULARITY_IMAGE="$HOME/RL_CheungMaenzerAbraham_Hockey/singularity_build/rl_hockey.simg"
-
-# Set working directory to your project directory
-# This should be the root of your RL_CheungMaenzerAbraham_Hockey project
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-# Install the rl_hockey package in editable mode (if not already installed in container)
-# This allows code updates without rebuilding the container
-singularity exec \
-    --bind "$PROJECT_DIR:$PROJECT_DIR" \
-    --pwd "$PROJECT_DIR" \
-    "$SINGULARITY_IMAGE" \
-    pip install -e "$PROJECT_DIR" --quiet
-
-# Run the hyperparameter training script using singularity
-singularity run \
-    --bind "$PROJECT_DIR:$PROJECT_DIR" \
-    --pwd "$PROJECT_DIR" \
-    "$SINGULARITY_IMAGE" \
-    python3 "$PROJECT_DIR/src/rl_hockey/common/training/shooting_training_simple_hyperparameters.py" \
-    --num_parallel 8 \
-    --output_dir results/hyperparameter_runs
diff --git a/resources/cluster/niklas/cluster_setup.sh b/resources/cluster/niklas/cluster_setup.sh
deleted file mode 100644
index c08ae85..0000000
--- a/resources/cluster/niklas/cluster_setup.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/bin/bash
-# Setup script to run on the TCML cluster after syncing files
-# Run this script on the cluster: bash resources/cluster_setup.sh
-
-set -e
-
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-echo "Setting up RL Hockey project on TCML cluster..."
-echo ""
-
-# Create singularity build directory
-echo "Creating singularity build directory..."
-mkdir -p singularity_build
-cd singularity_build
-
-# Copy container definition
-echo "Copying container definition..."
-cp ../resources/container/container.def ./rl_hockey.def
-
-# Copy requirements.txt to build directory
-echo "Copying requirements.txt..."
-cp ../requirements.txt ./requirements.txt
-
-# Verify files exist before building
-echo ""
-echo "Verifying files are in place for build:"
-if [ ! -f "./rl_hockey.def" ]; then
-    echo "ERROR: rl_hockey.def not found!"
-    exit 1
-fi
-echo "✓ rl_hockey.def found"
-
-if [ ! -f "./requirements.txt" ]; then
-    echo "ERROR: requirements.txt not found!"
-    exit 1
-fi
-echo "✓ requirements.txt found in project root ($(wc -l < ../requirements.txt) lines)"
-
-# Show files in build directory
-echo "Files in build directory:"
-ls -lh
-
-# Build the container
-echo ""
-echo "Building Singularity container (this may take 10-30 minutes)..."
-echo "This will install all packages from requirements.txt into the container."
-singularity build --fakeroot rl_hockey.simg rl_hockey.def
-
-echo ""
-echo "Container built successfully!"
-echo ""
-echo "Container location: $PROJECT_DIR/singularity_build/rl_hockey.simg"
-echo ""
-echo "Next steps:"
-echo "1. Update resources/train_single_run.sbatch if needed (check paths and email)"
-echo "2. Submit the job: sbatch resources/train_single_run.sbatch"
-echo "3. Monitor with: squeue -u \$USER"
diff --git a/resources/cluster/niklas/download_latest_run.sh b/resources/cluster/niklas/download_latest_run.sh
deleted file mode 100644
index 109cd93..0000000
--- a/resources/cluster/niklas/download_latest_run.sh
+++ /dev/null
@@ -1,74 +0,0 @@
-#!/bin/bash
-
-# Script to download the latest 5 tdmpc2 runs from the cluster
-# Usage: ./resources/download_latest_run.sh [server_address]
-
-# Configuration
-SERVER="${1:-tcml-login1}"
-# SSH config should handle the full hostname mapping
-REMOTE_RUNS_DIR="/home/stud421/RL_CheungMaenzerAbraham_Hockey/results/tdmpc2_runs"
-LOCAL_PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
-LOCAL_RUNS_DIR="${LOCAL_PROJECT_DIR}/results/tdmpc2_runs"
-
-echo "Connecting to ${SERVER}..."
-echo "Remote runs directory: ${REMOTE_RUNS_DIR}"
-echo "Local runs directory: ${LOCAL_RUNS_DIR}"
-
-# Find the latest 5 timestamped directories on the remote server
-echo ""
-echo "Finding latest 30 tdmpc2 run directories..."
-LATEST_DIRS=$(ssh "${SERVER}" "cd ${REMOTE_RUNS_DIR} && ls -td */ 2>/dev/null | head -30 | sed 's|/$||'")
-
-if [ -z "${LATEST_DIRS}" ]; then
-    echo "Error: No tdmpc2 run directories found on remote server."
-    exit 1
-fi
-
-# Count how many directories were found
-DIR_COUNT=$(echo "${LATEST_DIRS}" | wc -l)
-echo "Found ${DIR_COUNT} directory/directories to download:"
-echo "${LATEST_DIRS}" | nl
-
-# Create local directory if it doesn't exist
-mkdir -p "${LOCAL_RUNS_DIR}"
-
-# Download each directory using rsync (more efficient than scp)
-echo ""
-echo "Starting download of ${DIR_COUNT} run(s)..."
-echo "This may take a while depending on the size of the run data..."
-echo ""
-
-SUCCESS_COUNT=0
-FAILED_COUNT=0
-
-while IFS= read -r DIR; do
-    if [ -n "${DIR}" ]; then
-        echo "=========================================="
-        echo "Downloading: ${DIR}"
-        echo "=========================================="
-        
-        rsync -avz --progress "${SERVER}:${REMOTE_RUNS_DIR}/${DIR}" "${LOCAL_RUNS_DIR}/"
-        
-        if [ $? -eq 0 ]; then
-            echo ""
-            echo "Successfully downloaded: ${DIR}"
-            echo "Local path: ${LOCAL_RUNS_DIR}/${DIR}"
-            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
-        else
-            echo ""
-            echo "Error: Failed to download: ${DIR}"
-            FAILED_COUNT=$((FAILED_COUNT + 1))
-        fi
-        echo ""
-    fi
-done <<< "${LATEST_DIRS}"
-
-echo "=========================================="
-echo "Download summary:"
-echo "  Successful: ${SUCCESS_COUNT}"
-echo "  Failed: ${FAILED_COUNT}"
-echo "=========================================="
-
-if [ ${FAILED_COUNT} -gt 0 ]; then
-    exit 1
-fi
diff --git a/resources/cluster/niklas/download_videos.sh b/resources/cluster/niklas/download_videos.sh
deleted file mode 100755
index 1637251..0000000
--- a/resources/cluster/niklas/download_videos.sh
+++ /dev/null
@@ -1,82 +0,0 @@
-#!/bin/bash
-
-# Script to download videos from the cluster
-# Usage: ./resources/download_videos.sh [server_address] [num_videos]
-
-# Configuration
-SERVER="${1:-tcml-login1}"
-NUM_VIDEOS="${2:-5}"
-# SSH config should handle the full hostname mapping
-REMOTE_VIDEOS_DIR="/home/stud421/RL_CheungMaenzerAbraham_Hockey/videos"
-LOCAL_PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
-LOCAL_VIDEOS_DIR="${LOCAL_PROJECT_DIR}/videos"
-
-echo "Connecting to ${SERVER}..."
-echo "Remote videos directory: ${REMOTE_VIDEOS_DIR}"
-echo "Local videos directory: ${LOCAL_VIDEOS_DIR}"
-
-# Find the latest video files on the remote server
-echo ""
-echo "Finding latest ${NUM_VIDEOS} video file(s)..."
-LATEST_VIDEOS=$(ssh "${SERVER}" "cd ${REMOTE_VIDEOS_DIR} && ls -t *.mp4 2>/dev/null | head -${NUM_VIDEOS}")
-
-if [ -z "${LATEST_VIDEOS}" ]; then
-    echo "Error: No video files (.mp4) found on remote server."
-    echo "Checking if videos directory exists..."
-    ssh "${SERVER}" "ls -ld ${REMOTE_VIDEOS_DIR} 2>/dev/null || echo 'Directory does not exist'"
-    exit 1
-fi
-
-# Count how many videos were found
-VIDEO_COUNT=$(echo "${LATEST_VIDEOS}" | wc -l)
-echo "Found ${VIDEO_COUNT} video file(s) to download:"
-echo "${LATEST_VIDEOS}" | nl
-
-# Create local directory if it doesn't exist
-mkdir -p "${LOCAL_VIDEOS_DIR}"
-
-# Download each video file using rsync (more efficient than scp)
-echo ""
-echo "Starting download of ${VIDEO_COUNT} video(s)..."
-echo "This may take a while depending on the file size..."
-echo ""
-
-SUCCESS_COUNT=0
-FAILED_COUNT=0
-
-while IFS= read -r VIDEO; do
-    if [ -n "${VIDEO}" ]; then
-        echo "=========================================="
-        echo "Downloading: ${VIDEO}"
-        echo "=========================================="
-        
-        rsync -avz --progress "${SERVER}:${REMOTE_VIDEOS_DIR}/${VIDEO}" "${LOCAL_VIDEOS_DIR}/"
-        
-        if [ $? -eq 0 ]; then
-            echo ""
-            echo "Successfully downloaded: ${VIDEO}"
-            echo "Local path: ${LOCAL_VIDEOS_DIR}/${VIDEO}"
-            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
-        else
-            echo ""
-            echo "Error: Failed to download: ${VIDEO}"
-            FAILED_COUNT=$((FAILED_COUNT + 1))
-        fi
-        echo ""
-    fi
-done <<< "${LATEST_VIDEOS}"
-
-echo "=========================================="
-echo "Download summary:"
-echo "  Successful: ${SUCCESS_COUNT}"
-echo "  Failed: ${FAILED_COUNT}"
-echo "=========================================="
-
-if [ ${SUCCESS_COUNT} -gt 0 ]; then
-    echo ""
-    echo "Videos downloaded to: ${LOCAL_VIDEOS_DIR}"
-fi
-
-if [ ${FAILED_COUNT} -gt 0 ]; then
-    exit 1
-fi
diff --git a/resources/cluster/niklas/generate_clean_requirements.py b/resources/cluster/niklas/generate_clean_requirements.py
deleted file mode 100755
index 673c0d5..0000000
--- a/resources/cluster/niklas/generate_clean_requirements.py
+++ /dev/null
@@ -1,247 +0,0 @@
-#!/usr/bin/env python3
-"""
-Generate a clean requirements.txt from your conda environment that will work on the server.
-
-This script:
-1. Extracts only packages installed via pip (not conda)
-2. Validates packages exist on PyPI
-3. Generates a clean requirements.txt
-4. Optionally tests installation in a virtual environment
-"""
-
-import json
-import re
-import shutil
-import subprocess
-import sys
-from pathlib import Path
-from typing import List, Tuple
-
-
-def get_pip_installed_packages() -> List[Tuple[str, str]]:
-    """Get packages installed via pip (not conda)."""
-    try:
-        result = subprocess.run(
-            [sys.executable, "-m", "pip", "list", "--format=json"],
-            capture_output=True,
-            text=True,
-            check=True,
-        )
-        packages = json.loads(result.stdout)
-        return [(pkg["name"], pkg["version"]) for pkg in packages]
-    except Exception as e:
-        print(f"Error getting pip packages: {e}")
-        return []
-
-
-def check_package_on_pypi(package_name: str, version: str = None) -> Tuple[bool, str]:
-    """Check if a package exists on PyPI and if the version is available."""
-    try:
-        # Use pip index to check package availability
-        cmd = [sys.executable, "-m", "pip", "index", "versions", package_name]
-        result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)
-
-        if result.returncode == 0:
-            output = result.stdout
-            if version:
-                # Check if specific version exists
-                if f"=={version}" in output or version in output:
-                    return True, f"Version {version} available"
-                else:
-                    # Extract available versions
-                    versions = re.findall(r"(\d+\.\d+\.\d+)", output)
-                    if versions:
-                        return (
-                            False,
-                            f"Version {version} not found. Available: {versions[0]} (latest)",
-                        )
-                    return False, f"Version {version} not found"
-            return True, "Package available"
-        else:
-            return False, "Not found on PyPI"
-    except subprocess.TimeoutExpired:
-        return False, "Timeout checking PyPI"
-    except Exception as e:
-        # Fallback: try to install the package to see if it exists
-        try:
-            cmd = [
-                sys.executable,
-                "-m",
-                "pip",
-                "install",
-                f"{package_name}=={version}" if version else package_name,
-                "--dry-run",
-            ]
-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
-            if "ERROR" in result.stderr or "Could not find" in result.stderr:
-                return False, "Not found on PyPI"
-            return True, "Package available"
-        except:
-            return False, f"Error: {str(e)}"
-
-
-def is_system_package(package_name: str) -> bool:
-    """Check if a package is likely a system package (not on PyPI)."""
-    system_packages = {
-        "Brlapi",
-        "cloud-init",
-        "ubuntu-drivers-common",
-        "ubuntu-pro-client",
-        "ufw",
-        "unattended-upgrades",
-        "usb-creator",
-        "command-not-found",
-        "language-selector",
-        "python-apt",
-        "python-debian",
-        "systemd-python",
-        "distro-info",
-        "xkit",
-        "cupshelpers",
-        "dbus-python",
-        "duplicity",
-        "eduvpn-client",
-        "eduvpn-common",
-        "gpg",
-        "launchpadlib",
-        "louis",
-        "pycairo",
-        "pycups",
-        "PyGObject",
-        "jeepney",
-        "defer",
-    }
-    return package_name in system_packages
-
-
-def should_exclude_package(package_name: str, version: str) -> Tuple[bool, str]:
-    """Determine if a package should be excluded and why."""
-    # Exclude system packages
-    if is_system_package(package_name):
-        return True, "System package (not on PyPI)"
-
-    # Exclude packages with version 0.0.0 (often system packages)
-    if version == "0.0.0":
-        return True, "Version 0.0.0 (likely system package)"
-
-    # Exclude editable installs (they're local)
-    return False, ""
-
-
-def generate_clean_requirements(
-    output_file: str = "requirements_clean.txt", validate: bool = True
-) -> None:
-    """Generate a clean requirements.txt file."""
-    print("Generating clean requirements.txt from current environment...")
-    print("=" * 80)
-
-    # Get all pip-installed packages
-    print("\n1. Getting pip-installed packages...")
-    packages = get_pip_installed_packages()
-    print(f"   Found {len(packages)} packages installed via pip")
-
-    # Filter and validate packages
-    print("\n2. Validating packages...")
-    valid_packages = []
-    excluded_packages = []
-    failed_packages = []
-
-    for package_name, version in packages:
-        # Skip if should be excluded
-        should_exclude, reason = should_exclude_package(package_name, version)
-        if should_exclude:
-            excluded_packages.append((package_name, version, reason))
-            continue
-
-        # Validate on PyPI if requested
-        if validate:
-            exists, message = check_package_on_pypi(package_name, version)
-            if exists:
-                valid_packages.append((package_name, version))
-                print(f"   ✓ {package_name}=={version}")
-            else:
-                failed_packages.append((package_name, version, message))
-                print(f"   ✗ {package_name}=={version} - {message}")
-        else:
-            valid_packages.append((package_name, version))
-            print(f"   ✓ {package_name}=={version}")
-
-    # Write clean requirements.txt
-    print(f"\n3. Writing clean requirements to {output_file}...")
-    with open(output_file, "w") as f:
-        for package_name, version in sorted(valid_packages):
-            f.write(f"{package_name}=={version}\n")
-
-    # Summary
-    print("\n" + "=" * 80)
-    print("SUMMARY")
-    print("=" * 80)
-    print(f"Total packages found: {len(packages)}")
-    print(f"Valid packages (written to {output_file}): {len(valid_packages)}")
-    print(f"Excluded packages (system/conda): {len(excluded_packages)}")
-    if failed_packages:
-        print(f"Failed validation (not on PyPI): {len(failed_packages)}")
-
-    if excluded_packages:
-        print("\nExcluded packages:")
-        for pkg, ver, reason in excluded_packages[:10]:  # Show first 10
-            print(f"  - {pkg}=={ver} ({reason})")
-        if len(excluded_packages) > 10:
-            print(f"  ... and {len(excluded_packages) - 10} more")
-
-    if failed_packages:
-        print("\nPackages not found on PyPI:")
-        for pkg, ver, msg in failed_packages:
-            print(f"  - {pkg}=={ver}: {msg}")
-
-    print(f"\n✓ Clean requirements.txt generated: {output_file}")
-    print(
-        f"  This file contains {len(valid_packages)} packages that are available on PyPI"
-    )
-
-
-def main():
-    import argparse
-
-    parser = argparse.ArgumentParser(
-        description="Generate a clean requirements.txt from conda environment"
-    )
-    parser.add_argument(
-        "-o",
-        "--output",
-        default="requirements_clean.txt",
-        help="Output file name (default: requirements_clean.txt)",
-    )
-    parser.add_argument(
-        "--no-validate",
-        action="store_true",
-        help="Skip PyPI validation (faster but less reliable)",
-    )
-    parser.add_argument(
-        "--test",
-        action="store_true",
-        help="Test installation in a clean virtual environment",
-    )
-    parser.add_argument(
-        "--replace",
-        action="store_true",
-        help="Replace existing requirements.txt (backup will be created)",
-    )
-
-    args = parser.parse_args()
-
-    # Backup existing requirements.txt if replacing
-    if args.replace:
-        original_file = "requirements.txt"
-        if Path(original_file).exists():
-            backup_file = f"{original_file}.backup"
-            print(f"Backing up {original_file} to {backup_file}...")
-            shutil.copy(original_file, backup_file)
-            args.output = original_file
-
-    # Generate clean requirements
-    generate_clean_requirements(output_file=args.output, validate=not args.no_validate)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/resources/cluster/niklas/profile_tdmpc2.sbatch b/resources/cluster/niklas/profile_tdmpc2.sbatch
deleted file mode 100755
index bbbf9a8..0000000
--- a/resources/cluster/niklas/profile_tdmpc2.sbatch
+++ /dev/null
@@ -1,66 +0,0 @@
-#!/bin/bash
-
-#SBATCH --job-name=tdmpc2_profile
-# Job name
-
-#SBATCH --partition=day
-# Partition: day for quick profiling jobs
-
-#SBATCH --gres=gpu:2080ti:1
-# Request 1 A4000 GPU (16GB memory, CUDA capability 8.6, supports torch.compile)
-# A4000 has more memory than 2080ti (16GB vs 11GB) - better for profiling with memory tracking
-# Alternative options: gpu:2080ti:1 (11GB) or gpu:1080ti:1 (11GB, CUDA 6.1, no torch.compile)
-
-#SBATCH --mem=64G
-# Request 64GB of memory (torch.profiler is very memory-intensive, especially with profile_memory=True)
-
-#SBATCH --time=02:00:00
-# Time limit: 2 hours should be plenty for profiling
-
-#SBATCH --error=profile.%J.err
-# Error log file
-
-#SBATCH --output=profile.%J.out
-# Output log file
-
-#SBATCH --mail-type=END,FAIL
-# Email notifications when job ends or fails
-
-#SBATCH --mail-user=blabla@student.uni-tuebingen.de
-# Your email address (change this to your email)
-
-# Path to the Singularity container image
-SINGULARITY_IMAGE="$HOME/RL_CheungMaenzerAbraham_Hockey/singularity_build/rl_hockey.simg"
-
-# Set working directory to project root
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-# Configuration options (modify these as needed)
-CONFIG_PATH="$PROJECT_DIR/configs/curriculum_tdmpc2.json"
-OUTPUT_DIR="$PROJECT_DIR/results/profiling"
-NUM_ITERATIONS=50
-# Reduced from 100 to save memory - torch.profiler uses significant memory per iteration
-EXPORT_TRACES=""
-export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
-# Set EXPORT_TRACES="--export-traces" if you want Chrome trace files
-
-# Create output directory
-mkdir -p "$OUTPUT_DIR"
-
-# Run the profiling script using singularity
-singularity exec \
-    --nv \
-    --bind "$PROJECT_DIR:$PROJECT_DIR" \
-    --pwd "$PROJECT_DIR" \
-    "$SINGULARITY_IMAGE" \
-    /bin/bash -c "source /venv/bin/activate && export PYTHONPATH='$PROJECT_DIR/src:\$PYTHONPATH' && python3 '$PROJECT_DIR/src/rl_hockey/TD_MPC2/profile_tdmpc2.py' --config '$CONFIG_PATH' --output-dir '$OUTPUT_DIR' --num-iterations $NUM_ITERATIONS $EXPORT_TRACES"
-
-# Print completion message
-echo ""
-echo "Profiling complete! Check the output above for performance analysis."
-echo "Output directory: $OUTPUT_DIR"
-if [ -n "$EXPORT_TRACES" ]; then
-    echo "Trace files saved to: $OUTPUT_DIR/*.json"
-    echo "Open in Chrome: chrome://tracing"
-fi
\ No newline at end of file
diff --git a/resources/cluster/niklas/quick_fix_requirements.sh b/resources/cluster/niklas/quick_fix_requirements.sh
deleted file mode 100755
index 44a9840..0000000
--- a/resources/cluster/niklas/quick_fix_requirements.sh
+++ /dev/null
@@ -1,40 +0,0 @@
-#!/bin/bash
-# Quick script to generate clean requirements.txt from conda environment
-# Usage: bash resources/quick_fix_requirements.sh
-
-set -e
-
-echo "=========================================="
-echo "Generating Clean Requirements.txt"
-echo "=========================================="
-echo ""
-
-# Check if conda is available
-if ! command -v conda &> /dev/null; then
-    echo "ERROR: conda not found. Please activate your conda environment first."
-    exit 1
-fi
-
-# Check if we're in a conda environment
-if [ -z "$CONDA_DEFAULT_ENV" ]; then
-    echo "WARNING: No conda environment detected."
-    echo "Please activate your conda environment first:"
-    echo "  conda activate rl-hockey"
-    exit 1
-fi
-
-echo "Current conda environment: $CONDA_DEFAULT_ENV"
-echo ""
-
-# Run the Python script
-python resources/generate_clean_requirements.py --replace --test
-
-echo ""
-echo "=========================================="
-echo "Done! Your requirements.txt has been updated."
-echo "=========================================="
-echo ""
-echo "Next steps:"
-echo "1. Review the generated requirements.txt"
-echo "2. Rebuild your container: bash resources/cluster_setup.sh"
-echo ""
diff --git a/resources/cluster/niklas/run_agent_to_mp4.sbatch b/resources/cluster/niklas/run_agent_to_mp4.sbatch
deleted file mode 100644
index 3b1f676..0000000
--- a/resources/cluster/niklas/run_agent_to_mp4.sbatch
+++ /dev/null
@@ -1,50 +0,0 @@
-#!/bin/bash
-
-#SBATCH --job-name=hockey_agent_mp4
-# Job name
-
-#SBATCH --partition=week
-# Partition: day, week, or month
-
-#SBATCH --gres=gpu:2080ti:1
-# Request 1 A4000 GPU (CUDA capability 8.6, supports torch.compile)
-# Alternative options: gpu:A4000:1 (CUDA 8.6) or gpu:1080ti:1 (CUDA 6.1, no torch.compile)
-
-#SBATCH --mem=32G
-# Request 32GB of memory
-
-#SBATCH --time=12:00:00
-# Time limit: 12 hours should be enough for 25 games + video encoding
-
-#SBATCH --error=job.%J.err
-# Error log file
-
-#SBATCH --output=job.%J.out
-# Output log file
-
-#SBATCH --mail-type=ALL
-# Email notifications: NONE, BEGIN, END, FAIL, REQUEUE, ALL
-
-#SBATCH --mail-user=blabla@student.uni-tuebingen.de
-# Your email address
-
-# Path to the Singularity container image
-SINGULARITY_IMAGE="$HOME/RL_CheungMaenzerAbraham_Hockey/singularity_build/rl_hockey.simg"
-
-# Set working directory to project root
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-# Set MODEL_PATH to the most recent model file
-# Update this path to use a different model if needed
-export MODEL_PATH="$PROJECT_DIR/results/tdmpc2_runs/2026-01-22_08-42-36/models/TDMPC2_run_lr3e04_bs512_hencoder_dynamics_reward_termination_q_function_policy_cfce4de1_20260121_161543_ep003700.pt"
-
-# Run the agent video script using singularity
-# Since the container is read-only, we can't install packages into /venv
-# Instead, we add the src directory to PYTHONPATH so Python can find rl_hockey
-singularity exec \
-    --nv \
-    --bind "$PROJECT_DIR:$PROJECT_DIR" \
-    --pwd "$PROJECT_DIR" \
-    "$SINGULARITY_IMAGE" \
-    /bin/bash -c "source /venv/bin/activate && export PROJECT_DIR='$PROJECT_DIR' && export MODEL_PATH='$MODEL_PATH' && export PYTHONPATH='$PROJECT_DIR/src:\$PYTHONPATH' && python3 '$PROJECT_DIR/src/rl_hockey/scripts/run_agent_to_mp4.py'"
diff --git a/resources/cluster/niklas/train_continue_run.sbatch b/resources/cluster/niklas/train_continue_run.sbatch
deleted file mode 100644
index 916f0ef..0000000
--- a/resources/cluster/niklas/train_continue_run.sbatch
+++ /dev/null
@@ -1,70 +0,0 @@
-#!/bin/bash
-
-#SBATCH --job-name=hockey_continue_run
-# Job name
-
-#SBATCH --partition=week
-# Partition: day, week, or month
-
-#SBATCH --gres=gpu:A4000:1
-# Request 1 2080ti GPU (CUDA capability 7.5, supports torch.compile)
-# Alternative options: gpu:A4000:1 (CUDA 8.6) or gpu:1080ti:1 (CUDA 6.1, no torch.compile)
-
-#SBATCH --mem=64G
-# Request 32GB of memory (16 parallel environments + replay buffer need more memory)
-
-#SBATCH --time=7-00:00:00
-# Time limit: 7 days for week partition
-
-#SBATCH --error=job.%J.err
-# Error log file
-
-#SBATCH --output=job.%J.out
-# Output log file
-
-#SBATCH --mail-type=ALL
-# Email notifications: NONE, BEGIN, END, FAIL, REQUEUE, ALL
-
-#SBATCH --mail-user=blabla@student.uni-tuebingen.de
-# Your email address
-
-# Path to the Singularity container image
-SINGULARITY_IMAGE="$HOME/RL_CheungMaenzerAbraham_Hockey/singularity_build/rl_hockey.simg"
-
-# Set working directory to project root
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-# Set number of parallel environments to match CPU cores
-export NUM_ENVS=1
-
-# Set the path to the run directory to resume from
-# Example: results/tdmpc2_runs/2026-01-21_19-12-44
-# Modify this to point to your specific run directory
-export RESUME_PATH="results/tdmpc2_runs/2026-01-23_21-00-09"
-
-# Disable Python output buffering for immediate logging in batch jobs
-export PYTHONUNBUFFERED=1
-
-# Run the training script using singularity
-# Since the container is read-only, we can't install packages into /venv
-# Install missing packages to a user-writable location if needed
-USER_PACKAGES_DIR="$PROJECT_DIR/.user_packages"
-mkdir -p "$USER_PACKAGES_DIR"
-
-# Run the training script using singularity
-singularity exec \
-    --nv \
-    --bind "$PROJECT_DIR:$PROJECT_DIR" \
-    --pwd "$PROJECT_DIR" \
-    "$SINGULARITY_IMAGE" \
-    /bin/bash -c "source /venv/bin/activate && pip install --target '$USER_PACKAGES_DIR' psutil > /dev/null 2>&1 && export NUM_ENVS=$NUM_ENVS && export RESUME_PATH='$RESUME_PATH' && export PYTHONUNBUFFERED=1 && export PYTHONPATH='$PROJECT_DIR/src:$USER_PACKAGES_DIR:\$PYTHONPATH' && python3 -u '$PROJECT_DIR/src/rl_hockey/common/training/train_continue_run.py'"
-
-# Alternative: Run with custom parameters by modifying train_continue_run.py
-# Or uncomment and modify the following line to pass parameters directly:
-# singularity run \
-#     --nv \
-#     --bind "$PROJECT_DIR:$PROJECT_DIR" \
-#     --pwd "$PROJECT_DIR" \
-#     "$SINGULARITY_IMAGE" \
-#     python3 -c "from rl_hockey.common.training.train_continue_run import train_continue_run; train_continue_run(base_path='results/tdmpc2_runs/2026-01-21_19-12-44', base_output_dir='results/tdmpc2_runs', num_envs=4, device='cuda:0')"
diff --git a/resources/cluster/niklas/train_single_run.sbatch b/resources/cluster/niklas/train_single_run.sbatch
deleted file mode 100644
index 550ae43..0000000
--- a/resources/cluster/niklas/train_single_run.sbatch
+++ /dev/null
@@ -1,65 +0,0 @@
-#!/bin/bash
-
-#SBATCH --job-name=hockey_single_run
-# Job name
-
-#SBATCH --partition=week
-# Partition: day, week, or month
-
-#SBATCH --gres=gpu:2080ti:1
-# Request 1 2080ti GPU (CUDA capability 7.5, supports torch.compile)
-# Alternative options: gpu:A4000:1 (CUDA 8.6) or gpu:1080ti:1 (CUDA 6.1, no torch.compile)
-
-#SBATCH --mem=32G
-# Request 32GB of memory (16 parallel environments + replay buffer need more memory)
-
-#SBATCH --time=7-00:00:00
-# Time limit: 7 days for week partition
-
-#SBATCH --error=job.%J.err
-# Error log file
-
-#SBATCH --output=job.%J.out
-# Output log file
-
-#SBATCH --mail-type=ALL
-# Email notifications: NONE, BEGIN, END, FAIL, REQUEUE, ALL
-
-#SBATCH --mail-user=blabla@student.uni-tuebingen.de
-# Your email address
-
-# Path to the Singularity container image
-SINGULARITY_IMAGE="$HOME/RL_CheungMaenzerAbraham_Hockey/singularity_build/rl_hockey.simg"
-
-# Set working directory to project root
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-# Set number of parallel environments to match CPU cores
-export NUM_ENVS=1
-
-# Disable Python output buffering for immediate logging in batch jobs
-export PYTHONUNBUFFERED=1
-
-# Run the training script using singularity
-# Since the container is read-only, we can't install packages into /venv
-# Install missing packages to a user-writable location if needed
-USER_PACKAGES_DIR="$PROJECT_DIR/.user_packages"
-mkdir -p "$USER_PACKAGES_DIR"
-
-# Run the training script using singularity
-singularity exec \
-    --nv \
-    --bind "$PROJECT_DIR:$PROJECT_DIR" \
-    --pwd "$PROJECT_DIR" \
-    "$SINGULARITY_IMAGE" \
-    /bin/bash -c "source /venv/bin/activate && pip install --target '$USER_PACKAGES_DIR' psutil > /dev/null 2>&1 && export NUM_ENVS=$NUM_ENVS && export PYTHONUNBUFFERED=1 && export PYTHONPATH='$PROJECT_DIR/src:$USER_PACKAGES_DIR:\$PYTHONPATH' && python3 -u '$PROJECT_DIR/src/rl_hockey/common/training/train_single_run.py'"
-
-# Alternative: Run with custom parameters by modifying train_single_run.py
-# Or uncomment and modify the following line to pass parameters directly:
-# singularity run \
-#     --nv \
-#     --bind "$PROJECT_DIR:$PROJECT_DIR" \
-#     --pwd "$PROJECT_DIR" \
-#     "$SINGULARITY_IMAGE" \
-#     python3 -c "from rl_hockey.common.training.train_single_run import train_single_run; train_single_run('configs/curriculum_simple.json', base_output_dir='results/runs', run_name='cluster_run', num_envs=24, device='cuda:0')"
diff --git a/resources/cluster_setup_ansel.sh b/resources/cluster_setup_ansel.sh
deleted file mode 100644
index 7af361f..0000000
--- a/resources/cluster_setup_ansel.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/bin/bash
-# Setup script to run on the TCML cluster after syncing files
-# Run this script on the cluster: bash resources/cluster_setup.sh
-
-set -e
-
-PROJECT_DIR="$HOME/RL_CheungMaenzerAbraham_Hockey"
-cd "$PROJECT_DIR"
-
-echo "Setting up RL Hockey project on TCML cluster..."
-echo ""
-
-# Create singularity build directory
-echo "Creating singularity build directory..."
-mkdir -p singularity_build
-cd singularity_build
-
-# Copy container definition
-echo "Copying container definition..."
-cp ../resources/container/container.def ./rl_hockey.def
-
-# Copy requirements.txt to build directory
-echo "Copying requirements.txt..."
-cp ../requirements.txt ./requirements.txt
-
-# Verify files exist before building
-echo ""
-echo "Verifying files are in place for build:"
-if [ ! -f "./rl_hockey.def" ]; then
-    echo "ERROR: rl_hockey.def not found!"
-    exit 1
-fi
-echo "✓ rl_hockey.def found"
-
-if [ ! -f "./requirements.txt" ]; then
-    echo "ERROR: requirements.txt not found!"
-    exit 1
-fi
-echo "✓ requirements.txt found ($(wc -l < ./requirements.txt) lines)"
-
-# Show files in build directory
-echo "Files in build directory:"
-ls -lh
-
-# Build the container
-echo ""
-echo "Building Singularity container (this may take 10-30 minutes)..."
-echo "This will install all packages from requirements.txt into the container."
-singularity build --fakeroot rl_hockey.simg rl_hockey.def
-
-echo ""
-echo "Container built successfully!"
-echo ""
-echo "Container location: $PROJECT_DIR/singularity_build/rl_hockey.simg"
-echo ""
-echo "Next steps:"
-echo "1. Update resources/train_single_run.sbatch if needed (check paths and email)"
-echo "2. Submit the job: sbatch resources/train_single_run.sbatch"
-echo "3. Monitor with: squeue -u \$USER"
diff --git a/resources/container/container.def b/resources/container/container.def
index 526b154..ae27d12 100644
--- a/resources/container/container.def
+++ b/resources/container/container.def
@@ -1,9 +1,6 @@
 bootstrap: docker
 from: ubuntu:24.04
 
-%files
-    requirements.txt /opt/requirements.txt
-
 %post
     set -e
     export DEBIAN_FRONTEND=noninteractive
@@ -11,156 +8,83 @@ from: ubuntu:24.04
     echo "deb http://archive.ubuntu.com/ubuntu noble universe" >> /etc/apt/sources.list
     apt-get update
 
-    # Install base Python and build tools
     apt-get install -y python3-dev python3-pip python3-full swig git
-    
-    # Install system Python packages that are available via apt but not PyPI
-    # These are packages that pip freeze captures but need to be installed via apt
-    # Most are not needed for RL project, but installing common ones that might be dependencies
-    # Uncomment the ones you actually need for your project:
-    # apt-get install -y python3-apt python3-dbus python3-gi python3-cairo python3-brlapi
 
     python3 -m venv /venv
     . /venv/bin/activate
 
+    # Create requirements.txt directly in the container from embedded content
+    # This will be replaced by cluster_setup.sh with actual requirements
+    cat > /tmp/requirements.txt << 'REQUIREMENTS_EMBEDDED'
+# Requirements will be embedded here by cluster_setup.sh
+REQUIREMENTS_EMBEDDED
+
     # Install setuptools and wheel first
     pip install --upgrade pip setuptools wheel
 
+    # Install PyTorch with CUDA 12.6 (supports compute capability 6.1 for GTX 1080 Ti)
     # This must be installed before other requirements to ensure correct CUDA dependencies
     pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126
 
-    pip install tensordict
-    pip install pink-noise-rl
-    pip install matplotlib
-    pip install omegaconf
-    pip install psutil
-    pip install imageio imageio-ffmpeg
-
     # Install hockey environment (must be done separately as it's from git)
     pip install "hockey @ git+https://git@github.com/martius-lab/hockey-env.git"
 
-    # Install requirements from /opt/requirements.txt (copied via %files section)
+    # Install requirements from requirements.txt (excluding the editable rl_hockey package and hockey)
+    # Filter out rl_hockey and hockey since we install hockey separately above
     echo "Checking for requirements.txt..."
+    echo "Current directory: $(pwd)"
+    echo "Files in /tmp:"
+    ls -la /tmp/ | head -10 || true
+    echo ""
     
     if [ -f /tmp/requirements.txt ]; then
         echo "✓ Found requirements.txt at /tmp/requirements.txt"
         echo "Total lines in requirements.txt: $(wc -l < /tmp/requirements.txt)"
-        echo "Filtering out rl_hockey, hockey, torch packages, triton, CUDA packages, and system-specific packages..."
-        # NOTE: pip freeze captures ALL packages, including system packages installed via apt.
-        # These packages (like Brlapi, cupshelpers, python-apt, etc.) are available in your
-        # local Ubuntu system Python but are NOT available on PyPI. They need to be installed
-        # via apt, not pip. Most of these are not needed for the RL project.
-        # We use a conservative exclusion list - only exclude packages that are known to be
-        # system packages or already installed separately.
-        # Create a pattern file for system packages to exclude
-        cat > /tmp/exclude_patterns.txt << 'EXCLUDE_PATTERNS'
-^rl_hockey
-^hockey
-^torch
-^triton
-^nvidia-
-^Brlapi
-^cloud-init
-^ubuntu-drivers-common
-^ubuntu-pro-client
-^ufw
-^unattended-upgrades
-^usb-creator
-^command-not-found
-^language-selector
-^python-apt
-^python-debian
-^systemd-python
-^distro-info
-^xkit
-^cupshelpers
-^dbus-python
-^duplicity
-^eduvpn-client
-^eduvpn-common
-^gpg
-^launchpadlib
-^lazr\.
-^louis
-^pycairo
-^pycups
-^PyGObject
-^jeepney
-^defer
-==0\.0\.0$
-EXCLUDE_PATTERNS
-        
-        # Filter requirements using the pattern file
-        grep -vEf /tmp/exclude_patterns.txt /tmp/requirements.txt > /tmp/filtered_requirements.txt
-        rm -f /tmp/exclude_patterns.txt
+        echo "Filtering out rl_hockey, hockey, torch packages, triton, and CUDA packages (PyTorch installed separately)..."
+        # Create a temp file with filtered requirements
+        # Remove rl_hockey, hockey, torch packages, triton, and nvidia CUDA packages (handled by PyTorch install)
+        grep -v "rl_hockey" /tmp/requirements.txt | \
+        grep -v "^hockey" | \
+        grep -v "^torch" | \
+        grep -v "^triton" | \
+        grep -v "^nvidia-" > /tmp/filtered_requirements.txt
         echo "Packages to install: $(wc -l < /tmp/filtered_requirements.txt)"
         echo "First few packages to install:"
-        head -5 /opt/filtered_requirements.txt
+        head -5 /tmp/filtered_requirements.txt
         echo ""
         echo "Installing packages from requirements.txt (this may take several minutes)..."
-        # Install packages with better error handling
-        # Continue installation even if some packages fail (they might be system packages)
-        set +e  # Don't exit on error
-        # Install packages and capture output
-        # Use a subshell to capture exit code in a portable way
-        pip install -r /tmp/filtered_requirements.txt > /tmp/install.log 2>&1
-        install_exit_code=$?
-        # Display the output
-        cat /tmp/install.log
-        set -e  # Re-enable exit on error
-        
-        if [ $install_exit_code -ne 0 ]; then
-            echo ""
-            echo "WARNING: Some packages failed to install. Analyzing failures..."
-            echo ""
-            
-            # Extract failed packages that are not available on PyPI
-            failed_packages=$(grep -E "ERROR: Could not find a version|ERROR: No matching distribution found" /tmp/install.log 2>/dev/null | \
-                sed -n 's/.*requirement \([^ ]*\) (from.*/\1/p' | \
-                sed 's/==.*//' | sort -u || true)
-            
-            if [ -n "$failed_packages" ]; then
-                echo "The following packages could not be found on PyPI (likely system packages):"
-                echo "$failed_packages"
-                echo ""
-                echo "These packages are typically installed via apt, not pip."
-                echo "If you need them, add them to the apt-get install section in container.def"
-                echo ""
-            fi
-            
-            # Check if there were other types of errors
-            other_errors=$(grep -vE "ERROR: Could not find a version|ERROR: No matching distribution found|WARNING:" /tmp/install.log | grep -i "error" | head -5 || true)
-            if [ -n "$other_errors" ]; then
-                echo "Other installation errors detected:"
-                echo "$other_errors"
-                echo ""
-            fi
-            
-            # Count how many packages were successfully installed vs failed
-            installed_count=$(grep -c "Successfully installed" /tmp/install.log 2>/dev/null || echo "0")
-            echo "Installation summary:"
-            echo "  - Successfully installed packages detected in log"
-            echo "  - Some packages may have failed (see above)"
-            echo ""
-            echo "The container build will continue. Failed packages may need to be:"
-            echo "  1. Added to the exclude list if they're system packages"
-            echo "  2. Installed manually at runtime if they're actually needed"
-            echo "  3. Added to requirements.txt with correct versions if missing"
-        else
-            echo "✓ All packages installed successfully"
-        fi
-        rm -f /tmp/install.log
+        pip install -r /tmp/filtered_requirements.txt || {
+            echo "ERROR: pip install failed!"
+            exit 1
+        }
         echo "✓ Finished installing requirements.txt packages"
-        rm -f /opt/filtered_requirements.txt
+        rm -f /tmp/filtered_requirements.txt
     else
-        echo "ERROR: requirements.txt not found at /opt/requirements.txt"
+        echo "ERROR: requirements.txt not found at /tmp/requirements.txt"
+        echo "This means the %files section did not copy the file."
+        echo "Checking if we can find it elsewhere..."
+        find / -name "requirements.txt" -type f 2>/dev/null | head -5 || echo "Not found anywhere"
+        echo ""
+        echo "The container will only have minimal dependencies."
+        echo "You may need to install packages at runtime or rebuild with requirements.txt in the build directory."
         exit 1
     fi
 
+    # Note: The rl_hockey package will be installed at runtime from the mounted project directory
+    # This allows you to update code without rebuilding the container
+    # Installation command: pip install -e /path/to/RL_CheungMaenzerAbraham_Hockey
+
     # cleanup
     apt-get clean
     pip cache purge
 
+
 %runscript
     . /venv/bin/activate
     "$@"
+
+
+
+
+
+
diff --git a/resources/container/container_ansel.def b/resources/container/container_ansel.def
deleted file mode 100644
index 46c70e4..0000000
--- a/resources/container/container_ansel.def
+++ /dev/null
@@ -1,68 +0,0 @@
-bootstrap: docker
-from: ubuntu:24.04
-
-%files
-    requirements.txt /opt/requirements.txt
-
-%post
-    set -e
-    export DEBIAN_FRONTEND=noninteractive
-
-    echo "deb http://archive.ubuntu.com/ubuntu noble universe" >> /etc/apt/sources.list
-    apt-get update
-
-    apt-get install -y python3-dev python3-pip python3-full swig git
-
-    python3 -m venv /venv
-    . /venv/bin/activate
-
-    # Install setuptools and wheel first
-    pip install --upgrade pip setuptools wheel
-
-    # Install PyTorch with CUDA 12.6 (supports compute capability 6.1 for GTX 1080 Ti)
-    pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126
-
-    # Install hockey environment (must be done separately as it's from git)
-    pip install "hockey @ git+https://git@github.com/martius-lab/hockey-env.git"
-
-    # Install requirements from /opt/requirements.txt (copied via %files section)
-    echo "Checking for requirements.txt..."
-    
-    if [ -f /opt/requirements.txt ]; then
-        echo "✓ Found requirements.txt at /opt/requirements.txt"
-        echo "Total lines in requirements.txt: $(wc -l < /opt/requirements.txt)"
-        echo "Filtering out rl_hockey, hockey, torch packages, triton, and CUDA packages..."
-        
-        # Filter requirements and save to /opt/filtered_requirements.txt
-        grep -v "rl_hockey" /opt/requirements.txt | \
-        grep -v "^hockey" | \
-        grep -v "^torch" | \
-        grep -v "^triton" | \
-        grep -v "^nvidia-" > /opt/filtered_requirements.txt
-        
-        echo "Packages to install: $(wc -l < /opt/filtered_requirements.txt)"
-        echo "First few packages to install:"
-        head -5 /opt/filtered_requirements.txt
-        echo ""
-        echo "Installing packages from requirements.txt..."
-        pip install -r /opt/filtered_requirements.txt
-        echo "✓ Finished installing requirements.txt packages"
-        rm -f /opt/filtered_requirements.txt
-    else
-        echo "ERROR: requirements.txt not found at /opt/requirements.txt"
-        exit 1
-    fi
-
-    # cleanup
-    apt-get clean
-    pip cache purge
-
-%runscript
-    . /venv/bin/activate
-    "$@"
-
-
-
-
-
-
diff --git a/resources/container/readme.txt b/resources/container/readme.txt
index f1fcb55..bfdef3d 100644
--- a/resources/container/readme.txt
+++ b/resources/container/readme.txt
@@ -1,12 +1,12 @@
-To use the TCML cluster, we need to create a container. You can start with the attached container.def file, which installs:
-- the requirements of the virtual environment used in the exercises (PPO, DDPG, etc.); and
-- the hockey environment.
-
-
-To build the container, use the following command:
-singularity build --fakeroot /path/to/container.sif container.def
-
-
-Once the container is created, it can be used as follows:
-singularity run /path/to/container.sif python3 ./my_script.py
-
+To use the TCML cluster, we need to create a container. You can start with the attached container.def file, which installs:
+- the requirements of the virtual environment used in the exercises (PPO, DDPG, etc.); and
+- the hockey environment.
+
+
+To build the container, use the following command:
+singularity build --fakeroot /path/to/container.sif container.def
+
+
+Once the container is created, it can be used as follows:
+singularity run /path/to/container.sif python3 ./my_script.py
+
diff --git a/resources/container/requirements.txt b/resources/container/requirements.txt
new file mode 100644
index 0000000..f85c61e
--- /dev/null
+++ b/resources/container/requirements.txt
@@ -0,0 +1,177 @@
+annotated-types==0.7.0
+anyio==4.11.0
+argon2-cffi==25.1.0
+argon2-cffi-bindings==25.1.0
+arrow==1.4.0
+asttokens==3.0.1
+async-lru==2.0.5
+attrs==25.4.0
+babel==2.17.0
+beautifulsoup4==4.14.2
+black==25.12.0
+bleach==6.3.0
+box2d-py==2.3.8
+cattrs==24.1.2
+certifi==2025.11.12
+cffi==2.0.0
+charset-normalizer==3.4.4
+click==8.3.1
+cloudpickle==3.1.2
+comm==0.2.3
+contourpy==1.3.2
+cryptography==45.0.4
+cycler==0.12.1
+debugpy==1.8.17
+decorator==5.2.1
+defusedxml==0.7.1
+delegator.py==0.1.1
+exceptiongroup==1.3.1
+executing==2.2.1
+Farama-Notifications==0.0.4
+fastjsonschema==2.21.2
+filelock==3.20.0
+fonttools==4.60.1
+fqdn==1.5.1
+fsspec==2025.10.0
+gitdb==4.0.12
+GitPython==3.1.45
+glob2==0.7
+gymnasium==1.2.2
+h11==0.16.0
+hockey @ git+https://github.com/martius-lab/laser-hockey-env.git@59dbc689d0e79fea30814389a2414f1c9bce5617
+httpcore==1.0.9
+httpx==0.28.1
+idna==3.11
+imageio==2.37.0
+imageio-ffmpeg==0.6.0
+ipykernel==7.1.0
+ipympl==0.9.8
+ipython==8.37.0
+ipywidgets==8.1.8
+isoduration==20.11.0
+isort==7.0.0
+jedi==0.19.2
+Jinja2==3.1.6
+json5==0.12.1
+jsonpointer==3.0.0
+jsonschema==4.25.1
+jsonschema-specifications==2025.9.1
+jupyter==1.1.1
+jupyter-console==6.6.3
+jupyter-events==0.12.0
+jupyter-lsp==2.3.0
+jupyter_client==8.6.3
+jupyter_core==5.9.1
+jupyter_server==2.17.0
+jupyter_server_terminals==0.5.3
+jupyterlab==4.5.0
+jupyterlab_pygments==0.3.0
+jupyterlab_server==2.28.0
+jupyterlab_widgets==3.0.16
+kiwisolver==1.4.9
+lark==1.3.1
+MarkupSafe==3.0.3
+matplotlib==3.10.7
+matplotlib-inline==0.2.1
+mistune==3.1.4
+moviepy==2.2.1
+mpmath==1.3.0
+mypy_extensions==1.1.0
+nbclient==0.10.2
+nbconvert==7.16.6
+nbformat==5.10.4
+nest-asyncio==1.6.0
+networkx==3.4.2
+notebook==7.5.0
+notebook_shim==0.2.4
+numpy==2.2.6
+nvidia-cublas-cu12==12.8.4.1
+nvidia-cuda-cupti-cu12==12.8.90
+nvidia-cuda-nvrtc-cu12==12.8.93
+nvidia-cuda-runtime-cu12==12.8.90
+nvidia-cudnn-cu12==9.10.2.21
+nvidia-cufft-cu12==11.3.3.83
+nvidia-cufile-cu12==1.13.1.3
+nvidia-curand-cu12==10.3.9.90
+nvidia-cusolver-cu12==11.7.3.90
+nvidia-cusparse-cu12==12.5.8.93
+nvidia-cusparselt-cu12==0.7.1
+nvidia-nccl-cu12==2.27.5
+nvidia-nvjitlink-cu12==12.8.93
+nvidia-nvshmem-cu12==3.3.20
+nvidia-nvtx-cu12==12.8.90
+overrides==7.7.0
+packaging==25.0
+pandas==2.2.3
+pandocfilters==1.5.1
+parso==0.8.5
+pathspec==0.12.1
+pexpect==4.9.0
+pillow==12.0.0
+pink-noise-rl==2.0.1
+platformdirs==4.5.0
+proglog==0.1.12
+prometheus_client==0.23.1
+prompt_toolkit==3.0.52
+protobuf==6.33.1
+psutil==7.1.3
+ptyprocess==0.7.0
+pure_eval==0.2.3
+pycparser==2.23
+pydantic==2.12.5
+pydantic_core==2.41.5
+pygame==2.6.1
+pyglet==1.5.0
+Pygments==2.19.2
+pyparsing==3.2.5
+pypdfium2==4.30.1
+python-dateutil==2.9.0.post0
+python-dotenv==1.1.0
+python-json-logger==4.0.0
+pytokens==0.3.0
+pyttsx3==2.98
+PyYAML==6.0.3
+pyzmq==27.1.0
+RapidFuzz==3.12.2
+referencing==0.37.0
+requests==2.32.5
+requests-cache==1.2.1
+rfc3339-validator==0.1.4
+rfc3986-validator==0.1.1
+rfc3987-syntax==1.1.0
+-e git+ssh://git@github.com/NiklasAbraham/RL_CheungMaenzerAbraham_Hockey.git@4e121bd08e75daece5a67891c9344046ffe7339a#egg=rl_hockey
+rpds-py==0.29.0
+scipy==1.15.2
+Send2Trash==1.8.3
+sentry-sdk==2.46.0
+six==1.17.0
+smmap==5.0.2
+sniffio==1.3.1
+soupsieve==2.8
+stack-data==0.6.3
+sympy==1.14.0
+tensorboardX==2.6.4
+terminado==0.18.1
+timple==0.1.8
+tinycss2==1.4.0
+tomli==2.3.0
+torch==2.7.1
+torchaudio==2.7.1
+torchvision==0.22.1
+tornado==6.5.2
+tqdm==4.67.1
+traitlets==5.14.3
+triton==3.3.1
+typing-inspection==0.4.2
+typing_extensions==4.15.0
+tzdata==2025.2
+uri-template==1.3.0
+url-normalize==1.4.3
+urllib3==2.5.0
+wandb==0.23.0
+wcwidth==0.2.14
+webcolors==25.10.0
+webencodings==0.5.1
+websocket-client==1.9.0
+websockets==13.1
+widgetsnbextension==4.0.15
diff --git a/resources/download_job_files.sh b/resources/download_job_files.sh
deleted file mode 100755
index 791f36d..0000000
--- a/resources/download_job_files.sh
+++ /dev/null
@@ -1,49 +0,0 @@
-#!/bin/bash
-
-# Script to download job.* files from the cluster
-# Usage: ./resources/download_job_files.sh [server_address]
-
-# Configuration
-SERVER="${1:-tcml-login1}"
-# SSH config should handle the full hostname mapping
-REMOTE_PROJECT_DIR="~/RL_CheungMaenzerAbraham_Hockey"
-LOCAL_PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
-REMOTE_JOB_FILES_DIR="${REMOTE_PROJECT_DIR}"
-LOCAL_JOB_FILES_DIR="${LOCAL_PROJECT_DIR}"
-
-echo "Connecting to ${SERVER}..."
-echo "Remote project directory: ${REMOTE_JOB_FILES_DIR}"
-echo "Local project directory: ${LOCAL_JOB_FILES_DIR}"
-
-# Check if job files exist on the remote server
-echo ""
-echo "Checking for job.* files on remote server..."
-JOB_FILES=$(ssh "${SERVER}" "cd ${REMOTE_JOB_FILES_DIR} && ls job.* 2>/dev/null || echo ''")
-
-if [ -z "${JOB_FILES}" ]; then
-    echo "Warning: No job.* files found on remote server."
-    exit 0
-fi
-
-echo "Found job files:"
-echo "${JOB_FILES}" | while read -r file; do
-    if [ -n "${file}" ]; then
-        echo "  ${file}"
-    fi
-done
-
-# Download job files using rsync (more efficient than scp)
-echo ""
-echo "Downloading job.* files from remote server..."
-
-rsync -avz --progress "${SERVER}:${REMOTE_JOB_FILES_DIR}/job."* "${LOCAL_JOB_FILES_DIR}/"
-
-if [ $? -eq 0 ]; then
-    echo ""
-    echo "Successfully downloaded job files"
-    echo "Local path: ${LOCAL_JOB_FILES_DIR}/"
-else
-    echo ""
-    echo "Error: Failed to download the job files."
-    exit 1
-fi
diff --git a/resources/info/project.pdf b/resources/project.pdf
similarity index 100%
rename from resources/info/project.pdf
rename to resources/project.pdf
diff --git a/src/rl_hockey/DDDQN/__init__.py b/src/rl_hockey/DDDQN/__init__.py
index ffd85cd..fbc529d 100644
--- a/src/rl_hockey/DDDQN/__init__.py
+++ b/src/rl_hockey/DDDQN/__init__.py
@@ -1,4 +1,3 @@
 from rl_hockey.DDDQN.dddqn import DDDQN
-from rl_hockey.DDDQN.ddqn_per import DDQN_PER
 
-__all__ = ["DDDQN", "DDQN_PER"]
+__all__ = ["DDDQN"]
diff --git a/src/rl_hockey/DDDQN/dddqn.py b/src/rl_hockey/DDDQN/dddqn.py
index 1bea2b0..950d01f 100644
--- a/src/rl_hockey/DDDQN/dddqn.py
+++ b/src/rl_hockey/DDDQN/dddqn.py
@@ -66,16 +66,16 @@ class DDDQN(Agent):
                     action = np.random.randint(0, self.action_dim)
 
             return action
-
-    def act_batch(self, states, deterministic=False, **kwargs):
+    
+    def act_batch(self, states, deterministic=False):
         """Process a batch of states at once (for vectorized environments)"""
         with torch.no_grad():
             state_tensor = torch.from_numpy(states).float().to(DEVICE)
             if state_tensor.dim() == 1:
                 state_tensor = state_tensor.unsqueeze(0)
-
+            
             q_values = self.q_network(state_tensor)
-
+            
             if deterministic:
                 actions = q_values.argmax(dim=1).cpu().numpy()
             else:
@@ -83,16 +83,16 @@ class DDDQN(Agent):
                 # For each state in batch, decide whether to explore or exploit
                 batch_size = state_tensor.shape[0]
                 random_mask = np.random.random(batch_size) < eps
-
+                
                 # Get greedy actions
                 greedy_actions = q_values.argmax(dim=1).cpu().numpy()
-
+                
                 # Get random actions
                 random_actions = np.random.randint(0, self.action_dim, size=batch_size)
-
+                
                 # Combine: use random where mask is True, greedy otherwise
                 actions = np.where(random_mask, random_actions, greedy_actions)
-
+            
             return actions
 
     def evaluate(self, state):
@@ -114,32 +114,17 @@ class DDDQN(Agent):
             )
 
             state = torch.from_numpy(state).float().to(DEVICE, non_blocking=True)
-            action = (
-                torch.from_numpy(
-                    action.astype(np.int64) if action.dtype == np.float32 else action
-                )
-                .long()
-                .to(DEVICE, non_blocking=True)
-            )
+            action = torch.from_numpy(action.astype(np.int64) if action.dtype == np.float32 else action).long().to(DEVICE, non_blocking=True)
             if action.dim() > 1:
                 action = action.squeeze(1)
-            reward = (
-                torch.from_numpy(reward)
-                .float()
-                .to(DEVICE, non_blocking=True)
-                .squeeze(-1)
-            )
+            reward = torch.from_numpy(reward).float().to(DEVICE, non_blocking=True).squeeze(-1)
 
             reward_clip = self.config.get("reward_clip", None)
             if reward_clip is not None:
                 reward = torch.clamp(reward, -reward_clip, reward_clip)
-
-            next_state = (
-                torch.from_numpy(next_state).float().to(DEVICE, non_blocking=True)
-            )
-            done = (
-                torch.from_numpy(done).float().to(DEVICE, non_blocking=True).squeeze(-1)
-            )
+            
+            next_state = torch.from_numpy(next_state).float().to(DEVICE, non_blocking=True)
+            done = torch.from_numpy(done).float().to(DEVICE, non_blocking=True).squeeze(-1)
 
             with torch.no_grad():
                 next_q_values = self.q_network(next_state)
@@ -156,14 +141,12 @@ class DDDQN(Agent):
                 loss = F.smooth_l1_loss(current_q_value, target)
             else:
                 loss = F.mse_loss(current_q_value, target)
-
+            
             self.optimizer.zero_grad(set_to_none=True)
             loss.backward()
 
             grad_clip = self.config.get("grad_clip", 10.0)
-            torch.nn.utils.clip_grad_norm_(
-                self.q_network.parameters(), max_norm=grad_clip
-            )
+            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=grad_clip)
             self.optimizer.step()
 
             losses.append(loss.item())
@@ -206,15 +189,9 @@ class DDDQN(Agent):
         self.config.update(checkpoint["config"])
         hidden_dim = checkpoint.get("hidden_dim", [256, 256, 256])
 
-        self.q_network = DuelingDQN_Network(
-            self.state_dim, self.action_dim, hidden_dim=hidden_dim
-        ).to(DEVICE)
-        self.q_network_target = DuelingDQN_Network(
-            self.state_dim, self.action_dim, hidden_dim=hidden_dim
-        ).to(DEVICE)
-        self.optimizer = optim.Adam(
-            self.q_network.parameters(), lr=self.config["learning_rate"]
-        )
+        self.q_network = DuelingDQN_Network(self.state_dim, self.action_dim, hidden_dim=hidden_dim).to(DEVICE)
+        self.q_network_target = DuelingDQN_Network(self.state_dim, self.action_dim, hidden_dim=hidden_dim).to(DEVICE)
+        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.config["learning_rate"])
 
         self.q_network.load_state_dict(checkpoint["q_network"])
         self.q_network_target.load_state_dict(checkpoint["q_network_target"])
diff --git a/src/rl_hockey/DDDQN/ddqn_per.py b/src/rl_hockey/DDDQN/ddqn_per.py
deleted file mode 100644
index 28b77d1..0000000
--- a/src/rl_hockey/DDDQN/ddqn_per.py
+++ /dev/null
@@ -1,342 +0,0 @@
-import os
-
-import numpy as np
-import torch
-import torch.nn.functional as F
-import torch.optim as optim
-
-from rl_hockey.common.agent import Agent
-from rl_hockey.common.buffer import PrioritizedReplayBuffer, ReplayBuffer
-from rl_hockey.DDDQN.models import DuelingDQN_Network
-
-DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-
-class DDQN_PER(Agent):
-    """Double Deep Q-Network with optional Prioritized Experience Replay.
-
-    This implementation extends DDDQN with the ability to toggle
-    Prioritized Experience Replay (PER) on or off.
-
-    When PER is enabled:
-    - Samples transitions based on TD error priorities
-    - Uses importance sampling weights to correct for bias
-    - Updates priorities after each training step
-    """
-
-    def __init__(
-        self, state_dim, action_dim, hidden_dim=[256, 256], use_per=False, **user_config
-    ):
-        """Initialize DDQN with optional PER.
-
-        Parameters
-        ----------
-        state_dim : int
-            Dimension of the state space
-        action_dim : int
-            Dimension of the action space
-        hidden_dim : list
-            List of hidden layer dimensions
-        use_per : bool
-            Whether to use Prioritized Experience Replay
-        **user_config : dict
-            Additional configuration parameters including:
-            - batch_size: Batch size for training (default: 256)
-            - learning_rate: Learning rate (default: 1e-4)
-            - discount: Discount factor (default: 0.99)
-            - target_update_freq: Frequency of target network updates (default: 50)
-            - reward_clip: Clip rewards to this value (default: None)
-            - use_huber_loss: Use Huber loss instead of MSE (default: False)
-            - grad_clip: Gradient clipping value (default: 1.0)
-            - eps: Initial epsilon for epsilon-greedy (default: 1.0)
-            - eps_min: Minimum epsilon (default: 0.01)
-            - eps_decay: Epsilon decay factor (default: 0.9995)
-            - per_alpha: PER priority exponent (default: 0.6)
-            - per_beta: PER initial importance sampling exponent (default: 0.4)
-            - per_beta_increment: PER beta increment per sample (default: 0.0001)
-            - per_max_beta: PER maximum beta value (default: 1.0)
-            - per_eps: PER epsilon for priorities (default: 1e-6)
-        """
-        super().__init__()
-
-        self.state_dim = state_dim
-        self.action_dim = action_dim
-        self.use_per = use_per
-
-        self.config = {
-            "batch_size": 256,
-            "learning_rate": 1e-4,
-            "discount": 0.99,
-            "target_update_freq": 50,
-            "reward_clip": None,
-            "use_huber_loss": False,
-            "grad_clip": 1.0,
-            "eps": 1,
-            "eps_min": 0.01,
-            "eps_decay": 0.9995,
-            # PER parameters
-            "per_alpha": 0.6,
-            "per_beta": 0.4,
-            "per_beta_increment": 0.0001,
-            "per_max_beta": 1.0,
-            "per_eps": 1e-6,
-        }
-
-        self.config.update(user_config)
-
-        # Replace buffer with PER buffer if enabled
-        if self.use_per:
-            self.buffer = PrioritizedReplayBuffer(
-                max_size=self.config.get("buffer_size", 1_000_000),
-                alpha=self.config["per_alpha"],
-                beta=self.config["per_beta"],
-                beta_increment=self.config["per_beta_increment"],
-                max_beta=self.config["per_max_beta"],
-                eps=self.config["per_eps"],
-            )
-        else:
-            self.buffer = ReplayBuffer(
-                max_size=self.config.get("buffer_size", 1_000_000)
-            )
-
-        self.q_network = DuelingDQN_Network(
-            state_dim, action_dim, hidden_dim=hidden_dim
-        ).to(DEVICE)
-        self.q_network_target = DuelingDQN_Network(
-            state_dim, action_dim, hidden_dim=hidden_dim
-        ).to(DEVICE)
-        self.q_network_target.load_state_dict(self.q_network.state_dict())
-
-        self.optimizer = optim.Adam(
-            self.q_network.parameters(), lr=self.config["learning_rate"]
-        )
-
-        self.training_steps = 0
-
-    def act(self, state, deterministic=False):
-        with torch.no_grad():
-            state_tensor = torch.from_numpy(state).float().to(DEVICE)
-            if state_tensor.dim() == 1:
-                state_tensor = state_tensor.unsqueeze(0)
-
-            if deterministic:
-                q_values = self.q_network(state_tensor)
-                action = q_values.argmax(dim=1).item()
-            else:
-                eps = self.config["eps"]
-                if np.random.random() > eps:
-                    q_values = self.q_network(state_tensor)
-                    action = q_values.argmax(dim=1).item()
-                else:
-                    action = np.random.randint(0, self.action_dim)
-
-            return action
-
-    def act_batch(self, states, deterministic=False, **kwargs):
-        """Process a batch of states at once (for vectorized environments)"""
-        with torch.no_grad():
-            state_tensor = torch.from_numpy(states).float().to(DEVICE)
-            if state_tensor.dim() == 1:
-                state_tensor = state_tensor.unsqueeze(0)
-
-            q_values = self.q_network(state_tensor)
-
-            if deterministic:
-                actions = q_values.argmax(dim=1).cpu().numpy()
-            else:
-                eps = self.config["eps"]
-                # For each state in batch, decide whether to explore or exploit
-                batch_size = state_tensor.shape[0]
-                random_mask = np.random.random(batch_size) < eps
-
-                # Get greedy actions
-                greedy_actions = q_values.argmax(dim=1).cpu().numpy()
-
-                # Get random actions
-                random_actions = np.random.randint(0, self.action_dim, size=batch_size)
-
-                # Combine: use random where mask is True, greedy otherwise
-                actions = np.where(random_mask, random_actions, greedy_actions)
-
-            return actions
-
-    def evaluate(self, state):
-        with torch.no_grad():
-            state_tensor = torch.from_numpy(state).float().to(DEVICE)
-            if state_tensor.dim() == 1:
-                state_tensor = state_tensor.unsqueeze(0)
-
-            q_values = self.q_network(state_tensor)
-            max_q_value = q_values.max(dim=1)[0].item()
-            return max_q_value
-
-    def train(self, steps=1):
-        losses = []
-
-        for i in range(steps):
-            # Sample from buffer (PER returns additional values)
-            if self.use_per:
-                state, action, reward, next_state, done, weights, indices = (
-                    self.buffer.sample(self.config["batch_size"])
-                )
-                weights = (
-                    torch.from_numpy(weights).float().to(DEVICE, non_blocking=True)
-                )
-            else:
-                state, action, reward, next_state, done = self.buffer.sample(
-                    self.config["batch_size"]
-                )
-                weights = None
-                indices = None
-
-            state = torch.from_numpy(state).float().to(DEVICE, non_blocking=True)
-            action = (
-                torch.from_numpy(
-                    action.astype(np.int64) if action.dtype == np.float32 else action
-                )
-                .long()
-                .to(DEVICE, non_blocking=True)
-            )
-            if action.dim() > 1:
-                action = action.squeeze(1)
-            reward = (
-                torch.from_numpy(reward)
-                .float()
-                .to(DEVICE, non_blocking=True)
-                .squeeze(-1)
-            )
-
-            reward_clip = self.config.get("reward_clip", None)
-            if reward_clip is not None:
-                reward = torch.clamp(reward, -reward_clip, reward_clip)
-
-            next_state = (
-                torch.from_numpy(next_state).float().to(DEVICE, non_blocking=True)
-            )
-            done = (
-                torch.from_numpy(done).float().to(DEVICE, non_blocking=True).squeeze(-1)
-            )
-
-            with torch.no_grad():
-                next_q_values = self.q_network(next_state)
-                next_action = next_q_values.argmax(dim=1, keepdim=True)
-                next_q_values_target = self.q_network_target(next_state)
-                next_value = next_q_values_target.gather(1, next_action).squeeze(1)
-
-            target = reward + (1 - done) * self.config["discount"] * next_value
-
-            current_q_values = self.q_network(state)
-            current_q_value = current_q_values.gather(1, action.unsqueeze(1)).squeeze(1)
-
-            # Compute TD errors for PER
-            td_errors = (current_q_value - target).detach().cpu().numpy()
-
-            # Compute loss with optional importance sampling weights
-            if self.config.get("use_huber_loss", False):
-                if self.use_per and weights is not None:
-                    loss = F.smooth_l1_loss(current_q_value, target, reduction="none")
-                    loss = (loss * weights).mean()
-                else:
-                    loss = F.smooth_l1_loss(current_q_value, target)
-            else:
-                if self.use_per and weights is not None:
-                    loss = F.mse_loss(current_q_value, target, reduction="none")
-                    loss = (loss * weights).mean()
-                else:
-                    loss = F.mse_loss(current_q_value, target)
-
-            self.optimizer.zero_grad(set_to_none=True)
-            loss.backward()
-
-            grad_clip = self.config.get("grad_clip", 10.0)
-            torch.nn.utils.clip_grad_norm_(
-                self.q_network.parameters(), max_norm=grad_clip
-            )
-            self.optimizer.step()
-
-            # Update priorities if using PER
-            if self.use_per and indices is not None:
-                self.buffer.update_priorities(indices, td_errors)
-
-            losses.append(loss.item())
-            self.training_steps += 1
-
-            if self.training_steps % self.config["target_update_freq"] == 0:
-                self.q_network_target.load_state_dict(self.q_network.state_dict())
-
-        return {"loss": losses}
-
-    def save(self, filepath):
-        if not os.path.exists(os.path.dirname(filepath)):
-            os.makedirs(os.path.dirname(filepath))
-
-        if not filepath.endswith(".pt"):
-            filepath += ".pt"
-
-        hidden_dim = []
-        for module in self.q_network.feature_network:
-            if isinstance(module, torch.nn.Linear):
-                hidden_dim.append(module.out_features)
-
-        checkpoint = {
-            "q_network": self.q_network.state_dict(),
-            "q_network_target": self.q_network_target.state_dict(),
-            "optimizer": self.optimizer.state_dict(),
-            "training_steps": self.training_steps,
-            "config": self.config,
-            "state_dim": self.state_dim,
-            "action_dim": self.action_dim,
-            "hidden_dim": hidden_dim,
-            "use_per": self.use_per,
-        }
-
-        torch.save(checkpoint, filepath)
-
-    def load(self, filepath):
-        checkpoint = torch.load(filepath, map_location=DEVICE)
-        self.state_dim = checkpoint["state_dim"]
-        self.action_dim = checkpoint["action_dim"]
-        self.config.update(checkpoint["config"])
-        self.use_per = checkpoint.get("use_per", False)
-        hidden_dim = checkpoint.get("hidden_dim", [256, 256, 256])
-
-        # Reinitialize buffer with correct type
-        if self.use_per:
-            self.buffer = PrioritizedReplayBuffer(
-                max_size=self.config.get("buffer_size", 1_000_000),
-                alpha=self.config["per_alpha"],
-                beta=self.config["per_beta"],
-                beta_increment=self.config["per_beta_increment"],
-                max_beta=self.config["per_max_beta"],
-                eps=self.config["per_eps"],
-            )
-        else:
-            self.buffer = ReplayBuffer(
-                max_size=self.config.get("buffer_size", 1_000_000)
-            )
-
-        self.q_network = DuelingDQN_Network(
-            self.state_dim, self.action_dim, hidden_dim=hidden_dim
-        ).to(DEVICE)
-        self.q_network_target = DuelingDQN_Network(
-            self.state_dim, self.action_dim, hidden_dim=hidden_dim
-        ).to(DEVICE)
-        self.optimizer = optim.Adam(
-            self.q_network.parameters(), lr=self.config["learning_rate"]
-        )
-
-        self.q_network.load_state_dict(checkpoint["q_network"])
-        self.q_network_target.load_state_dict(checkpoint["q_network_target"])
-        try:
-            self.optimizer.load_state_dict(checkpoint["optimizer"])
-        except Exception:
-            pass
-
-        self.training_steps = checkpoint.get("training_steps", 0)
-
-    def on_episode_start(self, episode):
-        if self.config["eps_decay"] is not None:
-            self.config["eps"] = max(
-                self.config["eps_min"], self.config["eps"] * self.config["eps_decay"]
-            )
-        return {"eps": self.config["eps"]}
diff --git a/src/rl_hockey/TD_MPC2/2310.16828v2.pdf b/src/rl_hockey/TD_MPC2/2310.16828v2.pdf
deleted file mode 100644
index 490dbce..0000000
Binary files a/src/rl_hockey/TD_MPC2/2310.16828v2.pdf and /dev/null differ
diff --git a/src/rl_hockey/TD_MPC2/Notes_Niklas.md b/src/rl_hockey/TD_MPC2/Notes_Niklas.md
deleted file mode 100644
index eb7ae60..0000000
--- a/src/rl_hockey/TD_MPC2/Notes_Niklas.md
+++ /dev/null
@@ -1,184 +0,0 @@
-
-
-## Notes Niklas
-
-
-#### Jobs
-
-
-##### 1969941 - 2026-01-22_22-05-49
-- simple fucking run of the current repo state nothing exiting
-- no backprop reward shaping calssic boring things
-- Horizon 18
---> fucked because 18! and config error 
-
-
-##### 1970116 - 2026-01-22_22-35-14
-- run with minimla reward shaping
-- curretn state of repo and then with simple reward prop
-- Horizon 18
---> fucked because 18! and config error
-
-
-##### 1971497 - 2026-01-23_08-49-02
-- run with minimal reward shaping
-- run with backward prop
-
-##### 1971498 - 2026-01-23_08-51-48
-- run without shaping and back prop
-
-##### 1971519 - killed
-- run with backprop aber ohne reward shaping
-
-##### 1971525
-- more loggign , shorter horizion 5
-
-##### 1972025 
-- entire poliy change rolled back with the tanh, might be the ciritla fucking issue
-
-#### Architecture of TDMPC-2 
-
-```
-MODEL-BASED RL COMPONENTS:
-
-1. ENCODER NETWORK:
-   Maps observations to latent state representations
-OptimizedModule(
-  (_orig_mod): Encoder(
-    (net): Sequential(
-      (0): Linear(in_features=18, out_features=512, bias=True)
-      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (2): Mish()
-      (3): Linear(in_features=512, out_features=512, bias=True)
-      (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (5): Mish()
-      (6): Linear(in_features=512, out_features=512, bias=True)
-      (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (8): Mish()
-      (9): Linear(in_features=512, out_features=1024, bias=True)
-      (10): SimNorm()
-    )
-  )
-)
-   Trainable Parameters: 1,063,424
-
-2. DYNAMICS MODEL:
-   Predicts next latent state given current state and action
-OptimizedModule(
-  (_orig_mod): DynamicsSimple(
-    (net): Sequential(
-      (0): Linear(in_features=1028, out_features=512, bias=True)
-      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (2): Mish()
-      (3): Linear(in_features=512, out_features=512, bias=True)
-      (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (5): Mish()
-      (6): Linear(in_features=512, out_features=512, bias=True)
-      (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (8): Mish()
-      (9): Linear(in_features=512, out_features=1024, bias=True)
-      (10): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
-    )
-    (simnorm): SimNorm()
-  )
-)
-   Trainable Parameters: 1,582,592
-
-3. REWARD MODEL:
-   Predicts reward given latent state and action
-OptimizedModule(
-  (_orig_mod): Reward(
-    (net): Sequential(
-      (0): Linear(in_features=1028, out_features=512, bias=True)
-      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (2): Mish()
-      (3): Linear(in_features=512, out_features=512, bias=True)
-      (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (5): Mish()
-      (6): Linear(in_features=512, out_features=512, bias=True)
-      (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-      (8): Mish()
-      (9): Linear(in_features=512, out_features=101, bias=True)
-    )
-  )
-)
-   Trainable Parameters: 1,107,045
-
-3a. TERMINATION MODEL:
-   Predicts termination probability given latent state and action
-Termination(
-  (mlp): Sequential(
-    (0): Linear(in_features=1024, out_features=256, bias=True)
-    (1): ReLU()
-    (2): Linear(in_features=256, out_features=256, bias=True)
-    (3): ReLU()
-    (4): Linear(in_features=256, out_features=1, bias=True)
-  )
-)
-   Trainable Parameters: 328,449
-
-4. Q ENSEMBLE:
-   5 Q-networks for value estimation
-QEnsemble(
-  (q_functions): ModuleList(
-    (0-4): 5 x QFunction(
-      (net): Sequential(
-        (0): Linear(in_features=1028, out_features=512, bias=True)
-        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-        (2): Dropout(p=0.01, inplace=False)
-        (3): Mish()
-        (4): Linear(in_features=512, out_features=512, bias=True)
-        (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-        (6): Mish()
-        (7): Linear(in_features=512, out_features=512, bias=True)
-        (8): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-        (9): Mish()
-        (10): Linear(in_features=512, out_features=101, bias=True)
-      )
-    )
-  )
-)
-   Trainable Parameters: 5,535,225
-
-5. TARGET Q ENSEMBLE:
-   Target network for stable Q-learning
-   Same architecture as Q Ensemble
-   Trainable Parameters: 0
-
-6. POLICY NETWORK:
-   Learns to mimic the MPC planner for fast inference
-Policy(
-  (net): Sequential(
-    (0): Linear(in_features=1024, out_features=512, bias=True)
-    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-    (2): Mish()
-    (3): Linear(in_features=512, out_features=512, bias=True)
-    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-    (5): Mish()
-    (6): Linear(in_features=512, out_features=512, bias=True)
-    (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
-    (8): Mish()
-  )
-  (mean_head): Linear(in_features=512, out_features=4, bias=True)
-  (log_std_head): Linear(in_features=512, out_features=4, bias=True)
-)
-   Trainable Parameters: 1,057,288
-
-PARAMETER SUMMARY:
-  World Model (Encoder + Dynamics + Reward + Termination + Q): 9,616,735
-  Policy Network: 1,057,288
-  Target Q Network: 0
-  TOTAL TRAINABLE PARAMETERS: 10,674,023
-
-OPTIMIZERS:
-  World Model + Q Optimizer: Adam (LR: 0.0003)
-  Policy Optimizer: Adam (LR: 0.0003)
-
-MPC PLANNER:
-  Type: MPPI (Model Predictive Path Integral)
-  Horizon: 9
-  Samples per iteration: 512
-  Planning iterations: 6
-  Temperature: 0.5
-
-```
\ No newline at end of file
diff --git a/src/rl_hockey/TD_MPC2/__init__.py b/src/rl_hockey/TD_MPC2/__init__.py
deleted file mode 100644
index 2758780..0000000
--- a/src/rl_hockey/TD_MPC2/__init__.py
+++ /dev/null
@@ -1,17 +0,0 @@
-from .model_dynamics_simple import DynamicsSimple
-from .model_encoder import Encoder
-from .model_policy import Policy
-from .model_q_ensemble import QEnsemble
-from .model_reward import Reward
-from .mppi_planner_simple import MPPIPlannerSimplePaper
-from .tdmpc2 import TDMPC2
-
-__all__ = [
-    "TDMPC2",
-    "Encoder",
-    "DynamicsSimple",
-    "Reward",
-    "QEnsemble",
-    "Policy",
-    "MPPIPlannerSimplePaper",
-]
diff --git a/src/rl_hockey/TD_MPC2/model_dynamics_simple.py b/src/rl_hockey/TD_MPC2/model_dynamics_simple.py
deleted file mode 100644
index ea5f886..0000000
--- a/src/rl_hockey/TD_MPC2/model_dynamics_simple.py
+++ /dev/null
@@ -1,44 +0,0 @@
-import torch
-import torch.nn as nn
-
-from rl_hockey.TD_MPC2.util import SimNorm
-
-
-class DynamicsSimple(nn.Module):
-    """Predicts next latent state given current latent state and action."""
-
-    def __init__(
-        self,
-        latent_dim=512,
-        action_dim=8,
-        hidden_dim=[256, 256, 256],
-        simnorm_temperature=1.0,
-    ):
-        super().__init__()
-
-        layers = []
-
-        layers.append(nn.Linear(latent_dim + action_dim, hidden_dim[0]))
-        layers.append(nn.LayerNorm(hidden_dim[0]))
-        layers.append(nn.Mish())
-
-        for i in range(1, len(hidden_dim)):
-            layers.append(nn.Linear(hidden_dim[i - 1], hidden_dim[i]))
-            layers.append(nn.LayerNorm(hidden_dim[i]))
-            layers.append(nn.Mish())
-
-        layers.append(nn.Linear(hidden_dim[-1], latent_dim))
-        layers.append(nn.LayerNorm(latent_dim))
-
-        self.net = nn.Sequential(*layers)
-        self.simnorm = SimNorm(
-            latent_dim, simplex_dim=min(8, latent_dim), temperature=simnorm_temperature
-        )
-
-    def forward(self, latent, action):
-        """Forward pass through dynamics model."""
-        x = torch.cat([latent, action], dim=-1)
-        z_after_linear = self.net(x)
-        latent_next = self.simnorm(z_after_linear)
-
-        return latent_next
diff --git a/src/rl_hockey/TD_MPC2/model_encoder.py b/src/rl_hockey/TD_MPC2/model_encoder.py
deleted file mode 100644
index f43f4ac..0000000
--- a/src/rl_hockey/TD_MPC2/model_encoder.py
+++ /dev/null
@@ -1,41 +0,0 @@
-import torch.nn as nn
-
-from rl_hockey.TD_MPC2.util import SimNorm
-
-
-class Encoder(nn.Module):
-    """Encodes states to latent states."""
-
-    def __init__(
-        self,
-        state_dim=18,
-        latent_dim=512,
-        hidden_dim=[256, 256, 256],
-        simnorm_temperature=1.0,
-    ):
-        super().__init__()
-
-        layers = []
-        layers.append(nn.Linear(state_dim, hidden_dim[0]))
-        layers.append(nn.LayerNorm(hidden_dim[0]))
-        layers.append(nn.Mish())
-
-        for i in range(1, len(hidden_dim)):
-            layers.append(nn.Linear(hidden_dim[i - 1], hidden_dim[i]))
-            layers.append(nn.LayerNorm(hidden_dim[i]))
-            layers.append(nn.Mish())
-
-        layers.append(nn.Linear(hidden_dim[-1], latent_dim))
-        layers.append(
-            SimNorm(
-                latent_dim,
-                simplex_dim=min(8, latent_dim),
-                temperature=simnorm_temperature,
-            )
-        )
-
-        self.net = nn.Sequential(*layers)
-
-    def forward(self, state):
-        """Forward pass through encoder."""
-        return self.net(state)
diff --git a/src/rl_hockey/TD_MPC2/model_init.py b/src/rl_hockey/TD_MPC2/model_init.py
deleted file mode 100644
index 0b4d45a..0000000
--- a/src/rl_hockey/TD_MPC2/model_init.py
+++ /dev/null
@@ -1,55 +0,0 @@
-# TD-MPC2 weight initialization.
-# tdmpc2 calls init_encoder, init_dynamics, ... after building each module.
-
-import torch.nn as nn
-
-
-def init_encoder(module):
-    for m in module.modules():
-        if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=0.02)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-
-def init_dynamics(module):
-    for m in module.modules():
-        if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=0.02)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-
-def init_reward(module):
-    for m in module.modules():
-        if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=0.02)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-    module.net[-1].weight.data.fill_(0)
-
-
-def init_termination(module):
-    for m in module.modules():
-        if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=0.02)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-
-def init_q_ensemble(module):
-    for m in module.modules():
-        if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=0.02)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-    for q in module.q_functions:
-        q.net[-1].weight.data.fill_(0)
-
-
-def init_policy(module):
-    for m in module.modules():
-        if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=0.02)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
diff --git a/src/rl_hockey/TD_MPC2/model_policy.py b/src/rl_hockey/TD_MPC2/model_policy.py
deleted file mode 100644
index 357c28e..0000000
--- a/src/rl_hockey/TD_MPC2/model_policy.py
+++ /dev/null
@@ -1,73 +0,0 @@
-# Policy network.
-
-import torch
-import torch.nn as nn
-
-
-class Policy(nn.Module):
-    """
-    Stochastic policy for action initialization in MPC.
-    """
-
-    def __init__(
-        self,
-        latent_dim=512,
-        action_dim=8,
-        hidden_dim=[256, 256, 256],
-        log_std_min=-10.0,
-        log_std_max=2.0,
-    ):
-        super().__init__()
-        self.log_std_min = log_std_min
-        self.log_std_max = log_std_max
-
-        layers = []
-        layers.append(nn.Linear(latent_dim, hidden_dim[0]))
-        layers.append(nn.LayerNorm(hidden_dim[0]))
-        layers.append(nn.Mish())
-
-        for i in range(1, len(hidden_dim)):
-            layers.append(nn.Linear(hidden_dim[i - 1], hidden_dim[i]))
-            layers.append(nn.LayerNorm(hidden_dim[i]))
-            layers.append(nn.Mish())
-
-        self.net = nn.Sequential(*layers)
-        self.mean_head = nn.Linear(hidden_dim[-1], action_dim)
-        self.log_std_head = nn.Linear(hidden_dim[-1], action_dim)
-
-    def _distribution(self, latent):
-        h = self.net(latent)
-        mean = self.mean_head(h)
-        log_std_raw = self.log_std_head(h)
-        log_std_dif = self.log_std_max - self.log_std_min
-        log_std = self.log_std_min + 0.5 * log_std_dif * (torch.tanh(log_std_raw) + 1)
-        std = torch.exp(log_std)
-        return mean, std, log_std
-
-    def mean_action(self, latent):
-        mean, _, _ = self._distribution(latent)
-        return torch.tanh(mean)
-
-    def sample(self, latent):
-        """Sample action from policy."""
-        mean, std, log_std = self._distribution(latent)
-        eps = torch.randn_like(mean)
-        pre_tanh = mean + std * eps
-
-        gaussian_log_prob = -0.5 * eps.pow(2) - log_std - 0.9189385175704956
-        gaussian_log_prob = gaussian_log_prob.sum(dim=-1, keepdim=True)
-        scaled_log_prob = gaussian_log_prob * mean.shape[-1]
-
-        mean_action = torch.tanh(mean)
-        action = torch.tanh(pre_tanh)
-        squash_correction = torch.log(torch.relu(1 - action.pow(2)) + 1e-6).sum(
-            dim=-1, keepdim=True
-        )
-        log_prob = gaussian_log_prob - squash_correction
-        entropy_scale = scaled_log_prob / (log_prob + 1e-8)
-        scaled_entropy = -log_prob * entropy_scale
-
-        return action, log_prob, mean_action, scaled_entropy
-
-    def forward(self, latent):
-        return self.mean_action(latent)
diff --git a/src/rl_hockey/TD_MPC2/model_q_ensemble.py b/src/rl_hockey/TD_MPC2/model_q_ensemble.py
deleted file mode 100644
index de56f3d..0000000
--- a/src/rl_hockey/TD_MPC2/model_q_ensemble.py
+++ /dev/null
@@ -1,85 +0,0 @@
-# Ensemble of Q-functions.
-
-import torch
-import torch.nn as nn
-
-from rl_hockey.TD_MPC2.model_q_function import QFunction
-from rl_hockey.TD_MPC2.util import two_hot_inv
-
-
-class QEnsemble(nn.Module):
-    """
-    Ensemble of Q-functions for value estimation.
-    """
-
-    def __init__(
-        self,
-        num_q=5,
-        latent_dim=512,
-        action_dim=8,
-        hidden_dim=[256, 256, 256],
-        num_bins=101,
-        vmin=-100.0,
-        vmax=100.0,
-    ):
-        super().__init__()
-
-        self.q_functions = nn.ModuleList(
-            [
-                QFunction(latent_dim, action_dim, hidden_dim, num_bins)
-                for _ in range(num_q)
-            ]
-        )
-        self.num_q = num_q
-        self.num_bins = num_bins
-        self.vmin = vmin
-        self.vmax = vmax
-
-    def forward(self, latent, action):
-        """Forward pass through Q-ensemble."""
-        q_list = [q_func(latent, action) for q_func in self.q_functions]
-        return q_list
-
-    def min(self, latent, action):
-        """Returns minimum Q-value from a subsampled ensemble."""
-        return self.min_subsample(latent, action, k=2)
-
-    def min_subsample(self, latent, action, k=2):
-        """Returns minimum Q-value from k sampled heads."""
-        num_q = len(self.q_functions)
-        if k > num_q:
-            k = num_q
-        idx = torch.randperm(num_q, device=latent.device)[:k]
-        idx_list = idx.cpu().tolist()
-        q_logits_list = [self.q_functions[i](latent, action) for i in idx_list]
-        q_values = torch.cat(
-            [
-                two_hot_inv(q_logits, self.num_bins, self.vmin, self.vmax)
-                for q_logits in q_logits_list
-            ],
-            dim=1,
-        )
-        min_val = torch.min(q_values, dim=1, keepdim=True)[0]
-        return min_val
-
-    def avg_subsample(self, latent, action, k=2):
-        """Returns average Q-value from k sampled heads."""
-        num_q = len(self.q_functions)
-        if k > num_q:
-            k = num_q
-        idx = torch.randperm(num_q, device=latent.device)[:k]
-        idx_list = idx.cpu().tolist()
-        q_logits_list = [self.q_functions[i](latent, action) for i in idx_list]
-        q_values = torch.cat(
-            [
-                two_hot_inv(q_logits, self.num_bins, self.vmin, self.vmax)
-                for q_logits in q_logits_list
-            ],
-            dim=1,
-        )
-        avg_val = q_values.mean(dim=1, keepdim=True)
-        return avg_val
-
-    def avg(self, latent, action):
-        """Returns average Q-value from a subsampled ensemble."""
-        return self.avg_subsample(latent, action, k=2)
diff --git a/src/rl_hockey/TD_MPC2/model_q_function.py b/src/rl_hockey/TD_MPC2/model_q_function.py
deleted file mode 100644
index 96d625b..0000000
--- a/src/rl_hockey/TD_MPC2/model_q_function.py
+++ /dev/null
@@ -1,38 +0,0 @@
-# Q-function.
-
-import torch
-import torch.nn as nn
-
-
-class QFunction(nn.Module):
-    """
-    Q-function for value estimation.
-    """
-
-    def __init__(
-        self, latent_dim=512, action_dim=8, hidden_dim=[256, 256, 256], num_bins=101
-    ):
-        super().__init__()
-
-        layers = []
-        layers.append(nn.Linear(latent_dim + action_dim, hidden_dim[0]))
-        layers.append(nn.LayerNorm(hidden_dim[0]))
-        # Add Dropout after first layer (after LayerNorm)
-        layers.append(nn.Dropout(p=0.01))
-        layers.append(nn.Mish())
-
-        for i in range(1, len(hidden_dim)):
-            layers.append(nn.Linear(hidden_dim[i - 1], hidden_dim[i]))
-            layers.append(nn.LayerNorm(hidden_dim[i]))
-            layers.append(nn.Mish())
-
-        layers.append(nn.Linear(hidden_dim[-1], num_bins))
-
-        self.net = nn.Sequential(*layers)
-        self.num_bins = num_bins
-
-    def forward(self, latent, action):
-        """Forward pass through Q-function."""
-        x = torch.cat([latent, action], dim=-1)
-        q_logits = self.net(x)
-        return q_logits
diff --git a/src/rl_hockey/TD_MPC2/model_reward.py b/src/rl_hockey/TD_MPC2/model_reward.py
deleted file mode 100644
index 04b4541..0000000
--- a/src/rl_hockey/TD_MPC2/model_reward.py
+++ /dev/null
@@ -1,34 +0,0 @@
-import torch
-import torch.nn as nn
-
-
-class Reward(nn.Module):
-    """
-    Predicts reward given latent state and action.
-    """
-
-    def __init__(self, latent_dim=512, action_dim=8, hidden_dim=[256, 256, 256], num_bins=101, vmin=-10.0, vmax=10.0):
-        super().__init__()
-
-        layers = []
-        layers.append(nn.Linear(latent_dim + action_dim, hidden_dim[0]))
-        layers.append(nn.LayerNorm(hidden_dim[0]))
-        layers.append(nn.Mish())
-
-        for i in range(1, len(hidden_dim)):
-            layers.append(nn.Linear(hidden_dim[i - 1], hidden_dim[i]))
-            layers.append(nn.LayerNorm(hidden_dim[i]))
-            layers.append(nn.Mish())
-
-        layers.append(nn.Linear(hidden_dim[-1], num_bins))
-
-        self.net = nn.Sequential(*layers)
-        self.num_bins = num_bins
-        self.vmin = vmin
-        self.vmax = vmax
-
-    def forward(self, latent, action):
-        """Forward pass through reward model."""
-        x = torch.cat([latent, action], dim=-1)
-
-        return self.net(x)
diff --git a/src/rl_hockey/TD_MPC2/model_termination.py b/src/rl_hockey/TD_MPC2/model_termination.py
deleted file mode 100644
index e26ce95..0000000
--- a/src/rl_hockey/TD_MPC2/model_termination.py
+++ /dev/null
@@ -1,17 +0,0 @@
-import torch
-import torch.nn as nn
-
-
-class Termination(nn.Module):
-    def __init__(self, latent_dim, hidden_dim):
-        super().__init__()
-        self.mlp = nn.Sequential(
-            nn.Linear(latent_dim, hidden_dim[0]),
-            nn.ReLU(),
-            nn.Linear(hidden_dim[0], hidden_dim[1]),
-            nn.ReLU(),
-            nn.Linear(hidden_dim[1], 1),
-        )
-
-    def forward(self, z):
-        return self.mlp(z)
diff --git a/src/rl_hockey/TD_MPC2/mppi_planner_simple.py b/src/rl_hockey/TD_MPC2/mppi_planner_simple.py
deleted file mode 100644
index ba23985..0000000
--- a/src/rl_hockey/TD_MPC2/mppi_planner_simple.py
+++ /dev/null
@@ -1,256 +0,0 @@
-# MPPI planner from TD-MPC2.
-
-import torch
-import torch.nn as nn
-
-from rl_hockey.TD_MPC2.util import gumbel_softmax_sample, two_hot_inv
-
-
-class MPPIPlannerSimplePaper(nn.Module):
-    def __init__(
-        self,
-        dynamics,
-        reward,
-        termination,
-        q_ensemble,
-        policy,
-        horizon=5,
-        num_samples=512,
-        num_iterations=6,
-        temperature=0.5,
-        gamma=0.99,
-        std_init=2.0,
-        std_min=0.05,
-        std_decay=0.9,
-        num_bins=101,
-        vmin=-100.0,
-        vmax=100.0,
-        num_pi_trajs=24,
-        num_elites=64,
-    ):
-        super().__init__()
-        self.dynamics = dynamics
-        self.reward = reward
-        self.termination = termination
-        self.q_ensemble = q_ensemble
-        self.policy = policy
-        self.horizon = horizon
-        self.num_samples = num_samples
-        self.num_iterations = num_iterations
-        self.temperature = temperature
-        self.gamma = gamma
-        self.std_init = std_init
-        self.std_min = std_min
-        self.std_decay = std_decay
-        self.action_dim = None
-        self.num_bins = num_bins
-        self.vmin = vmin
-        self.vmax = vmax
-        self.num_pi_trajs = num_pi_trajs
-        self.num_elites = num_elites
-
-        self.register_buffer(
-            "gamma_powers",
-            torch.tensor([gamma**h for h in range(horizon + 1)], dtype=torch.float32),
-        )
-        self.register_buffer("_prev_mean", None)
-
-    def rollout_trajectories(
-        self,
-        z_init,
-        actions,
-        dynamics,
-        reward_predictor,
-        termination_predictor,
-        gamma=0.99,
-    ):
-        """Rollout trajectories in latent space."""
-        num_samples, horizon, action_dim = actions.shape
-
-        z = z_init.unsqueeze(0).expand(num_samples, -1)
-
-        returns = torch.zeros(num_samples, device=z.device)
-        termination_probs = torch.zeros(num_samples, device=z.device)
-
-        if hasattr(self, "gamma_powers"):
-            gamma_powers = self.gamma_powers[:horizon]
-        else:
-            gamma_powers = torch.tensor(
-                [gamma**h for h in range(horizon)], device=z.device
-            )
-
-        for h in range(horizon):
-            a = actions[:, h, :]
-            r_logits = reward_predictor(z, a)
-            r = two_hot_inv(r_logits, self.num_bins, self.vmin, self.vmax).squeeze(-1)
-            returns += gamma_powers[h] * r * (1 - termination_probs)
-
-            z = dynamics(z, a).clone()
-
-            t_logits = termination_predictor(z)
-            t_prob = torch.sigmoid(t_logits).squeeze(-1)
-
-            termination_probs = torch.clip(termination_probs + t_prob, 0, 1)
-
-        return returns, z, termination_probs
-
-    def plan(self, latent, return_mean=True, t0=False, return_stats=False):
-        """Plan action using MPPI."""
-        if self.action_dim is None:
-            if self.policy is not None:
-                test_mean = self.policy.mean_action(
-                    latent.unsqueeze(0) if latent.dim() == 1 else latent
-                )
-                if test_mean.dim() > 1:
-                    test_mean = test_mean.squeeze(0)
-                self.action_dim = test_mean.shape[-1]
-            else:
-                raise ValueError("action_dim not set and no policy provided")
-
-        if self._prev_mean is None:
-            self.register_buffer(
-                "_prev_mean",
-                torch.zeros(self.horizon, self.action_dim, device=latent.device),
-            )
-
-        if self.policy is not None and self.num_pi_trajs > 0:
-            pi_actions = torch.empty(
-                self.horizon, self.num_pi_trajs, self.action_dim, device=latent.device
-            )
-            _z = (
-                latent.repeat(self.num_pi_trajs, 1)
-                if latent.dim() == 1
-                else latent.repeat(self.num_pi_trajs, 1)
-            )
-            for t in range(self.horizon - 1):
-                pi_action, _, _, _ = self.policy.sample(_z)
-                pi_actions[t] = pi_action
-                _z = self.dynamics(_z, pi_actions[t])
-            pi_action, _, _, _ = self.policy.sample(_z)
-            pi_actions[-1] = pi_action
-
-        mean = torch.zeros(self.horizon, self.action_dim, device=latent.device)
-        std = torch.full(
-            (self.horizon, self.action_dim),
-            self.std_init,
-            dtype=torch.float,
-            device=latent.device,
-        )
-
-        if not t0 and self._prev_mean is not None:
-            mean[:-1] = self._prev_mean[1:]
-
-        actions = torch.empty(
-            self.horizon, self.num_samples, self.action_dim, device=latent.device
-        )
-
-        if self.policy is not None and self.num_pi_trajs > 0:
-            actions[:, : self.num_pi_trajs] = pi_actions
-
-        planning_stats = {
-            "elite_returns_per_iter": [],
-            "std_per_iter": [],
-            "mean_return_per_iter": [],
-            "all_returns_per_iter": [],
-        }
-
-        for iteration in range(self.num_iterations):
-            num_random_samples = self.num_samples - self.num_pi_trajs
-            if num_random_samples > 0:
-                r = torch.randn(
-                    self.horizon, num_random_samples, self.action_dim, device=std.device
-                )
-                actions_sample = mean.unsqueeze(1) + std.unsqueeze(1) * r
-                actions_sample = actions_sample.clamp(-1, 1)
-                actions[:, self.num_pi_trajs :] = actions_sample
-
-            actions_reshaped = actions.transpose(0, 1)
-
-            returns, final_z, final_termination_probs = self.rollout_trajectories(
-                latent,
-                actions_reshaped,
-                self.dynamics,
-                self.reward,
-                self.termination,
-                self.gamma,
-            )
-
-            final_actions = self.policy.mean_action(final_z)
-            q_values = self.q_ensemble.avg(final_z, final_actions)
-            if hasattr(self, "gamma_powers"):
-                terminal_gamma = self.gamma_powers[self.horizon]
-            else:
-                terminal_gamma = self.gamma**self.horizon
-            returns += (
-                terminal_gamma * q_values.squeeze(-1) * (1 - final_termination_probs)
-            )
-
-            returns = returns.nan_to_num(0)
-
-            elite_idxs = torch.topk(returns, self.num_elites, dim=0).indices
-            elite_returns = returns[elite_idxs]
-            elite_actions = actions[:, elite_idxs]
-
-            if return_stats:
-                planning_stats["elite_returns_per_iter"].append(
-                    {
-                        "min": elite_returns.min().item(),
-                        "max": elite_returns.max().item(),
-                        "mean": elite_returns.mean().item(),
-                        "std": elite_returns.std().item(),
-                    }
-                )
-                planning_stats["mean_return_per_iter"].append(returns.mean().item())
-                planning_stats["all_returns_per_iter"].append(
-                    {
-                        "min": returns.min().item(),
-                        "max": returns.max().item(),
-                        "std": returns.std().item(),
-                    }
-                )
-                planning_stats["std_per_iter"].append(std.mean().item())
-
-            max_return = elite_returns.max()
-            score = torch.exp(self.temperature * (elite_returns - max_return))
-            score = score / (score.sum() + 1e-9)
-
-            score_expanded = score.unsqueeze(0).unsqueeze(-1)
-            mean = (score_expanded * elite_actions).sum(dim=1) / (score.sum() + 1e-9)
-
-            mean_expanded = mean.unsqueeze(1)
-            std = (
-                (score_expanded * (elite_actions - mean_expanded) ** 2).sum(dim=1)
-                / (score.sum() + 1e-9)
-            ).sqrt()
-            std = std.clamp(self.std_min, self.std_init)
-
-        if self._prev_mean is not None:
-            self._prev_mean.copy_(mean)
-
-        rand_idx = gumbel_softmax_sample(score)
-        action = elite_actions[0, rand_idx, :]
-        final_std = std[0]
-
-        if not return_mean:
-            action = action + final_std * torch.randn(
-                self.action_dim, device=action.device
-            )
-            action = action.clamp(-1, 1)
-
-        if return_stats:
-            planning_stats["final_elite_returns"] = {
-                "min": elite_returns.min().item(),
-                "max": elite_returns.max().item(),
-                "mean": elite_returns.mean().item(),
-                "std": elite_returns.std().item(),
-            }
-            planning_stats["final_mean"] = mean[0].clone()
-            planning_stats["final_std"] = std[0].mean().item()
-            planning_stats["std_convergence"] = (
-                planning_stats["std_per_iter"][0] - planning_stats["std_per_iter"][-1]
-                if len(planning_stats["std_per_iter"]) > 1
-                else 0.0
-            )
-            return action, planning_stats
-
-        return action
diff --git a/src/rl_hockey/TD_MPC2/plot_episode_logs.py b/src/rl_hockey/TD_MPC2/plot_episode_logs.py
deleted file mode 100644
index 3a6fdad..0000000
--- a/src/rl_hockey/TD_MPC2/plot_episode_logs.py
+++ /dev/null
@@ -1,348 +0,0 @@
-"""
-Plotting function for TD-MPC2 episode logs.
-Reads all episode log CSV files (including checkpoints) and creates comprehensive plots.
-"""
-
-import csv
-import re
-from pathlib import Path
-from typing import Any, Dict, List, Optional
-
-import matplotlib
-import numpy as np
-
-matplotlib.use("Agg")  # Use non-interactive backend
-import matplotlib.pyplot as plt
-
-
-def load_episode_logs_from_csv(csv_path: Path) -> List[Dict[str, Any]]:
-    """Load episode logs from a CSV file."""
-    episode_logs = []
-
-    if not csv_path.exists():
-        return episode_logs
-
-    with open(csv_path, "r", newline="") as f:
-        reader = csv.DictReader(f)
-        for row in reader:
-            episode_log = {
-                "episode": int(row["episode"]),
-                "reward": float(row["reward"]) if row["reward"] else 0.0,
-                "shaped_reward": float(row["shaped_reward"])
-                if row["shaped_reward"]
-                else 0.0,
-                "backprop_reward": float(row["backprop_reward"])
-                if row.get("backprop_reward")
-                else 0.0,
-                "losses": {},
-            }
-
-            # Extract all loss columns
-            for key, value in row.items():
-                if (
-                    key
-                    not in [
-                        "episode",
-                        "reward",
-                        "shaped_reward",
-                        "backprop_reward",
-                        "total_gradient_steps",
-                    ]
-                    and value
-                ):
-                    try:
-                        episode_log["losses"][key] = float(value)
-                    except ValueError:
-                        pass
-
-            episode_logs.append(episode_log)
-
-    return episode_logs
-
-
-def find_all_episode_log_files(csvs_dir: Path, run_name: str) -> List[Path]:
-    """Find all episode log CSV files for a run (including checkpoints)."""
-    log_files = []
-
-    # Main episode logs file
-    main_file = csvs_dir / f"{run_name}_episode_logs.csv"
-    if main_file.exists():
-        log_files.append(main_file)
-
-    # Checkpoint episode logs files (pattern: {run_name}_ep{episode}_episode_logs.csv)
-    pattern = f"{run_name}_ep*_episode_logs.csv"
-    checkpoint_files = sorted(csvs_dir.glob(pattern))
-    log_files.extend(checkpoint_files)
-
-    return sorted(log_files, key=lambda x: x.name)
-
-
-def combine_episode_logs(log_files: List[Path]) -> List[Dict[str, Any]]:
-    """Combine episode logs from multiple CSV files, removing duplicates."""
-    all_logs = {}
-
-    for log_file in log_files:
-        logs = load_episode_logs_from_csv(log_file)
-        for log in logs:
-            episode_num = log["episode"]
-            # Keep the latest entry if there are duplicates
-            all_logs[episode_num] = log
-
-    # Sort by episode number
-    sorted_logs = [all_logs[ep] for ep in sorted(all_logs.keys())]
-    return sorted_logs
-
-
-def plot_episode_logs(
-    folder_path: str, window_size: int = 10, save_path: Optional[Path] = None
-):
-    """
-    Plot episode logs including all loss types.
-
-    Args:
-        folder_path: Path to the run folder (e.g., "results/tdmpc2_runs/2026-01-18_12-24-23")
-        window_size: Window size for moving average
-        save_path: Optional custom path for saving plot
-    """
-    folder = Path(folder_path)
-
-    if not folder.exists():
-        raise ValueError(f"Folder does not exist: {folder_path}")
-
-    csvs_dir = folder / "csvs"
-    plots_dir = folder / "plots"
-
-    if not csvs_dir.exists():
-        raise ValueError(f"Could not find csvs directory in {folder_path}")
-
-    # Find all episode log CSV files to determine run name
-    episode_log_files = list(csvs_dir.glob("*_episode_logs.csv"))
-
-    if not episode_log_files:
-        raise ValueError(f"No episode log files found in {csvs_dir}")
-
-    # Extract run name from the first file (remove _episode_logs.csv or _ep*_episode_logs.csv)
-    first_file = episode_log_files[0]
-    filename = first_file.stem  # Get filename without extension
-
-    # Remove checkpoint suffix if present (e.g., _ep001500_episode_logs)
-    # Pattern: _ep followed by digits, then _episode_logs
-    match = re.search(r"_ep\d+_episode_logs$", filename)
-    if match:
-        # Remove _epXXXXX_episode_logs suffix
-        run_name = filename[: match.start()]
-    else:
-        # Just remove _episode_logs suffix
-        run_name = filename.replace("_episode_logs", "")
-
-    # Find all episode log files for this run
-    log_files = find_all_episode_log_files(csvs_dir, run_name)
-
-    if not log_files:
-        print(f"Warning: No episode log files found for run {run_name}")
-        return
-
-    # Combine logs from all files
-    episode_logs = combine_episode_logs(log_files)
-
-    if not episode_logs:
-        print(f"Warning: No episode logs loaded for run {run_name}")
-        return
-
-    # Extract data
-    episodes = [log["episode"] for log in episode_logs]
-    rewards = [log["reward"] for log in episode_logs]
-    shaped_rewards = [log["shaped_reward"] for log in episode_logs]
-
-    # Extract all loss types
-    all_loss_keys = set()
-    for log in episode_logs:
-        all_loss_keys.update(log["losses"].keys())
-
-    sorted_loss_keys = sorted(all_loss_keys)
-
-    # Prepare loss data (only episodes that have training)
-    loss_data = {key: [] for key in sorted_loss_keys}
-    loss_episodes = {key: [] for key in sorted_loss_keys}
-
-    for log in episode_logs:
-        for loss_key in sorted_loss_keys:
-            if loss_key in log["losses"]:
-                loss_data[loss_key].append(log["losses"][loss_key])
-                loss_episodes[loss_key].append(log["episode"])
-
-    # Find the first episode where all losses are present (warm-up period ends)
-    first_complete_episode = None
-    if sorted_loss_keys:
-        # Find episodes that have all loss types present
-        all_episodes_sets = [
-            set(loss_episodes[key]) for key in sorted_loss_keys if loss_episodes[key]
-        ]
-        if all_episodes_sets and len(all_episodes_sets) == len(sorted_loss_keys):
-            # Find intersection of all episodes (episodes where all losses are present)
-            episodes_with_all_losses = set.intersection(*all_episodes_sets)
-            if episodes_with_all_losses:
-                first_complete_episode = min(episodes_with_all_losses)
-
-        # Filter ALL data (rewards, shaped rewards, and losses) to start from first_complete_episode
-        if first_complete_episode is not None:
-            # Filter rewards and shaped rewards
-            filtered_episodes = []
-            filtered_rewards = []
-            filtered_shaped_rewards = []
-            for ep, rew, sh_rew in zip(episodes, rewards, shaped_rewards):
-                if ep >= first_complete_episode:
-                    filtered_episodes.append(ep)
-                    filtered_rewards.append(rew)
-                    filtered_shaped_rewards.append(sh_rew)
-            episodes = filtered_episodes
-            rewards = filtered_rewards
-            shaped_rewards = filtered_shaped_rewards
-
-            # Filter loss data to only include episodes >= first_complete_episode
-            filtered_loss_data = {}
-            filtered_loss_episodes = {}
-            for key in sorted_loss_keys:
-                filtered_values = []
-                filtered_eps = []
-                for ep, val in zip(loss_episodes[key], loss_data[key]):
-                    if ep >= first_complete_episode:
-                        filtered_values.append(val)
-                        filtered_eps.append(ep)
-                # Only use filtered data if there are still values after filtering
-                if filtered_values:
-                    filtered_loss_data[key] = filtered_values
-                    filtered_loss_episodes[key] = filtered_eps
-                else:
-                    # Keep original data if filtering removed everything
-                    filtered_loss_data[key] = loss_data[key]
-                    filtered_loss_episodes[key] = loss_episodes[key]
-            loss_data = filtered_loss_data
-            loss_episodes = filtered_loss_episodes
-
-    # Create figure with subplots
-    num_losses = len(sorted_loss_keys)
-    if num_losses == 0:
-        # Only rewards, no losses
-        fig, axes = plt.subplots(2, 1, figsize=(12, 10))
-        axes = list(axes.flatten()) if isinstance(axes, np.ndarray) else [axes]
-        plot_rewards_only = False
-    else:
-        # Calculate grid size for subplots: rewards + shaped rewards + all losses
-        n_plots = 2 + num_losses  # rewards, shaped_rewards, and all loss types
-        n_cols = 2
-        n_rows = (n_plots + n_cols - 1) // n_cols
-        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))
-        axes = list(axes.flatten()) if isinstance(axes, np.ndarray) else [axes]
-        plot_rewards_only = False
-
-    # Set main title for the entire figure
-    fig.suptitle(run_name, fontsize=14, fontweight="bold", y=0.995)
-
-    # Plot rewards
-    ax_idx = 0
-    moving_avg_rewards = _moving_average(rewards, window_size)
-    axes[ax_idx].plot(episodes, rewards, alpha=0.3, label="Raw", color="blue")
-    axes[ax_idx].plot(
-        episodes,
-        moving_avg_rewards,
-        label=f"Moving Avg (window={window_size})",
-        color="blue",
-        linewidth=2,
-    )
-    axes[ax_idx].set_xlabel("Episode")
-    axes[ax_idx].set_ylabel("Reward")
-    axes[ax_idx].set_title("Reward per Episode")
-    axes[ax_idx].legend()
-    axes[ax_idx].grid(True, alpha=0.3)
-
-    # Plot shaped rewards
-    if not plot_rewards_only:
-        ax_idx += 1
-        moving_avg_shaped = _moving_average(shaped_rewards, window_size)
-        axes[ax_idx].plot(
-            episodes, shaped_rewards, alpha=0.3, label="Raw", color="green"
-        )
-        axes[ax_idx].plot(
-            episodes,
-            moving_avg_shaped,
-            label=f"Moving Avg (window={window_size})",
-            color="green",
-            linewidth=2,
-        )
-        axes[ax_idx].set_xlabel("Episode")
-        axes[ax_idx].set_ylabel("Shaped Reward")
-        axes[ax_idx].set_title("Shaped Reward per Episode")
-        axes[ax_idx].legend()
-        axes[ax_idx].grid(True, alpha=0.3)
-
-    # Plot each loss type with reward overlay
-    colors = plt.cm.tab10(np.linspace(0, 1, len(sorted_loss_keys)))
-    for i, loss_key in enumerate(sorted_loss_keys):
-        if not plot_rewards_only:
-            ax_idx += 1
-        if ax_idx >= len(axes):
-            break
-
-        loss_values = loss_data[loss_key]
-        loss_eps = loss_episodes[loss_key]
-
-        if loss_values:
-            # Plot losses on primary y-axis
-            moving_avg_losses = _moving_average(loss_values, window_size)
-            axes[ax_idx].plot(
-                loss_eps, loss_values, alpha=0.3, label="Raw", color=colors[i]
-            )
-            axes[ax_idx].plot(
-                loss_eps,
-                moving_avg_losses,
-                label=f"Moving Avg (window={window_size})",
-                color=colors[i],
-                linewidth=2,
-            )
-            axes[ax_idx].set_xlabel("Episode")
-            axes[ax_idx].set_ylabel("Loss")
-            axes[ax_idx].set_title(f"{loss_key} per Episode")
-            axes[ax_idx].legend()
-            axes[ax_idx].grid(True, alpha=0.3)
-
-    # Hide unused subplots
-    for idx in range(ax_idx + 1, len(axes)):
-        axes[idx].set_visible(False)
-
-    plt.tight_layout(rect=[0, 0, 1, 0.98])  # Leave space for main title
-
-    # Save plot
-    if save_path is None:
-        plots_dir.mkdir(parents=True, exist_ok=True)
-        save_path = plots_dir / f"{run_name}_episode_logs.png"
-
-    plt.savefig(save_path, dpi=150, bbox_inches="tight")
-    plt.close()
-
-    print(f"Episode logs plot saved to: {save_path}")
-
-
-def _moving_average(data: List[float], window_size: int) -> List[float]:
-    """Calculate moving average of data."""
-    if not data:
-        return []
-    moving_averages = []
-    for i in range(len(data)):
-        window_start = max(0, i - window_size + 1)
-        window = data[window_start : i + 1]
-        moving_averages.append(sum(window) / len(window))
-    return moving_averages
-
-
-if __name__ == "__main__":
-    # folder_path_1 = "results/tdmpc2_runs/2026-01-21_14-54-11"
-    # folder_path_2 = "results/tdmpc2_runs/2026-01-21_15-07-45"
-    # folder_path_3 = "results/tdmpc2_runs/2026-01-21_16-15-43"
-    # folder_path_4 = "results/tdmpc2_runs/2026-01-21_19-12-44"
-
-    folder_path_1 = "results/tdmpc2_runs/2026-01-23_21-00-09"
-
-    window_size = 20
-
-    plot_episode_logs(folder_path_1, window_size=window_size)
diff --git a/src/rl_hockey/TD_MPC2/profile_tdmpc2.py b/src/rl_hockey/TD_MPC2/profile_tdmpc2.py
deleted file mode 100644
index 618cda6..0000000
--- a/src/rl_hockey/TD_MPC2/profile_tdmpc2.py
+++ /dev/null
@@ -1,808 +0,0 @@
-"""
-Profile TD-MPC2 agent to identify performance bottlenecks.
-Uses torch.profiler to measure time spent in different operations.
-"""
-
-import argparse
-import gc
-import json
-import os
-
-import numpy as np
-import torch
-
-from rl_hockey.common.training.agent_factory import create_agent
-from rl_hockey.common.training.curriculum_manager import AgentConfig
-
-
-def load_agent_from_config(config_path: str, device: str = "cuda"):
-    """Load TD-MPC2 agent from config file."""
-    with open(config_path, "r") as f:
-        config = json.load(f)
-
-    agent_config_dict = config.get("agent", {})
-    state_dim = 18  # Standard hockey env observation dimension
-    action_dim = 8  # Standard hockey env action dimension
-
-    agent_config = AgentConfig(
-        type=agent_config_dict["type"],
-        hyperparameters=agent_config_dict.get("hyperparameters", {}),
-    )
-
-    agent = create_agent(
-        agent_config,
-        state_dim,
-        action_dim,
-        {},
-        device=device,
-    )
-
-    # Set all networks to evaluation mode for consistent profiling
-    if hasattr(agent, "encoder"):
-        agent.encoder.eval()
-    if hasattr(agent, "dynamics"):
-        agent.dynamics.eval()
-    if hasattr(agent, "reward"):
-        agent.reward.eval()
-    if hasattr(agent, "q_ensemble"):
-        agent.q_ensemble.eval()
-    if hasattr(agent, "policy"):
-        agent.policy.eval()
-    if hasattr(agent, "target_q_ensemble"):
-        agent.target_q_ensemble.eval()
-
-    return agent
-
-
-def profile_single_action(
-    agent, obs_dim: int, num_warmup: int = 10, num_iterations: int = 100
-):
-    """Profile single action selection."""
-    print("\n" + "=" * 80)
-    print("PROFILING: Single Action Selection (act)")
-    print("=" * 80)
-
-    device = agent.device
-    obs = torch.randn(obs_dim).to(device)
-
-    # Warmup
-    for _ in range(num_warmup):
-        _ = agent.act(obs.cpu().numpy())
-
-    # Profile
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,  # Disabled to save memory
-        with_stack=False,  # Disabled to save memory
-    ) as prof:
-        with torch.profiler.record_function("act_total"):
-            for _ in range(num_iterations):
-                _ = agent.act(obs.cpu().numpy())
-
-    # Extract results immediately and delete profiler
-    print(f"\nProfiled {num_iterations} action selections")
-    print("\nTop time-consuming operations:")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=30,
-    )
-    print(table_str)
-    
-    # Return table string only, not profiler object
-    return table_str
-
-
-def profile_batch_action(
-    agent,
-    obs_dim: int,
-    batch_size: int = 4,
-    num_warmup: int = 10,
-    num_iterations: int = 50,
-):
-    """Profile batch action selection."""
-    print("\n" + "=" * 80)
-    print(f"PROFILING: Batch Action Selection (act_batch, batch_size={batch_size})")
-    print("=" * 80)
-
-    device = agent.device
-    obs_batch = torch.randn(batch_size, obs_dim).to(device)
-
-    # Warmup
-    for _ in range(num_warmup):
-        _ = agent.act_batch(obs_batch.cpu().numpy())
-
-    # Profile
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,  # Disabled to save memory
-        with_stack=False,  # Disabled to save memory
-    ) as prof:
-        with torch.profiler.record_function("act_batch_total"):
-            for _ in range(num_iterations):
-                _ = agent.act_batch(obs_batch.cpu().numpy())
-
-    # Extract results immediately
-    print(f"\nProfiled {num_iterations} batch action selections")
-    print("\nTop time-consuming operations:")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=30,
-    )
-    print(table_str)
-
-    return table_str
-
-
-def profile_planning_step(
-    agent, obs_dim: int, num_warmup: int = 10, num_iterations: int = 100
-):
-    """Profile a single planning step in detail."""
-    print("\n" + "=" * 80)
-    print("PROFILING: Planning Step Details")
-    print("=" * 80)
-
-    device = agent.device
-    obs = torch.randn(obs_dim).to(device)
-
-    # Warmup
-    z = agent.encoder(obs.unsqueeze(0))
-    for _ in range(num_warmup):
-        _ = agent.planner.plan(z.squeeze(0), return_mean=True)
-
-    # Profile planning with detailed breakdown
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,  # Disabled to save memory
-        with_stack=False,  # Disabled to save memory
-    ) as prof:
-        with torch.profiler.record_function("planning_total"):
-            for _ in range(num_iterations):
-                z = agent.encoder(obs.unsqueeze(0)).squeeze(0)
-                with torch.profiler.record_function("planner_plan"):
-                    _ = agent.planner.plan(z, return_mean=True)
-
-    print(f"\nProfiled {num_iterations} planning steps")
-    print("\nTop time-consuming operations in planning:")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=30,
-    )
-    print(table_str)
-
-    return table_str
-
-
-def profile_model_forward_passes(
-    agent, obs_dim: int, num_samples: int, horizon: int, num_iterations: int = 50
-):
-    """Profile individual model forward passes."""
-    print("\n" + "=" * 80)
-    print("PROFILING: Individual Model Forward Passes")
-    print("=" * 80)
-
-    device = agent.device
-    batch_size = num_samples
-    latent_dim = agent.latent_dim
-    action_dim = agent.action_dim
-
-    # Create test data
-    obs = torch.randn(obs_dim).to(device)
-    latent = torch.randn(latent_dim).to(device)
-    action = torch.randn(action_dim).to(device)
-    latents_batch = torch.randn(batch_size, latent_dim).to(device)
-    actions_batch = torch.randn(batch_size, action_dim).to(device)
-
-    # Warmup
-    for _ in range(10):
-        _ = agent.encoder(obs.unsqueeze(0))
-        _ = agent.dynamics(latent.unsqueeze(0), action.unsqueeze(0))
-        _ = agent.reward(latent.unsqueeze(0), action.unsqueeze(0))
-        _ = agent.q_ensemble.min(latent.unsqueeze(0), action.unsqueeze(0))
-        _ = agent.dynamics(latents_batch, actions_batch)
-
-    results = []
-
-    print("\nProfiling encoder...")
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,
-        with_stack=False,
-    ) as prof:
-        for _ in range(num_iterations):
-            with torch.profiler.record_function("encoder"):
-                _ = agent.encoder(obs.unsqueeze(0))
-
-    print("\nEncoder forward pass:")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=10,
-    )
-    print(table_str)
-    results.append(("encoder", table_str))
-    del prof
-    if device != "cpu":
-        torch.cuda.empty_cache()
-        gc.collect()
-
-    print("\nProfiling dynamics (single)...")
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,
-        with_stack=False,
-    ) as prof:
-        for _ in range(num_iterations):
-            with torch.profiler.record_function("dynamics_single"):
-                _ = agent.dynamics(latent.unsqueeze(0), action.unsqueeze(0))
-
-    print("\nDynamics forward pass (single):")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=10,
-    )
-    print(table_str)
-    results.append(("dynamics_single", table_str))
-    del prof
-    if device != "cpu":
-        torch.cuda.empty_cache()
-        gc.collect()
-
-    print("\nProfiling dynamics (batched)...")
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,
-        with_stack=False,
-    ) as prof:
-        for _ in range(num_iterations):
-            with torch.profiler.record_function("dynamics_batch"):
-                _ = agent.dynamics(latents_batch, actions_batch)
-
-    print("\nDynamics forward pass (batch):")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=10,
-    )
-    print(table_str)
-    results.append(("dynamics_batch", table_str))
-    del prof
-    if device != "cpu":
-        torch.cuda.empty_cache()
-        gc.collect()
-
-    print("\nProfiling reward (batched)...")
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,
-        with_stack=False,
-    ) as prof:
-        for _ in range(num_iterations):
-            with torch.profiler.record_function("reward_batch"):
-                _ = agent.reward(latents_batch, actions_batch)
-
-    print("\nReward forward pass (batch):")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=10,
-    )
-    print(table_str)
-    results.append(("reward_batch", table_str))
-    del prof
-    if device != "cpu":
-        torch.cuda.empty_cache()
-        gc.collect()
-
-    print("\nProfiling Q-ensemble (batched)...")
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,
-        with_stack=False,
-    ) as prof:
-        for _ in range(num_iterations):
-            with torch.profiler.record_function("q_ensemble_batch"):
-                _ = agent.q_ensemble.min(latents_batch, actions_batch)
-
-    print("\nQ-ensemble forward pass (batch):")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=10,
-    )
-    print(table_str)
-    results.append(("q_ensemble_batch", table_str))
-    del prof
-    if device != "cpu":
-        torch.cuda.empty_cache()
-        gc.collect()
-
-    return results
-
-
-def profile_training_step(
-    agent,
-    obs_dim: int,
-    batch_size: int = 256,
-    num_warmup: int = 5,
-    num_iterations: int = 50,
-):
-    """Profile training step including forward and backward passes."""
-    print("\n" + "=" * 80)
-    print(f"PROFILING: Training Step (batch_size={batch_size})")
-    print("=" * 80)
-
-    device = agent.device
-
-    # Set networks to training mode
-    if hasattr(agent, "encoder"):
-        agent.encoder.train()
-    if hasattr(agent, "dynamics"):
-        agent.dynamics.train()
-    if hasattr(agent, "reward"):
-        agent.reward.train()
-    if hasattr(agent, "q_ensemble"):
-        agent.q_ensemble.train()
-    if hasattr(agent, "policy"):
-        agent.policy.train()
-
-    # Fill buffer with dummy data for sampling
-    # TD-MPC2 requires sequences, so we need to create episodes
-    # Each episode needs at least horizon+1 transitions
-    action_dim = agent.action_dim
-    horizon = agent.horizon if hasattr(agent, "horizon") else 5
-    episodes_needed = max(batch_size // 4, 10)  # At least 10 episodes
-    transitions_per_episode = horizon + 5  # Make episodes longer than horizon
-    
-    # Clear buffer first
-    if hasattr(agent.buffer, "clear"):
-        agent.buffer.clear()
-    
-    # Store episodes (sequences of transitions ending with done=True)
-    for ep in range(episodes_needed):
-        for step in range(transitions_per_episode):
-            obs = torch.randn(obs_dim).cpu().numpy()
-            action = torch.randn(action_dim).cpu().numpy()
-            reward = np.random.randn()
-            next_obs = torch.randn(obs_dim).cpu().numpy()
-            # Mark last transition in episode as done
-            done = (step == transitions_per_episode - 1)
-            agent.buffer.store((obs, action, reward, next_obs, done))
-
-    # Warmup
-    for _ in range(num_warmup):
-        _ = agent.train(steps=1)
-
-    # Profile with detailed memory and transfer tracking
-    with torch.profiler.profile(
-        activities=[
-            torch.profiler.ProfilerActivity.CPU,
-            torch.profiler.ProfilerActivity.CUDA,
-        ]
-        if device != "cpu"
-        else [torch.profiler.ProfilerActivity.CPU],
-        record_shapes=False,  # Disabled to save memory
-        profile_memory=False,  # Disabled to save memory
-        with_stack=False,  # Disabled to save memory
-    ) as prof:
-        with torch.profiler.record_function("training_total"):
-            for _ in range(num_iterations):
-                with torch.profiler.record_function("train_step"):
-                    _ = agent.train(steps=1)
-
-    # Extract results immediately
-    print(f"\nProfiled {num_iterations} training steps")
-    print("\nTop time-consuming operations:")
-    table_str = prof.key_averages().table(
-        sort_by="cuda_time_total" if device != "cpu" else "cpu_time_total",
-        row_limit=30,
-    )
-    print(table_str)
-
-    # Print memory transfer operations (CPU-GPU transfers)
-    print("\n" + "=" * 80)
-    print("CPU-GPU Transfer Operations (memcpy, MemcpyHtoD, etc.):")
-    print("=" * 80)
-    all_ops = prof.key_averages()
-    transfer_ops = [
-        op
-        for op in all_ops
-        if any(
-            keyword in str(op.key)
-            for keyword in [
-                "memcpy",
-                "Memcpy",
-                "copy",
-                "to",
-                "from_numpy",
-                "FloatTensor",
-            ]
-        )
-    ]
-    if transfer_ops:
-        print("\nTransfer-related operations (sorted by CPU time):")
-        # Sort by CPU time and show top transfer operations
-        transfer_ops_sorted = sorted(
-            transfer_ops, key=lambda x: x.cpu_time_total, reverse=True
-        )
-        for op in transfer_ops_sorted[:20]:  # Show top 20 transfer operations
-            cpu_time = op.cpu_time_total / 1000.0  # Convert to ms
-            cuda_time = (
-                op.cuda_time_total / 1000.0 if hasattr(op, "cuda_time_total") else 0.0
-            )
-            print(
-                f"  {op.key}: CPU={cpu_time:.3f}ms, CUDA={cuda_time:.3f}ms, Calls={op.count}"
-            )
-    else:
-        print("\nNo explicit transfer operations found in top operations.")
-        print("Transfer overhead may be included in tensor creation operations.")
-        print("Look for operations with high CPU time but low CUDA time.")
-
-    # Print memory usage
-    print("\n" + "=" * 80)
-    print("Memory Usage:")
-    print("=" * 80)
-    print(
-        prof.key_averages().table(
-            sort_by="cuda_memory_usage" if device != "cpu" else "cpu_memory_usage",
-            row_limit=20,
-        )
-    )
-
-    # Set back to eval mode
-    if hasattr(agent, "encoder"):
-        agent.encoder.eval()
-    if hasattr(agent, "dynamics"):
-        agent.dynamics.eval()
-    if hasattr(agent, "reward"):
-        agent.reward.eval()
-    if hasattr(agent, "q_ensemble"):
-        agent.q_ensemble.eval()
-    if hasattr(agent, "policy"):
-        agent.policy.eval()
-
-    return table_str
-
-
-def export_trace(prof, output_path: str):
-    """Export profiling trace for visualization in Chrome tracing."""
-    prof.export_chrome_trace(output_path)
-    print(f"\nTrace exported to: {output_path}")
-    print("Open in Chrome: chrome://tracing")
-
-
-def main(
-    config_path: str = "configs/curriculum_tdmpc2.json",
-    device: str = None,
-    output_dir: str = "results/profiling",
-    num_iterations: int = 100,
-    export_traces: bool = False,
-    gpu_id: int = None,
-):
-    """Main profiling function."""
-    # Handle GPU selection
-    if device is None:
-        device = "cuda" if torch.cuda.is_available() else "cpu"
-
-    if device != "cpu" and torch.cuda.is_available():
-        if gpu_id is not None:
-            # Use specified GPU
-            torch.cuda.set_device(gpu_id)
-            device = f"cuda:{gpu_id}"
-        else:
-            # Use default GPU (usually 0)
-            device = "cuda"
-
-        # Clear GPU cache before profiling to ensure clean state
-        torch.cuda.empty_cache()
-
-        # Print GPU information
-        current_device = torch.cuda.current_device()
-        print(f"Using device: {device}")
-        print(f"GPU {current_device}: {torch.cuda.get_device_name(current_device)}")
-        print(
-            f"GPU Memory: {torch.cuda.get_device_properties(current_device).total_memory / 1e9:.1f} GB"
-        )
-
-        # Check available GPU memory
-        if torch.cuda.is_available():
-            allocated = torch.cuda.memory_allocated(current_device) / 1e9
-            reserved = torch.cuda.memory_reserved(current_device) / 1e9
-            print(
-                f"GPU Memory Status: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved"
-            )
-    else:
-        print(f"Using device: {device}")
-
-    # Load agent
-    print(f"\nLoading agent from config: {config_path}")
-    agent = load_agent_from_config(config_path, device=device)
-
-    obs_dim = agent.obs_dim
-    num_samples = agent.num_samples
-    horizon = agent.horizon
-
-    print("\nAgent configuration:")
-    print(f"  obs_dim: {obs_dim}")
-    print(f"  action_dim: {agent.action_dim}")
-    print(f"  latent_dim: {agent.latent_dim}")
-    print(f"  horizon: {horizon}")
-    print(f"  num_samples: {num_samples}")
-    print(f"  num_iterations: {agent.num_iterations}")
-
-    # Create output directory and verify it's writable
-    try:
-        os.makedirs(output_dir, exist_ok=True)
-        # Test write access
-        test_file = os.path.join(output_dir, ".write_test")
-        with open(test_file, "w") as f:
-            f.write("test")
-        os.remove(test_file)
-        print(f"Output directory created/verified: {os.path.abspath(output_dir)}")
-    except Exception as e:
-        print(f"ERROR: Cannot create or write to output directory: {output_dir}")
-        print(f"Error: {e}")
-        raise
-
-    # Run profiling
-    print("\n" + "=" * 80)
-    print("STARTING PROFILING")
-    print("=" * 80)
-
-    # Collect all profiling results
-    profiling_results = []
-
-    # Helper to clear GPU cache and force garbage collection
-    def clear_gpu_cache():
-        if device != "cpu" and torch.cuda.is_available():
-            torch.cuda.empty_cache()
-            torch.cuda.synchronize()
-            # Force Python garbage collection to release profiler objects
-            gc.collect()
-            torch.cuda.empty_cache()
-    
-    # Helper to print memory status
-    def print_memory_status():
-        if device != "cpu" and torch.cuda.is_available():
-            current_device = torch.cuda.current_device()
-            allocated = torch.cuda.memory_allocated(current_device) / 1e9
-            reserved = torch.cuda.memory_reserved(current_device) / 1e9
-            total = torch.cuda.get_device_properties(current_device).total_memory / 1e9
-            free = total - reserved
-            print(f"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved, {free:.2f} GB free")
-
-    # 1. Profile single action
-    try:
-        clear_gpu_cache()
-        print_memory_status()
-        print("\n[1/5] Profiling single action selection...")
-        table1 = profile_single_action(
-            agent, obs_dim, num_iterations=num_iterations
-        )
-        profiling_results.append(("SINGLE ACTION SELECTION", table1))
-        clear_gpu_cache()
-        print_memory_status()
-    except Exception as e:
-        print(f"\nWARNING: Failed to profile single action: {e}")
-        profiling_results.append(("SINGLE ACTION SELECTION", f"ERROR: {str(e)}"))
-        clear_gpu_cache()
-
-    # 2. Profile batch action
-    try:
-        print("\n[2/5] Profiling batch action selection...")
-        table2 = profile_batch_action(
-            agent, obs_dim, batch_size=4, num_iterations=num_iterations // 2
-        )
-        profiling_results.append(("BATCH ACTION SELECTION", table2))
-        clear_gpu_cache()
-        print_memory_status()
-    except Exception as e:
-        print(f"\nWARNING: Failed to profile batch action: {e}")
-        profiling_results.append(("BATCH ACTION SELECTION", f"ERROR: {str(e)}"))
-        clear_gpu_cache()
-
-    # 3. Profile planning details
-    try:
-        print("\n[3/5] Profiling planning step...")
-        table3 = profile_planning_step(
-            agent, obs_dim, num_iterations=num_iterations
-        )
-        profiling_results.append(("PLANNING STEP", table3))
-        clear_gpu_cache()
-        print_memory_status()
-    except Exception as e:
-        print(f"\nWARNING: Failed to profile planning step: {e}")
-        profiling_results.append(("PLANNING STEP", f"ERROR: {str(e)}"))
-        clear_gpu_cache()
-
-    # 4. Profile individual models (use fewer iterations to save memory)
-    try:
-        print("\n[4/5] Profiling model forward passes...")
-        model_results = profile_model_forward_passes(
-            agent, obs_dim, num_samples, horizon, num_iterations=min(num_iterations, 30)
-        )
-        for name, table in model_results:
-            profiling_results.append((f"MODEL FORWARD PASS: {name.upper()}", table))
-        clear_gpu_cache()
-        print_memory_status()
-    except Exception as e:
-        print(f"\nWARNING: Failed to profile model forward passes: {e}")
-        profiling_results.append(("MODEL FORWARD PASSES", f"ERROR: {str(e)}"))
-        clear_gpu_cache()
-
-    # 5. Profile training step
-    try:
-        print("\n[5/5] Profiling training step...")
-        table4 = profile_training_step(
-            agent,
-            obs_dim,
-            batch_size=agent.config.get("batch_size", 256) if hasattr(agent, "config") else 256,
-            num_iterations=num_iterations // 2,  # Fewer iterations for training
-        )
-        profiling_results.append(("TRAINING STEP", table4))
-        clear_gpu_cache()
-        print_memory_status()
-    except Exception as e:
-        print(f"\nWARNING: Failed to profile training step: {e}")
-        profiling_results.append(("TRAINING STEP", f"ERROR: {str(e)}"))
-        clear_gpu_cache()
-
-    # Save all results to a summary file
-    summary_path = os.path.join(output_dir, "profiling_summary.txt")
-    summary_path_abs = os.path.abspath(summary_path)
-    print(f"\nSaving profiling summary to: {summary_path_abs}")
-    try:
-        with open(summary_path, "w") as f:
-            f.write("=" * 80 + "\n")
-            f.write("TD-MPC2 PROFILING SUMMARY\n")
-            f.write("=" * 80 + "\n\n")
-
-            f.write(f"Device: {device}\n")
-            if device != "cpu" and torch.cuda.is_available():
-                current_device = torch.cuda.current_device()
-                f.write(
-                    f"GPU {current_device}: {torch.cuda.get_device_name(current_device)}\n"
-                )
-                f.write(
-                    f"GPU Memory: {torch.cuda.get_device_properties(current_device).total_memory / 1e9:.1f} GB\n"
-                )
-            f.write(f"Config: {config_path}\n")
-            f.write(f"Iterations: {num_iterations}\n")
-            f.write("\nAgent configuration:\n")
-            f.write(f"  obs_dim: {obs_dim}\n")
-            f.write(f"  action_dim: {agent.action_dim}\n")
-            f.write(f"  latent_dim: {agent.latent_dim}\n")
-            f.write(f"  horizon: {horizon}\n")
-            f.write(f"  num_samples: {num_samples}\n")
-            f.write(f"  num_iterations: {agent.num_iterations}\n")
-            f.write("\n" + "=" * 80 + "\n\n")
-
-            for section_name, table in profiling_results:
-                f.write("\n" + "=" * 80 + "\n")
-                f.write(f"{section_name}\n")
-                f.write("=" * 80 + "\n\n")
-                f.write(table)
-                f.write("\n\n")
-
-            f.write("=" * 80 + "\n")
-            f.write("PROFILING COMPLETE\n")
-            f.write("=" * 80 + "\n")
-
-        print("\n" + "=" * 80)
-        print("PROFILING COMPLETE")
-        print("=" * 80)
-        print("\nSummary:")
-        print("  - Inference profiling: single_action, batch_action, planning")
-        print("  - Training profiling: training_step (forward + backward + optimizer)")
-        print(f"\nProfiling summary saved to: {summary_path_abs}")
-        print(f"File size: {os.path.getsize(summary_path)} bytes")
-    except Exception as e:
-        print(f"\nERROR: Failed to save profiling summary to {summary_path_abs}")
-        print(f"Error: {e}")
-        import traceback
-
-        traceback.print_exc()
-        # Try to save at least a minimal error report
-        try:
-            error_report_path = os.path.join(output_dir, "profiling_error.txt")
-            with open(error_report_path, "w") as f:
-                f.write(f"Profiling failed with error:\n{str(e)}\n\n")
-                f.write("Traceback:\n")
-                traceback.print_exc(file=f)
-            print(f"Error report saved to: {os.path.abspath(error_report_path)}")
-        except Exception:
-            pass
-        raise
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="Profile TD-MPC2 agent performance")
-    parser.add_argument(
-        "--config",
-        type=str,
-        default="configs/curriculum_tdmpc2.json",
-        help="Path to curriculum config JSON file",
-    )
-    parser.add_argument(
-        "--device",
-        type=str,
-        default=None,
-        help="Device to use (cuda/cpu). Auto-detects if not specified.",
-    )
-    parser.add_argument(
-        "--output-dir",
-        type=str,
-        default="results/profiling",
-        help="Directory to save profiling traces",
-    )
-    parser.add_argument(
-        "--num-iterations",
-        type=int,
-        default=100,
-        help="Number of iterations to profile",
-    )
-    parser.add_argument(
-        "--export-traces",
-        action="store_true",
-        help="Export Chrome tracing files for visualization",
-    )
-    parser.add_argument(
-        "--gpu-id",
-        type=int,
-        default=None,
-        help="GPU ID to use (0, 1, 2, etc.). Uses default GPU if not specified.",
-    )
-
-    args = parser.parse_args()
-
-    main(
-        config_path=args.config,
-        device=args.device,
-        output_dir=args.output_dir,
-        num_iterations=args.num_iterations,
-        export_traces=args.export_traces,
-        gpu_id=args.gpu_id,
-    )
diff --git a/src/rl_hockey/TD_MPC2/tdmpc2.py b/src/rl_hockey/TD_MPC2/tdmpc2.py
deleted file mode 100644
index 15cc3ff..0000000
--- a/src/rl_hockey/TD_MPC2/tdmpc2.py
+++ /dev/null
@@ -1,1321 +0,0 @@
-# TD-MPC2 implementation
-
-import copy
-import logging
-
-import numpy as np
-import torch
-import torch.nn.functional as F
-from torch.func import functional_call
-
-if torch.cuda.is_available():
-    torch.set_float32_matmul_precision("high")
-
-from rl_hockey.common.agent import Agent
-from rl_hockey.common.buffer import TDMPC2ReplayBuffer
-from rl_hockey.TD_MPC2.model_dynamics_simple import DynamicsSimple
-from rl_hockey.TD_MPC2.model_encoder import Encoder
-from rl_hockey.TD_MPC2.model_init import (
-    init_dynamics,
-    init_encoder,
-    init_policy,
-    init_q_ensemble,
-    init_reward,
-    init_termination,
-)
-from rl_hockey.TD_MPC2.model_policy import Policy
-from rl_hockey.TD_MPC2.model_q_ensemble import QEnsemble
-from rl_hockey.TD_MPC2.model_reward import Reward
-from rl_hockey.TD_MPC2.model_termination import Termination
-from rl_hockey.TD_MPC2.mppi_planner_simple import MPPIPlannerSimplePaper
-from rl_hockey.TD_MPC2.util import RunningScale, soft_ce, two_hot_inv
-
-DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-logger = logging.getLogger(__name__)
-
-
-def _load_state_dict_compat(model, state_dict, strict=True):
-    """Load state_dict into model, handling torch.compile key mismatch.
-
-    Checkpoints saved from non-compiled models use keys like 'mlp.0.weight'.
-    Compiled models (OptimizedModule) expect '_orig_mod.mlp.0.weight'.
-    - Compiled model + plain keys: load into model._orig_mod.
-    - Non-compiled model + prefixed keys: strip '_orig_mod.' and load into model.
-    """
-    if not state_dict:
-        return None
-    first_key = next(iter(state_dict.keys()))
-    has_prefix = first_key.startswith("_orig_mod.")
-    if hasattr(model, "_orig_mod"):
-        if not has_prefix:
-            return model._orig_mod.load_state_dict(state_dict, strict=strict)
-    elif has_prefix:
-        stripped = {k.removeprefix("_orig_mod."): v for k, v in state_dict.items()}
-        return model.load_state_dict(stripped, strict=strict)
-    return model.load_state_dict(state_dict, strict=strict)
-
-
-class TDMPC2(Agent):
-    def __init__(
-        self,
-        obs_dim=18,
-        action_dim=8,
-        latent_dim=512,
-        hidden_dim=None,
-        num_q=5,
-        simnorm_temperature=1.0,
-        log_std_min=-10.0,
-        log_std_max=2.0,
-        lr=3e-4,
-        enc_lr_scale=0.3,
-        gamma=0.99,
-        lambda_coef=0.5,
-        entropy_coef=1e-4,
-        horizon=3,
-        num_samples=512,
-        num_iterations=6,
-        num_elites=64,
-        num_pi_trajs=24,
-        capacity=1000000,
-        temperature=0.5,
-        batch_size=256,
-        device="cuda",
-        num_bins=101,
-        vmin=-10.0,
-        vmax=10.0,
-        tau=0.01,
-        grad_clip_norm=20.0,
-        consistency_coef=20.0,
-        reward_coef=0.1,
-        value_coef=0.1,
-        termination_coef=0.5,
-        n_step=1,
-        win_reward_bonus=10.0,
-        win_reward_discount=0.92,
-        use_amp=True,
-    ):
-        super().__init__()
-        self.obs_dim = obs_dim
-        self.action_dim = action_dim
-        self.latent_dim = latent_dim
-
-        # Handle hidden_dim dict, filling in missing network types with defaults
-        if hidden_dim is None:
-            default_hidden_dim = [256, 256, 256]
-            hidden_dim = {
-                "encoder": default_hidden_dim,
-                "dynamics": default_hidden_dim,
-                "reward": default_hidden_dim,
-                "termination": default_hidden_dim,
-                "q_function": default_hidden_dim,
-                "policy": default_hidden_dim,
-            }
-
-        if not isinstance(hidden_dim, dict):
-            raise ValueError(
-                f"hidden_dim must be a dict with network-specific hidden dimensions, got {type(hidden_dim)}"
-            )
-
-        default_hidden_dim = [256, 256, 256]
-        self.hidden_dim = hidden_dim
-        self.hidden_dim_dict = {
-            "encoder": hidden_dim.get("encoder", default_hidden_dim),
-            "dynamics": hidden_dim.get("dynamics", default_hidden_dim),
-            "reward": hidden_dim.get("reward", default_hidden_dim),
-            "termination": hidden_dim.get("termination", default_hidden_dim),
-            "q_function": hidden_dim.get("q_function", default_hidden_dim),
-            "policy": hidden_dim.get("policy", default_hidden_dim),
-        }
-
-        self.num_q = num_q
-        self.simnorm_temperature = simnorm_temperature
-        self.log_std_min = log_std_min
-        self.log_std_max = log_std_max
-        self.lr = lr
-        self.enc_lr_scale = enc_lr_scale
-        self.gamma = gamma
-        self.lambda_coef = lambda_coef
-        self.entropy_coef = entropy_coef
-        self.horizon = horizon
-        self.num_samples = num_samples
-        self.num_iterations = num_iterations
-        self.num_elites = num_elites
-        self.num_pi_trajs = num_pi_trajs
-        self.temperature = temperature
-        self.capacity = capacity
-        self.device = torch.device(device)
-        self.num_bins = num_bins
-        self.vmin = vmin
-        self.vmax = vmax
-        self.tau = tau
-        self.grad_clip_norm = grad_clip_norm
-        self.consistency_coef = consistency_coef
-        self.reward_coef = reward_coef
-        self.value_coef = value_coef
-        self.termination_coef = termination_coef
-        self.n_step = n_step
-        self.use_amp = use_amp and torch.cuda.is_available()
-
-        # Initialize GradScaler for mixed precision training
-        if self.use_amp:
-            self.scaler = torch.amp.GradScaler("cuda")
-        else:
-            self.scaler = None
-
-        self.config = {
-            "batch_size": batch_size,
-            "learning_rate": lr,
-            "gamma": gamma,
-            "horizon": horizon,
-            "num_samples": num_samples,
-            "num_iterations": num_iterations,
-            "temperature": temperature,
-            "simnorm_temperature": simnorm_temperature,
-            "log_std_min": log_std_min,
-            "log_std_max": log_std_max,
-            "lambda_coef": lambda_coef,
-            "entropy_coef": entropy_coef,
-            "num_bins": num_bins,
-            "vmin": vmin,
-            "vmax": vmax,
-            "consistency_coef": consistency_coef,
-            "reward_coef": reward_coef,
-            "value_coef": value_coef,
-            "termination_coef": termination_coef,
-            "tau": tau,
-            "grad_clip_norm": grad_clip_norm,
-            "n_step": n_step,
-        }
-
-        self.encoder = Encoder(
-            obs_dim,
-            latent_dim,
-            self.hidden_dim_dict["encoder"],
-            simnorm_temperature=simnorm_temperature,
-        ).to(self.device)
-        self.dynamics = DynamicsSimple(
-            latent_dim,
-            action_dim,
-            self.hidden_dim_dict["dynamics"],
-            simnorm_temperature=simnorm_temperature,
-        ).to(self.device)
-        self.reward = Reward(
-            latent_dim,
-            action_dim,
-            self.hidden_dim_dict["reward"],
-            num_bins,
-            vmin=vmin,
-            vmax=vmax,
-        ).to(self.device)
-        self.termination = Termination(
-            latent_dim, self.hidden_dim_dict["termination"]
-        ).to(self.device)
-        self.q_ensemble = QEnsemble(
-            num_q,
-            latent_dim,
-            action_dim,
-            self.hidden_dim_dict["q_function"],
-            num_bins,
-            vmin,
-            vmax,
-        ).to(self.device)
-
-        self._init_detached_q_ensemble()
-
-        self.target_q_ensemble = copy.deepcopy(self.q_ensemble)
-        for param in self.target_q_ensemble.parameters():
-            param.requires_grad = False
-        self.policy = Policy(
-            latent_dim,
-            action_dim,
-            self.hidden_dim_dict["policy"],
-            log_std_min=log_std_min,
-            log_std_max=log_std_max,
-        ).to(self.device)
-
-        init_encoder(self.encoder)
-        init_dynamics(self.dynamics)
-        init_reward(self.reward)
-        init_termination(self.termination)
-        init_q_ensemble(self.q_ensemble)
-        init_policy(self.policy)
-
-        self.optimizer = torch.optim.Adam(
-            [
-                {
-                    "params": self.encoder.parameters(),
-                    "lr": self.lr * self.enc_lr_scale,
-                },
-                {"params": self.dynamics.parameters()},
-                {"params": self.reward.parameters()},
-                {"params": self.termination.parameters()},
-                {"params": self.q_ensemble.parameters()},
-            ],
-            lr=self.lr,
-            capturable=True,
-        )
-        self.pi_optimizer = torch.optim.Adam(
-            list(self.policy.parameters()), lr=self.lr, eps=1e-5, capturable=True
-        )
-
-        self.scale = RunningScale(tau=self.tau).to(self.device)
-
-        self.planner = MPPIPlannerSimplePaper(
-            self.dynamics,
-            self.reward,
-            self.termination,
-            self.target_q_ensemble,
-            self.policy,
-            horizon,
-            num_samples,
-            num_iterations,
-            temperature,
-            self.gamma,
-            std_init=2.0,
-            std_min=0.05,
-            num_bins=num_bins,
-            vmin=vmin,
-            vmax=vmax,
-            num_pi_trajs=num_pi_trajs,
-            num_elites=num_elites,
-        )
-        self.planners = None
-
-        self._model_params = (
-            list(self.encoder.parameters())
-            + list(self.dynamics.parameters())
-            + list(self.reward.parameters())
-            + list(self.termination.parameters())
-            + list(self.q_ensemble.parameters())
-        )
-
-        batch_size = self.config.get("batch_size", 256)
-        # Buffer storage device: default to CPU for larger capacity and negligible transfer overhead
-        # Set buffer_device="cuda" only if profiling shows transfer is a bottleneck (unlikely)
-        buffer_device = self.config.get("buffer_device", "cpu")
-        self.buffer = TDMPC2ReplayBuffer(
-            max_size=capacity,
-            horizon=self.horizon,
-            batch_size=batch_size,
-            use_torch_tensors=True,
-            device=self.device,  # Where sampled batches go (for training)
-            buffer_device=buffer_device,  # Where episode data is stored
-            pin_memory=(buffer_device == "cpu"),  # Pin memory for faster CPU->GPU transfer
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-        )
-        self._obs_buffer = torch.empty(
-            (batch_size, obs_dim), device=self.device, dtype=torch.float32
-        )
-        self._actions_buffer = torch.empty(
-            (batch_size, action_dim), device=self.device, dtype=torch.float32
-        )
-        self._rewards_buffer = torch.empty(
-            (batch_size, 1), device=self.device, dtype=torch.float32
-        )
-        self._next_obs_buffer = torch.empty(
-            (batch_size, obs_dim), device=self.device, dtype=torch.float32
-        )
-        self._dones_buffer = torch.empty(
-            (batch_size, 1), device=self.device, dtype=torch.float32
-        )
-
-        compile_available = hasattr(torch, "compile")
-        if compile_available:
-            try:
-                self.encoder = torch.compile(self.encoder, mode="reduce-overhead")
-                logger.debug("Successfully compiled encoder")
-            except Exception as e:
-                logger.warning(
-                    f"torch.compile failed for encoder: {e}. Using eager mode."
-                )
-
-            try:
-                self.dynamics = torch.compile(self.dynamics, mode="reduce-overhead")
-                logger.debug("Successfully compiled dynamics")
-            except Exception as e:
-                logger.warning(
-                    f"torch.compile failed for dynamics: {e}. Using eager mode."
-                )
-
-            try:
-                self.reward = torch.compile(self.reward, mode="reduce-overhead")
-                logger.debug("Successfully compiled reward")
-            except Exception as e:
-                logger.warning(
-                    f"torch.compile failed for reward: {e}. Using eager mode."
-                )
-
-            try:
-                self.termination = torch.compile(
-                    self.termination, mode="reduce-overhead"
-                )
-                logger.debug("Successfully compiled termination")
-            except Exception as e:
-                logger.warning(
-                    f"torch.compile failed for termination: {e}. Using eager mode."
-                )
-
-            try:
-                self.q_ensemble = torch.compile(self.q_ensemble, mode="reduce-overhead")
-                logger.debug("Successfully compiled q_ensemble")
-            except Exception as e:
-                logger.warning(
-                    f"torch.compile failed for q_ensemble: {e}. Using eager mode."
-                )
-
-            try:
-                self.policy = torch.compile(self.policy, mode="reduce-overhead")
-                logger.debug("Successfully compiled policy")
-            except Exception as e:
-                logger.warning(
-                    f"torch.compile failed for policy: {e}. Using eager mode."
-                )
-
-        self.planner.to(self.device)
-
-    def store_transition(self, transition, winner=None):
-        """Store a transition in the replay buffer.
-
-        Args:
-            transition: (state, action, reward, next_state, done)
-            winner: Optional winner information (1 for agent win, -1 for loss, 0 for draw).
-                Used for reward shaping in winning episodes.
-        """
-        self.buffer.store(transition, winner=winner)
-
-    @torch.no_grad()
-    def rollout_dynamics_multi_step(self, z0, action_sequence, max_horizon):
-        """
-        Roll out the dynamics model from z0 using the given action sequence.
-        Returns predicted latent states at 1, 2, ..., max_horizon steps ahead.
-
-        z0: (latent_dim,) numpy or tensor
-        action_sequence: list of (action_dim,) arrays, length >= max_horizon
-        max_horizon: int
-
-        Returns:
-            dict[int, np.ndarray]: {1: z_1, 2: z_2, ..., max_horizon: z_h}
-        """
-        if isinstance(z0, np.ndarray):
-            z = torch.FloatTensor(z0).to(self.device).unsqueeze(0)
-        else:
-            z = z0.to(self.device)
-            if z.dim() == 1:
-                z = z.unsqueeze(0)
-        out = {}
-        for h in range(1, max_horizon + 1):
-            a = action_sequence[h - 1]
-            if isinstance(a, np.ndarray):
-                a = torch.FloatTensor(a).to(self.device).unsqueeze(0)
-            else:
-                a = a.to(self.device)
-                if a.dim() == 1:
-                    a = a.unsqueeze(0)
-            z = self.dynamics(z, a)
-            out[h] = z.squeeze(0).cpu().numpy()
-        return out
-
-    @torch.no_grad()
-    def act(self, obs, deterministic=False, t0=False):
-        """Select action using MPC planning."""
-        obs = torch.FloatTensor(obs).to(self.device)
-
-        with torch.amp.autocast("cuda", enabled=self.use_amp):
-            z = self.encoder(obs.unsqueeze(0)).squeeze(0)
-            action = self.planner.plan(z, return_mean=deterministic, t0=t0)
-
-        return action.cpu().numpy()
-
-    @torch.no_grad()
-    def act_with_stats(
-        self,
-        obs,
-        deterministic=False,
-        prev_action=None,
-        prev_latent=None,
-        prev_predicted_next_latent=None,
-        t0=False,
-    ):
-        """Select action using MPC planning and collect statistics."""
-        obs_tensor = torch.FloatTensor(obs).to(self.device)
-        obs_batch = obs_tensor.unsqueeze(0)
-
-        with torch.amp.autocast("cuda", enabled=self.use_amp):
-            z = self.encoder(obs_batch).squeeze(0)
-            z = z.clone()
-
-            plan_result = self.planner.plan(
-                z, return_mean=deterministic, return_stats=True, t0=t0
-            )
-        if isinstance(plan_result, tuple):
-            action, planning_stats = plan_result
-        else:
-            action = plan_result
-            planning_stats = {}
-
-        action = action.clone()
-
-        stats = {}
-
-        stats["encoder_latent_norm"] = z.norm().item()
-        stats["encoder_latent_mean"] = z.mean().item()
-        stats["encoder_latent_std"] = z.std().item()
-        stats["_latent_state"] = z.cpu().numpy()
-
-        with torch.amp.autocast("cuda", enabled=self.use_amp):
-            policy_action = self.policy.mean_action(z.unsqueeze(0)).squeeze(0)
-
-            q_logits_list = self.q_ensemble(z.unsqueeze(0), action.unsqueeze(0))
-            from rl_hockey.TD_MPC2.util import two_hot_inv
-
-            q_values = []
-            for q_logits in q_logits_list:
-                q_val = two_hot_inv(
-                    q_logits, self.num_bins, self.vmin, self.vmax
-                ).item()
-                q_values.append(q_val)
-
-            stats["q_values"] = q_values
-            stats["q_min"] = min(q_values)
-            stats["q_max"] = max(q_values)
-            stats["q_mean"] = sum(q_values) / len(q_values)
-            q_std_val = (
-                torch.tensor(q_values).std().item() if len(q_values) > 1 else 0.0
-            )
-            stats["q_std"] = q_std_val
-
-            stats["q_spread"] = max(q_values) - min(q_values)
-            q_mean_val = stats["q_mean"]
-            stats["q_coefficient_of_variation"] = (
-                q_std_val / abs(q_mean_val) if abs(q_mean_val) > 1e-8 else 0.0
-            )
-
-            q_logits_policy = self.q_ensemble(
-                z.unsqueeze(0), policy_action.unsqueeze(0)
-            )
-            q_values_policy = []
-            for q_logits in q_logits_policy:
-                q_val = two_hot_inv(
-                    q_logits, self.num_bins, self.vmin, self.vmax
-                ).item()
-                q_values_policy.append(q_val)
-
-            stats["q_policy_values"] = q_values_policy
-            stats["q_policy_min"] = min(q_values_policy)
-            stats["q_policy_max"] = max(q_values_policy)
-            stats["q_policy_mean"] = sum(q_values_policy) / len(q_values_policy)
-
-            z_next_pred = self.dynamics(z.unsqueeze(0), action.unsqueeze(0)).squeeze(0)
-            latent_change = z_next_pred - z
-            stats["dynamics_latent_change_norm"] = latent_change.norm().item()
-            stats["dynamics_latent_change_mean"] = latent_change.mean().item()
-            stats["dynamics_latent_change_std"] = latent_change.std().item()
-            stats["dynamics_latent_next_norm"] = z_next_pred.norm().item()
-
-            if prev_predicted_next_latent is not None:
-                prev_pred_tensor = (
-                    prev_predicted_next_latent
-                    if isinstance(prev_predicted_next_latent, torch.Tensor)
-                    else torch.FloatTensor(prev_predicted_next_latent).to(self.device)
-                )
-                dynamics_error = z - prev_pred_tensor
-                stats["dynamics_prediction_error_norm"] = dynamics_error.norm().item()
-                stats["dynamics_prediction_error_mse"] = (
-                    (dynamics_error**2).mean().item()
-                )
-                stats["dynamics_prediction_error_mean"] = dynamics_error.mean().item()
-                stats["dynamics_prediction_error_std"] = dynamics_error.std().item()
-            else:
-                stats["dynamics_prediction_error_norm"] = None
-                stats["dynamics_prediction_error_mse"] = None
-                stats["dynamics_prediction_error_mean"] = None
-                stats["dynamics_prediction_error_std"] = None
-
-            stats["_predicted_next_latent"] = z_next_pred.clone().cpu().numpy()
-
-            reward_logits = self.reward(z.unsqueeze(0), action.unsqueeze(0))
-            reward_pred = two_hot_inv(
-                reward_logits, self.num_bins, self.vmin, self.vmax
-            ).item()
-            stats["reward_pred"] = reward_pred
-
-        stats["action_norm"] = action.norm().item()
-        stats["action_mean"] = action.mean().item()
-        stats["action_std"] = action.std().item()
-        stats["action_min"] = action.min().item()
-        stats["action_max"] = action.max().item()
-
-        action_diff = action - policy_action
-        stats["action_policy_diff_norm"] = action_diff.norm().item()
-        stats["action_policy_diff_mean"] = action_diff.mean().item()
-
-        if prev_action is not None:
-            prev_action_tensor = torch.FloatTensor(prev_action).to(self.device)
-            action_diff_temporal = action - prev_action_tensor
-            stats["action_smoothness"] = action_diff_temporal.norm().item()
-        else:
-            stats["action_smoothness"] = None
-
-        if prev_latent is not None:
-            prev_latent_tensor = (
-                prev_latent
-                if isinstance(prev_latent, torch.Tensor)
-                else torch.FloatTensor(prev_latent).to(self.device)
-            )
-            latent_diff_temporal = z - prev_latent_tensor
-            stats["latent_smoothness"] = latent_diff_temporal.norm().item()
-        else:
-            stats["latent_smoothness"] = None
-
-        if planning_stats:
-            stats["planning_stats"] = planning_stats
-            if "final_elite_returns" in planning_stats:
-                final_elite = planning_stats["final_elite_returns"]
-                stats["mppi_elite_return_min"] = final_elite["min"]
-                stats["mppi_elite_return_max"] = final_elite["max"]
-                stats["mppi_elite_return_mean"] = final_elite["mean"]
-                stats["mppi_elite_return_std"] = final_elite["std"]
-            if "final_std" in planning_stats:
-                stats["mppi_final_std"] = planning_stats["final_std"]
-            if "std_convergence" in planning_stats:
-                stats["mppi_std_convergence"] = planning_stats["std_convergence"]
-
-        return action.cpu().numpy(), stats
-
-    @torch.no_grad()
-    def act_batch(self, obs_batch, deterministic=False, t0s=None):
-        """Select actions for batch of observations."""
-        obs_batch = torch.FloatTensor(obs_batch).to(self.device)
-        num_envs = obs_batch.shape[0]
-
-        if self.planners is None or len(self.planners) != num_envs:
-            self.planners = [copy.deepcopy(self.planner) for _ in range(num_envs)]
-
-        with torch.amp.autocast("cuda", enabled=self.use_amp):
-            z_batch = self.encoder(obs_batch)
-            actions = []
-            if t0s is None:
-                t0s = [False] * num_envs
-
-            for i, (z, t0) in enumerate(zip(z_batch, t0s)):
-                action = self.planners[i].plan(z, return_mean=deterministic, t0=t0)
-                actions.append(action)
-
-        return torch.stack(actions).cpu().numpy()
-
-    def evaluate(self, obs):
-        """Evaluate state value using Q-ensemble."""
-        with torch.no_grad():
-            obs = torch.FloatTensor(obs).to(self.device)
-            with torch.amp.autocast("cuda", enabled=self.use_amp):
-                z = self.encoder(obs.unsqueeze(0))
-                action = self.policy.mean_action(z)
-                q_value = self.q_ensemble.min(z, action)
-
-            return q_value.item()
-
-    def train(self, steps=1):
-        all_losses = {
-            "consistency_loss": [],
-            "reward_loss": [],
-            "value_loss": [],
-            "termination_loss": [],
-            "policy_loss": [],
-            "total_loss": [],
-            "loss": [],
-            "grad_norm_encoder": [],
-            "grad_norm_dynamics": [],
-            "grad_norm_reward": [],
-            "grad_norm_termination": [],
-            "grad_norm_q_ensemble": [],
-            "grad_norm_policy": [],
-        }
-
-        for _ in range(steps):
-            # Mark step boundary for CUDAGraphs (like reference repo)
-            if hasattr(torch.compiler, "cudagraph_mark_step_begin"):
-                torch.compiler.cudagraph_mark_step_begin()
-
-            batch_size = self.config.get("batch_size", 256)
-
-            used_sequences = False
-            if hasattr(self.buffer, "sample_sequences") and self.horizon > 1:
-                seq = self.buffer.sample_sequences(batch_size, self.horizon)
-                if seq is not None:
-                    obs_seq, actions_seq, rewards_seq, dones_seq = seq
-
-                    if not isinstance(obs_seq, torch.Tensor):
-                        obs_seq = torch.from_numpy(obs_seq).float()
-                        actions_seq = torch.from_numpy(actions_seq).float()
-                        rewards_seq = torch.from_numpy(rewards_seq).float()
-                        dones_seq = torch.from_numpy(dones_seq).float()
-
-                    obs_seq = obs_seq.to(self.device, non_blocking=True)
-                    actions_seq = actions_seq.to(self.device, non_blocking=True)
-                    rewards_seq = rewards_seq.to(self.device, non_blocking=True)
-                    dones_seq = dones_seq.to(self.device, non_blocking=True)
-
-                    if rewards_seq.dim() == 2:
-                        rewards_seq = rewards_seq.unsqueeze(-1)
-                    if dones_seq.dim() == 2:
-                        dones_seq = dones_seq.unsqueeze(-1)
-
-                    batch_size_actual, horizon_plus_one, _ = obs_seq.shape
-                    horizon = horizon_plus_one - 1
-
-                    obs_flat = obs_seq.reshape(batch_size_actual * (horizon + 1), -1)
-                    with torch.amp.autocast("cuda", enabled=self.use_amp):
-                        z_flat = self.encoder(obs_flat)
-                    z_seq = z_flat.reshape(
-                        batch_size_actual, horizon + 1, self.latent_dim
-                    )
-
-                    # Pre-compute TD targets BEFORE the loop (like reference repo)
-                    # This batches all target computations
-                    with torch.no_grad():
-                        next_z = z_seq[:, 1:].detach()  # (batch, horizon, latent)
-                        # Compute TD targets for all timesteps at once
-                        td_targets = self._compute_td_targets(
-                            next_z, rewards_seq, dones_seq, horizon
-                        )
-
-                    # Latent rollout - SEQUENTIAL (autoregressive, like reference repo)
-                    # This is necessary because zs is used for policy update
-                    zs = torch.empty(
-                        horizon + 1,
-                        batch_size_actual,
-                        self.latent_dim,
-                        device=self.device,
-                    )
-                    z_pred = z_seq[:, 0].clone()
-                    zs[0] = z_pred
-                    consistency_loss = 0.0
-
-                    # Pre-compute lambda weights (avoid recomputation)
-                    lambda_weights = self.lambda_coef ** torch.arange(
-                        horizon, device=self.device, dtype=torch.float32
-                    )
-
-                    # Sequential dynamics rollout (like reference repo)
-                    for t in range(horizon):
-                        a_t = actions_seq[:, t]
-                        z_target = z_seq[:, t + 1].detach()
-
-                        with torch.amp.autocast("cuda", enabled=self.use_amp):
-                            z_next_pred = self.dynamics(z_pred, a_t)
-                        zs[t + 1] = z_next_pred
-
-                        # Consistency loss with pre-computed weight
-                        consistency_loss = consistency_loss + lambda_weights[t] * F.mse_loss(
-                            z_next_pred, z_target
-                        )
-
-                        z_pred = z_next_pred
-
-                    consistency_loss = consistency_loss / horizon
-
-                    # === BATCHED PREDICTIONS (like reference repo) ===
-                    # After building zs, do batched reward and Q predictions
-                    _zs = zs[:-1]  # (horizon, batch, latent) - states 0 to H-1
-                    _zs_flat = _zs.reshape(-1, self.latent_dim)  # (horizon*batch, latent)
-                    _actions_flat = actions_seq.permute(1, 0, 2).reshape(-1, self.action_dim)  # (horizon*batch, action)
-
-                    with torch.amp.autocast("cuda", enabled=self.use_amp):
-                        # Batched reward prediction
-                        reward_preds = self.reward(_zs_flat, _actions_flat)  # (horizon*batch, num_bins)
-                        # Batched Q prediction
-                        q_preds_all = self.q_ensemble(_zs_flat, _actions_flat)  # list of (horizon*batch, num_bins)
-
-                    # Compute reward loss with lambda weighting
-                    rewards_flat = rewards_seq.permute(1, 0, 2).reshape(-1, 1)  # (horizon*batch, 1)
-                    reward_loss_per_sample = soft_ce(
-                        reward_preds, rewards_flat, self.num_bins, self.vmin, self.vmax
-                    )  # (horizon*batch,)
-                    reward_loss_per_step = reward_loss_per_sample.reshape(horizon, batch_size_actual).mean(dim=1)
-                    reward_loss = (lambda_weights * reward_loss_per_step).sum() / horizon
-
-                    # Compute value loss with lambda weighting
-                    td_targets_flat = td_targets.permute(1, 0, 2).reshape(-1, 1)  # (horizon*batch, 1)
-                    value_loss = 0.0
-                    for q_pred in q_preds_all:
-                        value_loss_per_sample = soft_ce(
-                            q_pred, td_targets_flat, self.num_bins, self.vmin, self.vmax
-                        )
-                        value_loss_per_step = value_loss_per_sample.reshape(horizon, batch_size_actual).mean(dim=1)
-                        value_loss = value_loss + (lambda_weights * value_loss_per_step).sum()
-                    value_loss = value_loss / (horizon * self.num_q)
-                    zs_bh = zs[1:].permute(1, 0, 2).reshape(-1, self.latent_dim)
-                    with torch.amp.autocast("cuda", enabled=self.use_amp):
-                        termination_pred_logits = self.termination(zs_bh)
-                    termination_target = dones_seq.reshape(-1, 1)
-                    termination_loss = F.binary_cross_entropy_with_logits(
-                        termination_pred_logits, termination_target
-                    )
-
-                    used_sequences = True
-
-            if not used_sequences:
-                raise RuntimeError(
-                    "TD-MPC2 requires sequence sampling! "
-                    "ReplayBuffer.sample_sequences returned None or is missing. "
-                    "The agent cannot learn a consistent world model without sequences."
-                )
-
-            total_loss = (
-                self.consistency_coef * consistency_loss
-                + self.reward_coef * reward_loss
-                + self.value_coef * value_loss
-                + self.termination_coef * termination_loss
-            )
-
-            self.optimizer.zero_grad()
-
-            # Use scaler for mixed precision backward pass
-            if self.scaler is not None:
-                self.scaler.scale(total_loss).backward()
-
-                # Compute gradient norms before clipping (unscale first)
-                self.scaler.unscale_(self.optimizer)
-                grad_norm_encoder = torch.nn.utils.clip_grad_norm_(
-                    self.encoder.parameters(), max_norm=float("inf")
-                )
-                grad_norm_dynamics = torch.nn.utils.clip_grad_norm_(
-                    self.dynamics.parameters(), max_norm=float("inf")
-                )
-                grad_norm_reward = torch.nn.utils.clip_grad_norm_(
-                    self.reward.parameters(), max_norm=float("inf")
-                )
-                grad_norm_termination = torch.nn.utils.clip_grad_norm_(
-                    self.termination.parameters(), max_norm=float("inf")
-                )
-                grad_norm_q_ensemble = torch.nn.utils.clip_grad_norm_(
-                    self.q_ensemble.parameters(), max_norm=float("inf")
-                )
-
-                torch.nn.utils.clip_grad_norm_(
-                    self._model_params, max_norm=self.grad_clip_norm
-                )
-
-                self.scaler.step(self.optimizer)
-                self.scaler.update()
-            else:
-                total_loss.backward()
-
-                # Compute gradient norms before clipping
-                grad_norm_encoder = torch.nn.utils.clip_grad_norm_(
-                    self.encoder.parameters(), max_norm=float("inf")
-                )
-                grad_norm_dynamics = torch.nn.utils.clip_grad_norm_(
-                    self.dynamics.parameters(), max_norm=float("inf")
-                )
-                grad_norm_reward = torch.nn.utils.clip_grad_norm_(
-                    self.reward.parameters(), max_norm=float("inf")
-                )
-                grad_norm_termination = torch.nn.utils.clip_grad_norm_(
-                    self.termination.parameters(), max_norm=float("inf")
-                )
-                grad_norm_q_ensemble = torch.nn.utils.clip_grad_norm_(
-                    self.q_ensemble.parameters(), max_norm=float("inf")
-                )
-
-                torch.nn.utils.clip_grad_norm_(
-                    self._model_params, max_norm=self.grad_clip_norm
-                )
-                self.optimizer.step()
-
-            self.optimizer.zero_grad(set_to_none=True)
-
-            # Mark step boundary for CUDAGraphs to enable fast path
-            # This ensures all backward operations are complete before next forward pass
-            if hasattr(torch.compiler, "cudagraph_mark_step_begin"):
-                torch.compiler.cudagraph_mark_step_begin()
-
-            policy_loss, grad_norm_policy = self._update_policy(zs.detach())
-            self._update_target_network(tau=self.tau)
-            if steps == 1:
-                all_losses["consistency_loss"].append(consistency_loss.item())
-                all_losses["reward_loss"].append(reward_loss.item())
-                all_losses["value_loss"].append(value_loss.item())
-                all_losses["termination_loss"].append(termination_loss.item())
-                all_losses["policy_loss"].append(policy_loss.item())
-                all_losses["total_loss"].append(total_loss.item())
-                all_losses["loss"].append(total_loss.item())
-                all_losses["grad_norm_encoder"].append(grad_norm_encoder.item())
-                all_losses["grad_norm_dynamics"].append(grad_norm_dynamics.item())
-                all_losses["grad_norm_reward"].append(grad_norm_reward.item())
-                all_losses["grad_norm_termination"].append(grad_norm_termination.item())
-                all_losses["grad_norm_q_ensemble"].append(grad_norm_q_ensemble.item())
-                all_losses["grad_norm_policy"].append(grad_norm_policy.item())
-            else:
-                all_losses["consistency_loss"].append(consistency_loss)
-                all_losses["reward_loss"].append(reward_loss)
-                all_losses["value_loss"].append(value_loss)
-                all_losses["termination_loss"].append(termination_loss)
-                all_losses["policy_loss"].append(policy_loss)
-                all_losses["total_loss"].append(total_loss)
-                all_losses["loss"].append(total_loss)
-                all_losses["grad_norm_encoder"].append(grad_norm_encoder)
-                all_losses["grad_norm_dynamics"].append(grad_norm_dynamics)
-                all_losses["grad_norm_reward"].append(grad_norm_reward)
-                all_losses["grad_norm_termination"].append(grad_norm_termination)
-                all_losses["grad_norm_q_ensemble"].append(grad_norm_q_ensemble)
-                all_losses["grad_norm_policy"].append(grad_norm_policy)
-
-        if steps > 1 and len(all_losses["consistency_loss"]) > 0:
-            if isinstance(all_losses["consistency_loss"][0], torch.Tensor):
-                all_losses["consistency_loss"] = [
-                    loss.item() for loss in all_losses["consistency_loss"]
-                ]
-                all_losses["reward_loss"] = [
-                    loss.item() for loss in all_losses["reward_loss"]
-                ]
-                all_losses["value_loss"] = [
-                    loss.item() for loss in all_losses["value_loss"]
-                ]
-                all_losses["termination_loss"] = [
-                    loss.item() for loss in all_losses["termination_loss"]
-                ]
-                all_losses["policy_loss"] = [
-                    loss.item() for loss in all_losses["policy_loss"]
-                ]
-                all_losses["total_loss"] = [
-                    loss.item() for loss in all_losses["total_loss"]
-                ]
-                all_losses["loss"] = [loss.item() for loss in all_losses["loss"]]
-                all_losses["grad_norm_encoder"] = [
-                    grad.item() for grad in all_losses["grad_norm_encoder"]
-                ]
-                all_losses["grad_norm_dynamics"] = [
-                    grad.item() for grad in all_losses["grad_norm_dynamics"]
-                ]
-                all_losses["grad_norm_reward"] = [
-                    grad.item() for grad in all_losses["grad_norm_reward"]
-                ]
-                all_losses["grad_norm_termination"] = [
-                    grad.item() for grad in all_losses["grad_norm_termination"]
-                ]
-                all_losses["grad_norm_q_ensemble"] = [
-                    grad.item() for grad in all_losses["grad_norm_q_ensemble"]
-                ]
-                all_losses["grad_norm_policy"] = [
-                    grad.item() for grad in all_losses["grad_norm_policy"]
-                ]
-
-        return all_losses
-
-    @torch.no_grad()
-    def _compute_td_targets(self, next_z, rewards_seq, dones_seq, horizon):
-        """
-        Compute TD targets for all timesteps at once (like reference repo).
-
-        This batches the target computation instead of doing it inside the loop.
-
-        Args:
-            next_z: (batch, horizon, latent_dim) - encoded next observations
-            rewards_seq: (batch, horizon, 1) - rewards
-            dones_seq: (batch, horizon, 1) - termination flags
-            horizon: int - planning horizon
-
-        Returns:
-            td_targets: (batch, horizon, 1) - TD targets for each timestep
-        """
-        batch_size = next_z.shape[0]
-
-        # For n_step=1: simple 1-step TD targets
-        # For n_step>1: would need more complex handling at episode boundaries
-        if self.n_step == 1:
-            # Flatten for batched computation
-            next_z_flat = next_z.reshape(-1, self.latent_dim)  # (batch*horizon, latent)
-
-            with torch.amp.autocast("cuda", enabled=self.use_amp):
-                # Get policy actions for all next states
-                next_actions, _, _, _ = self.policy.sample(next_z_flat)
-                # Get target Q-values for all next states
-                target_q_flat = self.target_q_ensemble.min(next_z_flat, next_actions)
-
-            target_q = target_q_flat.reshape(batch_size, horizon, 1)
-
-            # TD target: r + gamma * (1 - done) * Q(s', a')
-            td_targets = rewards_seq + self.gamma * (1.0 - dones_seq) * target_q
-        else:
-            # n-step returns: compute for each timestep
-            # This is more complex due to variable-length returns at episode boundaries
-            td_targets = torch.empty(
-                batch_size, horizon, 1, device=self.device, dtype=rewards_seq.dtype
-            )
-            for t in range(horizon):
-                n = min(self.n_step, horizon - t)
-                gamma_powers = (
-                    self.gamma
-                    ** torch.arange(n, device=self.device, dtype=rewards_seq.dtype)
-                ).view(1, n, 1)
-                reward_sum = (rewards_seq[:, t : t + n] * gamma_powers).sum(dim=1)
-
-                z_bootstrap = next_z[:, min(t + n - 1, horizon - 1)]
-                with torch.amp.autocast("cuda", enabled=self.use_amp):
-                    next_action, _, _, _ = self.policy.sample(z_bootstrap)
-                    target_q = self.target_q_ensemble.min(z_bootstrap, next_action)
-
-                d_n = dones_seq[:, min(t + n - 1, horizon - 1)]
-                bootstrap = (self.gamma**n) * (1.0 - d_n) * target_q
-                td_targets[:, t] = reward_sum + bootstrap
-
-        return td_targets
-
-    def _init_detached_q_ensemble(self):
-        """
-        Initialize detached Q-ensemble.
-
-        Detachment is handled via stateless functional_call in _update_policy,
-        so this is intentionally a no-op aside from keeping the API stable.
-        """
-        return
-
-    def _q_ensemble_detached(self, latent, action):
-        """Forward through Q-ensemble with detached parameters."""
-        params = {
-            name: param.detach() for name, param in self.q_ensemble.named_parameters()
-        }
-        buffers = dict(self.q_ensemble.named_buffers())
-        return functional_call(self.q_ensemble, {**params, **buffers}, (latent, action))
-
-    def _update_policy(self, zs):
-        """Update policy to maximize Q-values + entropy."""
-        self.pi_optimizer.zero_grad()
-
-        # Mark step boundary for CUDAGraphs before forward pass
-        # This helps CUDAGraphs use the fast path by ensuring clean state
-        if hasattr(torch.compiler, "cudagraph_mark_step_begin"):
-            torch.compiler.cudagraph_mark_step_begin()
-
-        num_states = zs.shape[0]
-        batch_size = zs.shape[1]
-        zs_flat = zs.reshape(-1, zs.shape[-1])
-
-        with torch.amp.autocast("cuda", enabled=self.use_amp):
-            actions, log_probs, _, scaled_entropies = self.policy.sample(zs_flat)
-            q_logits_all = self._q_ensemble_detached(zs_flat, actions)
-
-            q_values = torch.stack(
-                [
-                    two_hot_inv(q_logits, self.num_bins, self.vmin, self.vmax)
-                    for q_logits in q_logits_all
-                ],
-                dim=0,
-            )
-            qs_flat = q_values.mean(dim=0)
-            qs = qs_flat.reshape(num_states, batch_size, 1)
-
-            if scaled_entropies.dim() == 1:
-                scaled_entropies = scaled_entropies.reshape(num_states, batch_size, 1)
-            else:
-                scaled_entropies = scaled_entropies.reshape(num_states, batch_size, -1)
-
-            self.scale.update(qs[0].detach())
-            qs_scaled = self.scale(qs)
-            rho = torch.pow(
-                torch.tensor(self.lambda_coef, device=self.device),
-                torch.arange(num_states, device=self.device),
-            )
-            pi_loss = (
-                -(self.entropy_coef * scaled_entropies + qs_scaled).mean(dim=(1, 2))
-                * rho
-            ).mean()
-
-        # Use scaler for mixed precision backward pass
-        if self.scaler is not None:
-            self.scaler.scale(pi_loss).backward()
-
-            # Compute gradient norm before clipping (unscale first)
-            self.scaler.unscale_(self.pi_optimizer)
-            grad_norm_policy = torch.nn.utils.clip_grad_norm_(
-                self.policy.parameters(), max_norm=float("inf")
-            )
-
-            torch.nn.utils.clip_grad_norm_(
-                self.policy.parameters(), max_norm=self.grad_clip_norm
-            )
-            self.scaler.step(self.pi_optimizer)
-            self.scaler.update()
-        else:
-            pi_loss.backward()
-
-            # Compute gradient norm before clipping
-            grad_norm_policy = torch.nn.utils.clip_grad_norm_(
-                self.policy.parameters(), max_norm=float("inf")
-            )
-
-            torch.nn.utils.clip_grad_norm_(
-                self.policy.parameters(), max_norm=self.grad_clip_norm
-            )
-            self.pi_optimizer.step()
-
-        self.pi_optimizer.zero_grad(set_to_none=True)
-
-        # Mark step boundary after policy update completes
-        if hasattr(torch.compiler, "cudagraph_mark_step_begin"):
-            torch.compiler.cudagraph_mark_step_begin()
-
-        return pi_loss.detach(), grad_norm_policy
-
-    def _update_target_network(self, tau=0.01):
-        with torch.no_grad():
-            source_params = list(self.q_ensemble.parameters())
-            target_params = list(self.target_q_ensemble.parameters())
-            for param, target_param in zip(source_params, target_params):
-                target_param.data.mul_(1 - tau).add_(param.data, alpha=tau)
-
-    def save(self, path):
-        """Save agent"""
-        torch.save(
-            {
-                "encoder": self.encoder.state_dict(),
-                "dynamics": self.dynamics.state_dict(),
-                "reward": self.reward.state_dict(),
-                "termination": self.termination.state_dict(),
-                "q_ensemble": self.q_ensemble.state_dict(),
-                "policy": self.policy.state_dict(),
-                "optimizer": self.optimizer.state_dict(),
-                "pi_optimizer": self.pi_optimizer.state_dict(),
-                "scale": self.scale.state_dict(),
-                "config": self.config,
-                "obs_dim": self.obs_dim,
-                "action_dim": self.action_dim,
-                "latent_dim": self.latent_dim,
-                "hidden_dim": self.hidden_dim,
-                "num_q": self.num_q,
-                "num_bins": self.num_bins,
-                "vmin": self.vmin,
-                "vmax": self.vmax,
-            },
-            path,
-        )
-
-    def load(self, path):
-        """Load agent"""
-        checkpoint = torch.load(path, map_location="cpu")
-
-        checkpoint_num_bins = checkpoint.get("num_bins") or checkpoint.get(
-            "config", {}
-        ).get("num_bins")
-        if checkpoint_num_bins is not None and checkpoint_num_bins != self.num_bins:
-            logger.warning(
-                f"Architecture mismatch detected: checkpoint has num_bins={checkpoint_num_bins}, "
-                f"but current model has num_bins={self.num_bins}. "
-                f"Reward and Q-function output layers will be initialized from scratch."
-            )
-
-        checkpoint_latent_dim = checkpoint.get("latent_dim")
-        checkpoint_action_dim = checkpoint.get("action_dim")
-        if (
-            checkpoint_latent_dim is not None
-            and checkpoint_latent_dim != self.latent_dim
-        ):
-            raise ValueError(
-                f"Cannot load checkpoint: latent_dim mismatch "
-                f"(checkpoint: {checkpoint_latent_dim}, current: {self.latent_dim})"
-            )
-        if (
-            checkpoint_action_dim is not None
-            and checkpoint_action_dim != self.action_dim
-        ):
-            raise ValueError(
-                f"Cannot load checkpoint: action_dim mismatch "
-                f"(checkpoint: {checkpoint_action_dim}, current: {self.action_dim})"
-            )
-
-        _load_state_dict_compat(self.encoder, checkpoint["encoder"])
-        _load_state_dict_compat(self.dynamics, checkpoint["dynamics"])
-
-        if checkpoint_num_bins is not None and checkpoint_num_bins != self.num_bins:
-            load_result = _load_state_dict_compat(
-                self.reward, checkpoint["reward"], strict=False
-            )
-            missing_keys, unexpected_keys = load_result.missing_keys, load_result.unexpected_keys
-            if missing_keys:
-                logger.debug(f"Reward model missing keys (expected): {missing_keys}")
-            if unexpected_keys:
-                logger.debug(f"Reward model unexpected keys: {unexpected_keys}")
-            logger.info(
-                f"Reward model loaded with architecture adaptation "
-                f"(num_bins: {checkpoint_num_bins} -> {self.num_bins}). "
-                f"Output layer initialized from scratch."
-            )
-        else:
-            _load_state_dict_compat(self.reward, checkpoint["reward"])
-
-        if "termination" in checkpoint:
-            _load_state_dict_compat(self.termination, checkpoint["termination"])
-        else:
-            logger.warning(
-                "Termination model not found in checkpoint, initializing from scratch."
-            )
-
-        if checkpoint_num_bins is not None and checkpoint_num_bins != self.num_bins:
-            load_result = _load_state_dict_compat(
-                self.q_ensemble, checkpoint["q_ensemble"], strict=False
-            )
-            missing_keys, unexpected_keys = load_result.missing_keys, load_result.unexpected_keys
-            if missing_keys:
-                logger.debug(f"Q ensemble missing keys (expected): {missing_keys}")
-            if unexpected_keys:
-                logger.debug(f"Q ensemble unexpected keys: {unexpected_keys}")
-            logger.info(
-                f"Q ensemble loaded with architecture adaptation "
-                f"(num_bins: {checkpoint_num_bins} -> {self.num_bins}). "
-                f"Output layers initialized from scratch."
-            )
-        else:
-            _load_state_dict_compat(self.q_ensemble, checkpoint["q_ensemble"])
-
-        _load_state_dict_compat(self.policy, checkpoint["policy"])
-
-        try:
-            self.optimizer.load_state_dict(checkpoint["optimizer"])
-            self.pi_optimizer.load_state_dict(checkpoint["pi_optimizer"])
-        except Exception as e:
-            logger.warning(f"Could not load optimizer state: {e}")
-
-        if "scale" in checkpoint:
-            try:
-                self.scale.load_state_dict(checkpoint["scale"])
-            except Exception as e:
-                logger.warning(f"Could not load RunningScale state: {e}")
-
-        if checkpoint_num_bins is not None and checkpoint_num_bins != self.num_bins:
-            load_result = _load_state_dict_compat(
-                self.target_q_ensemble, checkpoint["q_ensemble"], strict=False
-            )
-            missing_keys, unexpected_keys = load_result.missing_keys, load_result.unexpected_keys
-            if missing_keys:
-                logger.debug(
-                    f"Target Q ensemble missing keys (expected): {missing_keys}"
-                )
-            if unexpected_keys:
-                logger.debug(f"Target Q ensemble unexpected keys: {unexpected_keys}")
-        else:
-            _load_state_dict_compat(self.target_q_ensemble, checkpoint["q_ensemble"])
-
-        self._init_detached_q_ensemble()
-
-    def log_architecture(self):
-        """Log network architecture."""
-
-        def count_parameters(model):
-            return sum(p.numel() for p in model.parameters() if p.requires_grad)
-
-        lines = []
-        lines.append("=" * 80)
-        lines.append("TD-MPC2 AGENT ARCHITECTURE")
-        lines.append("=" * 80)
-        lines.append(f"Observation Dimension: {self.obs_dim}")
-        lines.append(f"Action Dimension: {self.action_dim}")
-        lines.append(f"Latent Dimension: {self.latent_dim}")
-        lines.append("Hidden Dimensions (per network):")
-        for network_type, hidden_dims in self.hidden_dim_dict.items():
-            lines.append(f"  {network_type}: {hidden_dims}")
-        lines.append(f"Device: {self.device}")
-        lines.append("")
-
-        lines.append("Configuration:")
-        for key, value in self.config.items():
-            lines.append(f"  {key}: {value}")
-        lines.append("")
-
-        lines.append("MODEL-BASED RL COMPONENTS:")
-        lines.append("")
-
-        lines.append("1. ENCODER NETWORK:")
-        lines.append("   Maps observations to latent state representations")
-        lines.append(str(self.encoder))
-        lines.append(f"   Trainable Parameters: {count_parameters(self.encoder):,}")
-        lines.append("")
-
-        lines.append("2. DYNAMICS MODEL:")
-        lines.append("   Predicts next latent state given current state and action")
-        lines.append(str(self.dynamics))
-        lines.append(f"   Trainable Parameters: {count_parameters(self.dynamics):,}")
-        lines.append("")
-
-        lines.append("3. REWARD MODEL:")
-        lines.append("   Predicts reward given latent state and action")
-        lines.append(str(self.reward))
-        lines.append(f"   Trainable Parameters: {count_parameters(self.reward):,}")
-        lines.append("")
-
-        lines.append("3a. TERMINATION MODEL:")
-        lines.append(
-            "   Predicts termination probability given latent state and action"
-        )
-        lines.append(str(self.termination))
-        lines.append(f"   Trainable Parameters: {count_parameters(self.termination):,}")
-        lines.append("")
-
-        lines.append("4. Q ENSEMBLE:")
-        lines.append(f"   {self.num_q} Q-networks for value estimation")
-        lines.append(str(self.q_ensemble))
-        lines.append(f"   Trainable Parameters: {count_parameters(self.q_ensemble):,}")
-        lines.append("")
-
-        lines.append("5. TARGET Q ENSEMBLE:")
-        lines.append("   Target network for stable Q-learning")
-        lines.append("   Same architecture as Q Ensemble")
-        lines.append(
-            f"   Trainable Parameters: {count_parameters(self.target_q_ensemble):,}"
-        )
-        lines.append("")
-
-        lines.append("6. POLICY NETWORK:")
-        lines.append("   Learns to mimic the MPC planner for fast inference")
-        lines.append(str(self.policy))
-        lines.append(f"   Trainable Parameters: {count_parameters(self.policy):,}")
-        lines.append("")
-
-        model_params = (
-            count_parameters(self.encoder)
-            + count_parameters(self.dynamics)
-            + count_parameters(self.reward)
-            + count_parameters(self.termination)
-            + count_parameters(self.q_ensemble)
-        )
-        total_params = (
-            model_params
-            + count_parameters(self.policy)
-            + count_parameters(self.target_q_ensemble)
-        )
-
-        lines.append("PARAMETER SUMMARY:")
-        lines.append(
-            f"  World Model (Encoder + Dynamics + Reward + Termination + Q): {model_params:,}"
-        )
-        lines.append(f"  Policy Network: {count_parameters(self.policy):,}")
-        lines.append(
-            f"  Target Q Network: {count_parameters(self.target_q_ensemble):,}"
-        )
-        lines.append(f"  TOTAL TRAINABLE PARAMETERS: {total_params:,}")
-        lines.append("")
-
-        lines.append("OPTIMIZERS:")
-        lines.append(
-            f"  World Model + Q Optimizer: {self.optimizer.__class__.__name__} (LR: {self.lr})"
-        )
-        lines.append(
-            f"  Policy Optimizer: {self.pi_optimizer.__class__.__name__} (LR: {self.lr})"
-        )
-        lines.append("")
-
-        lines.append("MPC PLANNER:")
-        lines.append("  Type: MPPI (Model Predictive Path Integral)")
-        lines.append(f"  Horizon: {self.horizon}")
-        lines.append(f"  Samples per iteration: {self.num_samples}")
-        lines.append(f"  Planning iterations: {self.num_iterations}")
-        lines.append(f"  Temperature: {self.temperature}")
-        if hasattr(self, "fast_mode"):
-            lines.append(f"  Fast Mode: {self.fast_mode} (use policy network only)")
-        lines.append("")
-
-        lines.append("=" * 80)
-
-        return "\n".join(lines)
diff --git a/src/rl_hockey/TD_MPC2/util.py b/src/rl_hockey/TD_MPC2/util.py
deleted file mode 100644
index 254ee63..0000000
--- a/src/rl_hockey/TD_MPC2/util.py
+++ /dev/null
@@ -1,136 +0,0 @@
-# Utility functions for TD-MPC2.
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-
-class SimNorm(nn.Module):
-    """Simplicial normalization layer."""
-
-    def __init__(self, dim, simplex_dim=8, temperature=1.0):
-        super().__init__()
-        self.dim = dim
-        self.simplex_dim = simplex_dim
-        self.temperature = temperature
-
-    def forward(self, x):
-        if x.dim() != 2 or x.shape[1] != self.dim:
-            raise ValueError(
-                f"Input x must have shape (batch, {self.dim}), got {tuple(x.shape)}"
-            )
-        if self.dim % self.simplex_dim != 0:
-            raise ValueError(
-                f"dim ({self.dim}) must be divisible by simplex_dim ({self.simplex_dim})"
-            )
-
-        group = self.dim // self.simplex_dim
-        x = x.reshape(x.shape[0], group, self.simplex_dim)
-        x = F.softmax(x / self.temperature, dim=2)
-        x = x.reshape(x.shape[0], self.dim)
-
-        return x
-
-
-def symlog(x):
-    """Symmetric logarithmic function."""
-    return torch.sign(x) * torch.log(1 + torch.abs(x))
-
-
-def symexp(x):
-    """Symmetric exponential function."""
-    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)
-
-
-def two_hot(x, num_bins, vmin, vmax):
-    """Converts scalars to soft two-hot encoded targets."""
-    if num_bins == 0:
-        return x
-    elif num_bins == 1:
-        return symlog(x)
-    
-    if x.dim() == 1:
-        x = x.unsqueeze(-1)
-    
-    x_symlog = torch.clamp(symlog(x), vmin, vmax).squeeze(1)
-    bin_size = (vmax - vmin) / (num_bins - 1)
-    bin_idx = torch.floor((x_symlog - vmin) / bin_size)
-    bin_offset = ((x_symlog - vmin) / bin_size - bin_idx).unsqueeze(-1)
-    
-    bin_idx = bin_idx.long()
-    
-    soft_two_hot = torch.zeros(x.shape[0], num_bins, device=x.device, dtype=x.dtype)
-    soft_two_hot = soft_two_hot.scatter(1, bin_idx.unsqueeze(1), 1 - bin_offset)
-    soft_two_hot = soft_two_hot.scatter(
-        1, (bin_idx.unsqueeze(1) + 1) % num_bins, bin_offset
-    )
-    
-    return soft_two_hot
-
-
-def two_hot_inv(x, num_bins, vmin, vmax):
-    """Converts soft two-hot encoded vectors to scalars."""
-    if num_bins == 0:
-        return x
-    elif num_bins == 1:
-        return symexp(x)
-    
-    x_probs = F.softmax(x, dim=-1)
-    dreg_bins = torch.linspace(vmin, vmax, num_bins, device=x.device, dtype=x.dtype)
-    x_symlog = torch.sum(x_probs * dreg_bins, dim=-1, keepdim=True)
-    return symexp(x_symlog)
-
-
-def soft_ce(pred, target, num_bins, vmin, vmax):
-    """Computes cross entropy loss between predictions and soft targets."""
-    target_two_hot = two_hot(target, num_bins, vmin, vmax)
-    pred_log_softmax = F.log_softmax(pred, dim=-1)
-    loss = -(target_two_hot * pred_log_softmax).sum(-1, keepdim=True)
-    
-    return loss
-
-
-class RunningScale(torch.nn.Module):
-    """Running trimmed scale estimator for normalizing Q-values."""
-
-    def __init__(self, tau=0.01):
-        super().__init__()
-        self.tau = tau
-        self.register_buffer("value", torch.ones(1, dtype=torch.float32))
-        self.register_buffer("_percentiles", torch.tensor([5, 95], dtype=torch.float32))
-
-    def _positions(self, x_shape):
-        positions = self._percentiles * (x_shape - 1) / 100
-        floored = torch.floor(positions)
-        ceiled = floored + 1
-        ceiled = torch.where(ceiled > x_shape - 1, x_shape - 1, ceiled)
-        weight_ceiled = positions - floored
-        weight_floored = 1.0 - weight_ceiled
-        return floored.long(), ceiled.long(), weight_floored.unsqueeze(1), weight_ceiled.unsqueeze(1)
-
-    def _percentile(self, x):
-        x_dtype, x_shape = x.dtype, x.shape
-        x = x.flatten(1, x.ndim - 1) if x.ndim > 1 else x.unsqueeze(0)
-        in_sorted = torch.sort(x, dim=0).values
-        floored, ceiled, weight_floored, weight_ceiled = self._positions(x.shape[0])
-        d0 = in_sorted[floored] * weight_floored
-        d1 = in_sorted[ceiled] * weight_ceiled
-        return (d0 + d1).reshape(-1, *x_shape[1:]).to(x_dtype)
-
-    def update(self, x):
-        percentiles = self._percentile(x.detach())
-        value = torch.clamp(percentiles[1] - percentiles[0], min=1.0)
-        self.value.data.lerp_(value, self.tau)
-
-    def forward(self, x):
-        return x / self.value
-
-    def __repr__(self):
-        return f"RunningScale(value={self.value.item():.4f})"
-
-
-def gumbel_softmax_sample(logits, temperature=1.0):
-    """Sample from the Gumbel-Softmax distribution."""
-    gumbels = -torch.empty_like(logits).exponential_().log()
-    gumbels = (logits.log() + gumbels) / temperature
-    y_soft = gumbels.softmax(dim=-1)
-    return y_soft.argmax(-1)
diff --git a/src/rl_hockey/TD_MPC2_repo/LICENSE b/src/rl_hockey/TD_MPC2_repo/LICENSE
deleted file mode 100644
index bc27ced..0000000
--- a/src/rl_hockey/TD_MPC2_repo/LICENSE
+++ /dev/null
@@ -1,21 +0,0 @@
-MIT License
-
-Copyright (c) Nicklas Hansen (2023).
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/src/rl_hockey/TD_MPC2_repo/README.md b/src/rl_hockey/TD_MPC2_repo/README.md
deleted file mode 100755
index ddf858a..0000000
--- a/src/rl_hockey/TD_MPC2_repo/README.md
+++ /dev/null
@@ -1,157 +0,0 @@
-<h1>TD-MPC2</span></h1>
-
-Official implementation of
-
-[TD-MPC2: Scalable, Robust World Models for Continuous Control](https://www.tdmpc2.com) by
-
-[Nicklas Hansen](https://nicklashansen.github.io), [Hao Su](https://cseweb.ucsd.edu/~haosu)\*, [Xiaolong Wang](https://xiaolonw.github.io)\* (UC San Diego)</br>
-
-<img src="assets/0.gif" width="12.5%"><img src="assets/1.gif" width="12.5%"><img src="assets/2.gif" width="12.5%"><img src="assets/3.gif" width="12.5%"><img src="assets/4.gif" width="12.5%"><img src="assets/5.gif" width="12.5%"><img src="assets/6.gif" width="12.5%"><img src="assets/7.gif" width="12.5%"></br>
-
-[[Website]](https://www.tdmpc2.com) [[Paper]](https://arxiv.org/abs/2310.16828) [[Models]](https://www.tdmpc2.com/models)  [[Dataset]](https://www.tdmpc2.com/dataset)
-
-----
-
-**Announcement (Apr 2025): support for episodic tasks!**
-
-We have added support for episodic RL (tasks with terminations) in the latest release. This functionality can be enabled with `episodic=true` but remains disabled by default to ensure reproducibility of results across releases.
-
-----
-
-
-## Overview
-
-TD-MPC**2** is a scalable, robust model-based reinforcement learning algorithm. It compares favorably to existing model-free and model-based methods across **104** continuous control tasks spanning multiple domains, with a *single* set of hyperparameters (*right*). We further demonstrate the scalability of TD-MPC**2** by training a single 317M parameter agent to perform **80** tasks across multiple domains, embodiments, and action spaces (*left*). 
-
-<img src="assets/8.png" width="100%" style="max-width: 640px"><br/>
-
-This repository contains code for training and evaluating both single-task online RL and multi-task offline RL TD-MPC**2** agents. We additionally open-source **300+** [model checkpoints](https://www.tdmpc2.com/models) (including 12 multi-task models) across 4 task domains: [DMControl](https://arxiv.org/abs/1801.00690), [Meta-World](https://meta-world.github.io/), [ManiSkill2](https://maniskill2.github.io/), and [MyoSuite](https://sites.google.com/view/myosuite), as well as our [30-task and 80-task datasets](https://www.tdmpc2.com/dataset) used to train the multi-task models. Our codebase supports both state and pixel observations. We hope that this repository will serve as a useful community resource for future research on model-based RL.
-
-----
-
-## Getting started
-
-You will need a machine with a GPU and at least 12 GB of RAM for single-task online RL with TD-MPC**2**, and 128 GB of RAM for multi-task offline RL on our provided 80-task dataset. A GPU with at least 8 GB of memory is recommended for single-task online RL and for evaluation of the provided multi-task models (up to 317M parameters). Training of the 317M parameter model requires a GPU with at least 24 GB of memory.
-
-We provide a `Dockerfile` for easy installation. You can build the docker image by running
-
-```
-cd docker && docker build . -t <user>/tdmpc2:1.0.1
-```
-
-This docker image contains all dependencies needed for running DMControl. We also provide a pre-built docker image [here](https://hub.docker.com/repository/docker/nicklashansen/tdmpc2/tags/1.0.1/sha256-b07d4e04d4b28ffd9a63ac18ec1541950e874bb51d276c7d09b36135f170dd93).
-
-If you prefer to use `conda` rather than docker, start by running the following command:
-
-```
-conda env create -f docker/environment.yaml
-```
-
-The `docker/environment.yaml` file installs dependencies required for training on DMControl tasks. Other domains can be installed by following the instructions in `docker/environment.yaml`.
-
-If you want to run ManiSkill2, you will additionally need to download and link the necessary assets by running
-
-```
-python -m mani_skill2.utils.download_asset all
-```
-
-which downloads assets to `./data`. You may move these assets to any location. Then, add the following line to your `~/.bashrc`:
-
-```
-export MS2_ASSET_DIR=<path>/<to>/<data>
-```
-
-and restart your terminal. Note that Meta-World requires MuJoCo 2.1.0 and `gym==0.21.0` which is becoming increasingly difficult to install. We host the unrestricted MuJoCo 2.1.0 license (courtesy of Google DeepMind) at [https://www.tdmpc2.com/files/mjkey.txt](https://www.tdmpc2.com/files/mjkey.txt). You can download the license by running
-
-```
-wget https://www.tdmpc2.com/files/mjkey.txt -O ~/.mujoco/mjkey.txt
-```
-
-Depending on your existing system packages, you may need to install other dependencies. See `docker/Dockerfile` for a list of recommended system packages.
-
-----
-
-## Supported tasks
-
-This codebase provides support for all **104** continuous control tasks from **DMControl**, **Meta-World**, **ManiSkill2**, and **MyoSuite** used in our paper. Specifically, it supports 39 tasks from DMControl (including 11 custom tasks), 50 tasks from Meta-World, 5 tasks from ManiSkill2, and 10 tasks from MyoSuite, and covers all tasks used in the paper. See below table for expected name formatting for each task domain:
-
-| domain | task
-| --- | --- |
-| dmcontrol | dog-run
-| dmcontrol | cheetah-run-backwards
-| metaworld | mw-assembly
-| metaworld | mw-pick-place-wall
-| maniskill | pick-cube
-| maniskill | pick-ycb
-| myosuite  | myo-key-turn
-| myosuite  | myo-key-turn-hard
-
-which can be run by specifying the `task` argument for `evaluation.py`. Multi-task training and evaluation is specified by setting `task=mt80` or `task=mt30` for the 80-task and 30-task sets, respectively. While you generally do not need to access the underlying task IDs or embeddings during training or evaluation of our multi-task models, the mapping from task name to task embedding used in our work can be found [here](https://github.com/nicklashansen/tdmpc2/blob/7ec6bc83a82a5188ca3faddc59aea83f430ab570/tdmpc2/common/__init__.py#L26). As of April 2025, our codebase also provides basic support for other MuJoCo/Box2d Gymnasium tasks; refer to the `envs` directory for a list of tasks. It should be relatively straightforward to add support for custom tasks by following the examples in `envs`.
-
-**Note:** we also provide support for image observations in the DMControl tasks. Use argument `obs=rgb` if you wish to train visual policies.
-
-
-## Example usage
-
-We provide examples on how to evaluate our provided TD-MPC**2** checkpoints, as well as how to train your own TD-MPC**2** agents, below.
-
-### Evaluation
-
-See below examples on how to evaluate downloaded single-task and multi-task checkpoints.
-
-```
-$ python evaluate.py task=mt80 model_size=48 checkpoint=/path/to/mt80-48M.pt
-$ python evaluate.py task=mt30 model_size=317 checkpoint=/path/to/mt30-317M.pt
-$ python evaluate.py task=dog-run checkpoint=/path/to/dog-1.pt save_video=true
-```
-
-All single-task checkpoints expect `model_size=5`. Multi-task checkpoints are available in multiple model sizes. Available arguments are `model_size={1, 5, 19, 48, 317}`. Note that single-task evaluation of multi-task checkpoints is currently not supported. See `config.yaml` for a full list of arguments.
-
-### Training
-
-See below examples on how to train TD-MPC**2** on a single task (online RL) and on multi-task datasets (offline RL). We recommend configuring [Weights and Biases](https://wandb.ai) (`wandb`) in `config.yaml` to track training progress.
-
-```
-$ python train.py task=mt80 model_size=48 batch_size=1024
-$ python train.py task=mt30 model_size=317 batch_size=1024
-$ python train.py task=dog-run steps=7000000
-$ python train.py task=walker-walk obs=rgb
-```
-
-We recommend using default hyperparameters for single-task online RL, including the default model size of 5M parameters (`model_size=5`). Multi-task offline RL benefits from a larger model size, but larger models are also increasingly costly to train and evaluate. Available arguments are `model_size={1, 5, 19, 48, 317}`. See `config.yaml` for a full list of arguments.
-
-----
-
-## Citation
-
-If you find our work useful, please consider citing our paper as follows:
-
-```
-@inproceedings{hansen2024tdmpc2,
-  title={TD-MPC2: Scalable, Robust World Models for Continuous Control}, 
-  author={Nicklas Hansen and Hao Su and Xiaolong Wang},
-  booktitle={International Conference on Learning Representations (ICLR)},
-  year={2024}
-}
-```
-as well as the original TD-MPC paper:
-```
-@inproceedings{hansen2022tdmpc,
-  title={Temporal Difference Learning for Model Predictive Control},
-  author={Nicklas Hansen and Xiaolong Wang and Hao Su},
-  booktitle={International Conference on Machine Learning (ICML)},
-  year={2022}
-}
-```
-
-----
-
-## Contributing
-
-You are very welcome to contribute to this project. Feel free to open an issue or pull request if you have any suggestions or bug reports, but please review our [guidelines](CONTRIBUTING.md) first. Our goal is to build a codebase that can easily be extended to new environments and tasks, and we would love to hear about your experience!
-
-----
-
-## License
-
-This project is licensed under the MIT License - see the `LICENSE` file for details. Note that the repository relies on third-party code, which is subject to their respective licenses.
diff --git a/src/rl_hockey/TD_MPC2_repo/__init__.py b/src/rl_hockey/TD_MPC2_repo/__init__.py
deleted file mode 100644
index 128eef2..0000000
--- a/src/rl_hockey/TD_MPC2_repo/__init__.py
+++ /dev/null
@@ -1,7 +0,0 @@
-"""
-TD_MPC2_repo package - Reference implementation wrapper for TD-MPC2.
-"""
-
-from rl_hockey.TD_MPC2_repo.tdmpc2_repo_wrapper import TDMPC2RepoWrapper
-
-__all__ = ['TDMPC2RepoWrapper']
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/__init__.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/__init__.py
deleted file mode 100755
index e69de29..0000000
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/__init__.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/__init__.py
deleted file mode 100644
index 7fa5309..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/__init__.py
+++ /dev/null
@@ -1,60 +0,0 @@
-MODEL_SIZE = { # parameters (M)
-	1:   {'enc_dim': 256,
-		  'mlp_dim': 384,
-		  'latent_dim': 128,
-		  'num_enc_layers': 2,
-		  'num_q': 2},
-	5:   {'enc_dim': 256,
-		  'mlp_dim': 512,
-		  'latent_dim': 512,
-		  'num_enc_layers': 2},
-	19:  {'enc_dim': 1024,
-		  'mlp_dim': 1024,
-		  'latent_dim': 768,
-		  'num_enc_layers': 3},
-	48:  {'enc_dim': 1792,
-		  'mlp_dim': 1792,
-		  'latent_dim': 768,
-		  'num_enc_layers': 4},
-	317: {'enc_dim': 4096,
-		  'mlp_dim': 4096,
-		  'latent_dim': 1376,
-		  'num_enc_layers': 5,
-		  'num_q': 8},
-}
-
-TASK_SET = {
-	'mt30': [
-		# 19 original dmcontrol tasks
-		'walker-stand', 'walker-walk', 'walker-run', 'cheetah-run', 'reacher-easy',
-	    'reacher-hard', 'acrobot-swingup', 'pendulum-swingup', 'cartpole-balance', 'cartpole-balance-sparse',
-		'cartpole-swingup', 'cartpole-swingup-sparse', 'cup-catch', 'finger-spin', 'finger-turn-easy',
-		'finger-turn-hard', 'fish-swim', 'hopper-stand', 'hopper-hop',
-		# 11 custom dmcontrol tasks
-		'walker-walk-backwards', 'walker-run-backwards', 'cheetah-run-backwards', 'cheetah-run-front', 'cheetah-run-back',
-		'cheetah-jump', 'hopper-hop-backwards', 'reacher-three-easy', 'reacher-three-hard', 'cup-spin',
-		'pendulum-spin',
-	],
-	'mt80': [
-		# 19 original dmcontrol tasks
-		'walker-stand', 'walker-walk', 'walker-run', 'cheetah-run', 'reacher-easy',
-	    'reacher-hard', 'acrobot-swingup', 'pendulum-swingup', 'cartpole-balance', 'cartpole-balance-sparse',
-		'cartpole-swingup', 'cartpole-swingup-sparse', 'cup-catch', 'finger-spin', 'finger-turn-easy',
-		'finger-turn-hard', 'fish-swim', 'hopper-stand', 'hopper-hop',
-		# 11 custom dmcontrol tasks
-		'walker-walk-backwards', 'walker-run-backwards', 'cheetah-run-backwards', 'cheetah-run-front', 'cheetah-run-back',
-		'cheetah-jump', 'hopper-hop-backwards', 'reacher-three-easy', 'reacher-three-hard', 'cup-spin',
-		'pendulum-spin',
-		# meta-world mt50
-		'mw-assembly', 'mw-basketball', 'mw-button-press-topdown', 'mw-button-press-topdown-wall', 'mw-button-press',
-		'mw-button-press-wall', 'mw-coffee-button', 'mw-coffee-pull', 'mw-coffee-push', 'mw-dial-turn',
-		'mw-disassemble', 'mw-door-open', 'mw-door-close', 'mw-drawer-close', 'mw-drawer-open',
-		'mw-faucet-open', 'mw-faucet-close', 'mw-hammer', 'mw-handle-press-side', 'mw-handle-press',
-		'mw-handle-pull-side', 'mw-handle-pull', 'mw-lever-pull', 'mw-peg-insert-side', 'mw-peg-unplug-side',
-		'mw-pick-out-of-hole', 'mw-pick-place', 'mw-pick-place-wall', 'mw-plate-slide', 'mw-plate-slide-side',
-		'mw-plate-slide-back', 'mw-plate-slide-back-side', 'mw-push-back', 'mw-push', 'mw-push-wall',
-		'mw-reach', 'mw-reach-wall', 'mw-shelf-place', 'mw-soccer', 'mw-stick-push',
-		'mw-stick-pull', 'mw-sweep-into', 'mw-sweep', 'mw-window-open', 'mw-window-close',
-		'mw-bin-picking', 'mw-box-close', 'mw-door-lock', 'mw-door-unlock', 'mw-hand-insert',
-	],
-}
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/buffer.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/buffer.py
deleted file mode 100644
index 4b16f6f..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/buffer.py
+++ /dev/null
@@ -1,115 +0,0 @@
-import torch
-from tensordict.tensordict import TensorDict
-from torchrl.data.replay_buffers import ReplayBuffer, LazyTensorStorage
-from torchrl.data.replay_buffers.samplers import SliceSampler
-
-
-class Buffer():
-	"""
-	Replay buffer for TD-MPC2 training. Based on torchrl.
-	Uses CUDA memory if available, and CPU memory otherwise.
-	"""
-
-	def __init__(self, cfg):
-		self.cfg = cfg
-		self._device = torch.device('cuda:0')
-		self._capacity = min(cfg.buffer_size, cfg.steps)
-		self._sampler = SliceSampler(
-			num_slices=self.cfg.batch_size,
-			end_key=None,
-			traj_key='episode',
-			truncated_key=None,
-			strict_length=True,
-			cache_values=cfg.multitask,
-		)
-		self._batch_size = cfg.batch_size * (cfg.horizon+1)
-		self._num_eps = 0
-
-	@property
-	def capacity(self):
-		"""Return the capacity of the buffer."""
-		return self._capacity
-
-	@property
-	def num_eps(self):
-		"""Return the number of episodes in the buffer."""
-		return self._num_eps
-
-	def _reserve_buffer(self, storage):
-		"""
-		Reserve a buffer with the given storage.
-		"""
-		return ReplayBuffer(
-			storage=storage,
-			sampler=self._sampler,
-			pin_memory=False,
-			prefetch=0,
-			batch_size=self._batch_size,
-		)
-
-	def _init(self, tds):
-		"""Initialize the replay buffer. Use the first episode to estimate storage requirements."""
-		print(f'Buffer capacity: {self._capacity:,}')
-		mem_free, _ = torch.cuda.mem_get_info()
-		bytes_per_step = sum([
-				(v.numel()*v.element_size() if not isinstance(v, TensorDict) \
-				else sum([x.numel()*x.element_size() for x in v.values()])) \
-			for v in tds.values()
-		]) / len(tds)
-		total_bytes = bytes_per_step*self._capacity
-		print(f'Storage required: {total_bytes/1e9:.2f} GB')
-		# Heuristic: decide whether to use CUDA or CPU memory
-		storage_device = 'cuda:0' if 2.5*total_bytes < mem_free else 'cpu'
-		print(f'Using {storage_device.upper()} memory for storage.')
-		self._storage_device = torch.device(storage_device)
-		return self._reserve_buffer(
-			LazyTensorStorage(self._capacity, device=self._storage_device)
-		)
-
-	def load(self, td):
-		"""
-		Load a batch of episodes into the buffer. This is useful for loading data from disk,
-		and is more efficient than adding episodes one by one.
-		"""
-		num_new_eps = len(td)
-		episode_idx = torch.arange(self._num_eps, self._num_eps+num_new_eps, dtype=torch.int64)
-		td['episode'] = episode_idx.unsqueeze(-1).expand(-1, td['reward'].shape[1])
-		if self._num_eps == 0:
-			self._buffer = self._init(td[0])
-		td = td.reshape(td.shape[0]*td.shape[1])
-		self._buffer.extend(td)
-		self._num_eps += num_new_eps
-		return self._num_eps
-
-	def add(self, td):
-		"""Add an episode to the buffer."""
-		td['episode'] = torch.full_like(td['reward'], self._num_eps, dtype=torch.int64)
-		if self._num_eps == 0:
-			self._buffer = self._init(td)
-		self._buffer.extend(td)
-		self._num_eps += 1
-		return self._num_eps
-
-	def _prepare_batch(self, td):
-		"""
-		Prepare a sampled batch for training (post-processing).
-		Expects `td` to be a TensorDict with batch size TxB.
-		"""
-		td = td.select("obs", "action", "reward", "terminated", "task", strict=False).to(self._device, non_blocking=True)
-		obs = td.get('obs').contiguous()
-		action = td.get('action')[1:].contiguous()
-		reward = td.get('reward')[1:].unsqueeze(-1).contiguous()
-		terminated = td.get('terminated', None)
-		if terminated is not None:
-			terminated = td.get('terminated')[1:].unsqueeze(-1).contiguous()
-		else:
-			terminated = torch.zeros_like(reward)
-		task = td.get('task', None)
-		if task is not None:
-			task = task[0].contiguous()
-		return obs, action, reward, terminated, task
-
-	def sample(self):
-		"""Sample a batch of subsequences from the buffer."""
-		td = self._buffer.sample().view(-1, self.cfg.horizon+1).permute(1, 0)
-		return self._prepare_batch(td)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/init.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/init.py
deleted file mode 100644
index 45a3f5e..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/init.py
+++ /dev/null
@@ -1,22 +0,0 @@
-import torch.nn as nn
-
-
-def weight_init(m):
-	"""Custom weight initialization for TD-MPC2."""
-	if isinstance(m, nn.Linear):
-		nn.init.trunc_normal_(m.weight, std=0.02)
-		if m.bias is not None:
-			nn.init.constant_(m.bias, 0)
-	elif isinstance(m, nn.Embedding):
-		nn.init.uniform_(m.weight, -0.02, 0.02)
-	elif isinstance(m, nn.ParameterList):
-		for i,p in enumerate(m):
-			if p.dim() == 3: # Linear
-				nn.init.trunc_normal_(p, std=0.02) # Weight
-				nn.init.constant_(m[i+1], 0) # Bias
-
-
-def zero_(params):
-	"""Initialize parameters to zero."""
-	for p in params:
-		p.data.fill_(0)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/layers.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/layers.py
deleted file mode 100644
index 97e977c..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/layers.py
+++ /dev/null
@@ -1,221 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from tensordict import from_modules
-from copy import deepcopy
-
-
-class Ensemble(nn.Module):
-	"""
-	Vectorized ensemble of modules.
-	"""
-
-	def __init__(self, modules, **kwargs):
-		super().__init__()
-		# combine_state_for_ensemble causes graph breaks
-		self.params = from_modules(*modules, as_module=True)
-		with self.params[0].data.to("meta").to_module(modules[0]):
-			self.module = deepcopy(modules[0])
-		self._repr = str(modules[0])
-		self._n = len(modules)
-
-	def __len__(self):
-		return self._n
-
-	def _call(self, params, *args, **kwargs):
-		with params.to_module(self.module):
-			return self.module(*args, **kwargs)
-
-	def forward(self, *args, **kwargs):
-		return torch.vmap(self._call, (0, None), randomness="different")(self.params, *args, **kwargs)
-
-	def __repr__(self):
-		return f'Vectorized {len(self)}x ' + self._repr
-
-
-class ShiftAug(nn.Module):
-	"""
-	Random shift image augmentation.
-	Adapted from https://github.com/facebookresearch/drqv2
-	"""
-	def __init__(self, pad=3):
-		super().__init__()
-		self.pad = pad
-		self.padding = tuple([self.pad] * 4)
-
-	def forward(self, x):
-		x = x.float()
-		n, _, h, w = x.size()
-		assert h == w
-		x = F.pad(x, self.padding, 'replicate')
-		eps = 1.0 / (h + 2 * self.pad)
-		arange = torch.linspace(-1.0 + eps, 1.0 - eps, h + 2 * self.pad, device=x.device, dtype=x.dtype)[:h]
-		arange = arange.unsqueeze(0).repeat(h, 1).unsqueeze(2)
-		base_grid = torch.cat([arange, arange.transpose(1, 0)], dim=2)
-		base_grid = base_grid.unsqueeze(0).repeat(n, 1, 1, 1)
-		shift = torch.randint(0, 2 * self.pad + 1, size=(n, 1, 1, 2), device=x.device, dtype=x.dtype)
-		shift *= 2.0 / (h + 2 * self.pad)
-		grid = base_grid + shift
-		return F.grid_sample(x, grid, padding_mode='zeros', align_corners=False)
-
-
-class PixelPreprocess(nn.Module):
-	"""
-	Normalizes pixel observations to [-0.5, 0.5].
-	"""
-
-	def __init__(self):
-		super().__init__()
-
-	def forward(self, x):
-		return x.div(255.).sub(0.5)
-
-
-class SimNorm(nn.Module):
-	"""
-	Simplicial normalization.
-	Adapted from https://arxiv.org/abs/2204.00616.
-	"""
-
-	def __init__(self, cfg):
-		super().__init__()
-		self.dim = cfg.simnorm_dim
-
-	def forward(self, x):
-		shp = x.shape
-		x = x.view(*shp[:-1], -1, self.dim)
-		x = F.softmax(x, dim=-1)
-		return x.view(*shp)
-
-	def __repr__(self):
-		return f"SimNorm(dim={self.dim})"
-
-
-class NormedLinear(nn.Linear):
-	"""
-	Linear layer with LayerNorm, activation, and optionally dropout.
-	"""
-
-	def __init__(self, *args, dropout=0., act=None, **kwargs):
-		super().__init__(*args, **kwargs)
-		self.ln = nn.LayerNorm(self.out_features)
-		if act is None:
-			act = nn.Mish(inplace=False)
-		self.act = act
-		self.dropout = nn.Dropout(dropout, inplace=False) if dropout else None
-
-	def forward(self, x):
-		x = super().forward(x)
-		if self.dropout:
-			x = self.dropout(x)
-		return self.act(self.ln(x))
-
-	def __repr__(self):
-		repr_dropout = f", dropout={self.dropout.p}" if self.dropout else ""
-		return f"NormedLinear(in_features={self.in_features}, "\
-			f"out_features={self.out_features}, "\
-			f"bias={self.bias is not None}{repr_dropout}, "\
-			f"act={self.act.__class__.__name__})"
-
-
-def mlp(in_dim, mlp_dims, out_dim, act=None, dropout=0.):
-	"""
-	Basic building block of TD-MPC2.
-	MLP with LayerNorm, Mish activations, and optionally dropout.
-	"""
-	if isinstance(mlp_dims, int):
-		mlp_dims = [mlp_dims]
-	dims = [in_dim] + mlp_dims + [out_dim]
-	mlp = nn.ModuleList()
-	for i in range(len(dims) - 2):
-		mlp.append(NormedLinear(dims[i], dims[i+1], dropout=dropout*(i==0)))
-	mlp.append(NormedLinear(dims[-2], dims[-1], act=act) if act else nn.Linear(dims[-2], dims[-1]))
-	return nn.Sequential(*mlp)
-
-
-def conv(in_shape, num_channels, act=None):
-	"""
-	Basic convolutional encoder for TD-MPC2 with raw image observations.
-	4 layers of convolution with ReLU activations, followed by a linear layer.
-	"""
-	assert in_shape[-1] == 64 # assumes rgb observations to be 64x64
-	layers = [
-		ShiftAug(), PixelPreprocess(),
-		nn.Conv2d(in_shape[0], num_channels, 7, stride=2), nn.ReLU(inplace=False),
-		nn.Conv2d(num_channels, num_channels, 5, stride=2), nn.ReLU(inplace=False),
-		nn.Conv2d(num_channels, num_channels, 3, stride=2), nn.ReLU(inplace=False),
-		nn.Conv2d(num_channels, num_channels, 3, stride=1), nn.Flatten()]
-	if act:
-		layers.append(act)
-	return nn.Sequential(*layers)
-
-
-def enc(cfg, out={}):
-	"""
-	Returns a dictionary of encoders for each observation in the dict.
-	"""
-	for k in cfg.obs_shape.keys():
-		if k == 'state':
-			out[k] = mlp(cfg.obs_shape[k][0] + cfg.task_dim, max(cfg.num_enc_layers-1, 1)*[cfg.enc_dim], cfg.latent_dim, act=SimNorm(cfg))
-		elif k == 'rgb':
-			out[k] = conv(cfg.obs_shape[k], cfg.num_channels, act=SimNorm(cfg))
-		else:
-			raise NotImplementedError(f"Encoder for observation type {k} not implemented.")
-	return nn.ModuleDict(out)
-
-
-def api_model_conversion(target_state_dict, source_state_dict):
-	"""
-	Converts a checkpoint from our old API to the new torch.compile compatible API.
-	"""
-	# check whether checkpoint is already in the new format
-	if "_detach_Qs_params.0.weight" in source_state_dict:
-		return source_state_dict
-
-	name_map = ['weight', 'bias', 'ln.weight', 'ln.bias']
-	new_state_dict = dict()
-
-	# rename keys
-	for key, val in list(source_state_dict.items()):
-		if key.startswith('_Qs.'):
-			num = key[len('_Qs.params.'):]
-			new_key = str(int(num) // 4) + "." + name_map[int(num) % 4]
-			new_total_key = "_Qs.params." + new_key
-			del source_state_dict[key]
-			new_state_dict[new_total_key] = val
-			new_total_key = "_detach_Qs_params." + new_key
-			new_state_dict[new_total_key] = val
-		elif key.startswith('_target_Qs.'):
-			num = key[len('_target_Qs.params.'):]
-			new_key = str(int(num) // 4) + "." + name_map[int(num) % 4]
-			new_total_key = "_target_Qs_params." + new_key
-			del source_state_dict[key]
-			new_state_dict[new_total_key] = val
-
-	# add batch_size and device from target_state_dict to new_state_dict
-	for prefix in ('_Qs.', '_detach_Qs_', '_target_Qs_'):
-		for key in ('__batch_size', '__device'):
-			new_key = prefix + 'params.' + key
-			new_state_dict[new_key] = target_state_dict[new_key]
-
-	# check that every key in new_state_dict is in target_state_dict
-	for key in new_state_dict.keys():
-		assert key in target_state_dict, f"key {key} not in target_state_dict"
-	# check that all Qs keys in target_state_dict are in new_state_dict
-	for key in target_state_dict.keys():
-		if 'Qs' in key:
-			assert key in new_state_dict, f"key {key} not in new_state_dict"
-	# check that source_state_dict contains no Qs keys
-	for key in source_state_dict.keys():
-		assert 'Qs' not in key, f"key {key} contains 'Qs'"
-
-	# copy log_std_min and log_std_max from target_state_dict to new_state_dict
-	new_state_dict['log_std_min'] = target_state_dict['log_std_min']
-	new_state_dict['log_std_dif'] = target_state_dict['log_std_dif']
-	if '_action_masks' in target_state_dict:
-		new_state_dict['_action_masks'] = target_state_dict['_action_masks']
-
-	# copy new_state_dict to source_state_dict
-	source_state_dict.update(new_state_dict)
-
-	return source_state_dict
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/logger.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/logger.py
deleted file mode 100755
index c3dd7df..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/logger.py
+++ /dev/null
@@ -1,241 +0,0 @@
-import dataclasses
-import os
-import datetime
-import re
-
-import numpy as np
-import pandas as pd
-from termcolor import colored
-
-from common import TASK_SET
-
-
-CONSOLE_FORMAT = [
-	("iteration", "I", "int"),
-	("episode", "E", "int"),
-	("step", "I", "int"),
-	("episode_reward", "R", "float"),
-	("episode_success", "S", "float"),
-	("elapsed_time", "T", "time"),
-]
-
-CAT_TO_COLOR = {
-	"pretrain": "yellow",
-	"train": "blue",
-	"eval": "green",
-}
-
-
-def make_dir(dir_path):
-	"""Create directory if it does not already exist."""
-	try:
-		os.makedirs(dir_path)
-	except OSError:
-		pass
-	return dir_path
-
-
-def print_run(cfg):
-	"""
-	Pretty-printing of current run information.
-	Logger calls this method at initialization.
-	"""
-	prefix, color, attrs = "  ", "green", ["bold"]
-
-	def _limstr(s, maxlen=36):
-		return str(s[:maxlen]) + "..." if len(str(s)) > maxlen else s
-
-	def _pprint(k, v):
-		print(
-			prefix + colored(f'{k.capitalize()+":":<15}', color, attrs=attrs), _limstr(v)
-		)
-
-	observations  = ", ".join([str(v) for v in cfg.obs_shape.values()])
-	kvs = [
-		("task", cfg.task_title),
-		("steps", f"{int(cfg.steps):,}"),
-		("observations", observations),
-		("actions", cfg.action_dim),
-		("experiment", cfg.exp_name),
-	]
-	w = np.max([len(_limstr(str(kv[1]))) for kv in kvs]) + 25
-	div = "-" * w
-	print(div)
-	for k, v in kvs:
-		_pprint(k, v)
-	print(div)
-
-
-def cfg_to_group(cfg, return_list=False):
-	"""
-	Return a wandb-safe group name for logging.
-	Optionally returns group name as list.
-	"""
-	lst = [cfg.task, re.sub("[^0-9a-zA-Z]+", "-", cfg.exp_name)]
-	return lst if return_list else "-".join(lst)
-
-
-class VideoRecorder:
-	"""Utility class for logging evaluation videos."""
-
-	def __init__(self, cfg, wandb, fps=15):
-		self.cfg = cfg
-		self._save_dir = make_dir(cfg.work_dir / 'eval_video')
-		self._wandb = wandb
-		self.fps = fps
-		self.frames = []
-		self.enabled = False
-
-	def init(self, env, enabled=True):
-		self.frames = []
-		self.enabled = self._save_dir and self._wandb and enabled
-		self.record(env)
-
-	def record(self, env):
-		if self.enabled:
-			self.frames.append(env.render())
-
-	def save(self, step, key='videos/eval_video'):
-		if self.enabled and len(self.frames) > 0:
-			frames = np.stack(self.frames)
-			return self._wandb.log(
-				{key: self._wandb.Video(frames.transpose(0, 3, 1, 2), fps=self.fps, format='mp4')}, step=step
-			)
-
-
-class Logger:
-	"""Primary logging object. Logs either locally or using wandb."""
-
-	def __init__(self, cfg):
-		self._log_dir = make_dir(cfg.work_dir)
-		self._model_dir = make_dir(self._log_dir / "models")
-		self._save_csv = cfg.save_csv
-		self._save_agent = cfg.save_agent
-		self._group = cfg_to_group(cfg)
-		self._seed = cfg.seed
-		self._eval = []
-		print_run(cfg)
-		self.project = cfg.get("wandb_project", "none")
-		self.entity = cfg.get("wandb_entity", "none")
-		if not cfg.enable_wandb or self.project == "none" or self.entity == "none":
-			print(colored("Wandb disabled.", "blue", attrs=["bold"]))
-			cfg.save_agent = False
-			cfg.save_video = False
-			self._wandb = None
-			self._video = None
-			return
-		os.environ["WANDB_SILENT"] = "true" if cfg.wandb_silent else "false"
-		import wandb
-
-		wandb.init(
-			project=self.project,
-			entity=self.entity,
-			name=str(cfg.seed),
-			group=self._group,
-			tags=cfg_to_group(cfg, return_list=True) + [f"seed:{cfg.seed}"],
-			dir=self._log_dir,
-			config=dataclasses.asdict(cfg),
-		)
-		print(colored("Logs will be synced with wandb.", "blue", attrs=["bold"]))
-		self._wandb = wandb
-		self._video = (
-			VideoRecorder(cfg, self._wandb)
-			if self._wandb and cfg.save_video
-			else None
-		)
-
-	@property
-	def video(self):
-		return self._video
-
-	@property
-	def model_dir(self):
-		return self._model_dir
-
-	def save_agent(self, agent=None, identifier='final'):
-		if self._save_agent and agent:
-			fp = self._model_dir / f'{str(identifier)}.pt'
-			agent.save(fp)
-			if self._wandb:
-				artifact = self._wandb.Artifact(
-					self._group + '-' + str(self._seed) + '-' + str(identifier),
-					type='model',
-				)
-				artifact.add_file(fp)
-				self._wandb.log_artifact(artifact)
-
-	def finish(self, agent=None):
-		try:
-			self.save_agent(agent)
-		except Exception as e:
-			print(colored(f"Failed to save model: {e}", "red"))
-		if self._wandb:
-			self._wandb.finish()
-
-	def _format(self, key, value, ty):
-		if ty == "int":
-			return f'{colored(key+":", "blue")} {int(value):,}'
-		elif ty == "float":
-			return f'{colored(key+":", "blue")} {value:.01f}'
-		elif ty == "time":
-			value = str(datetime.timedelta(seconds=int(value)))
-			return f'{colored(key+":", "blue")} {value}'
-		else:
-			raise f"invalid log format type: {ty}"
-
-	def _print(self, d, category):
-		category = colored(category, CAT_TO_COLOR[category])
-		pieces = [f" {category:<14}"]
-		for k, disp_k, ty in CONSOLE_FORMAT:
-			if k in d:
-				pieces.append(f"{self._format(disp_k, d[k], ty):<22}")
-		print("   ".join(pieces))
-
-	def pprint_multitask(self, d, cfg):
-		"""Pretty-print evaluation metrics for multi-task training."""
-		print(colored(f'Evaluated agent on {len(cfg.tasks)} tasks:', 'yellow', attrs=['bold']))
-		dmcontrol_reward = []
-		metaworld_reward = []
-		metaworld_success = []
-		for k, v in d.items():
-			if '+' not in k:
-				continue
-			task = k.split('+')[1]
-			if task in TASK_SET['mt30'] and k.startswith('episode_reward'): # DMControl
-				dmcontrol_reward.append(v)
-				print(colored(f'  {task:<22}\tR: {v:.01f}', 'yellow'))
-			elif task in TASK_SET['mt80'] and task not in TASK_SET['mt30']: # Meta-World
-				if k.startswith('episode_reward'):
-					metaworld_reward.append(v)
-				elif k.startswith('episode_success'):
-					metaworld_success.append(v)
-					print(colored(f'  {task:<22}\tS: {v:.02f}', 'yellow'))
-		dmcontrol_reward = np.nanmean(dmcontrol_reward)
-		d['episode_reward+avg_dmcontrol'] = dmcontrol_reward
-		print(colored(f'  {"dmcontrol":<22}\tR: {dmcontrol_reward:.01f}', 'yellow', attrs=['bold']))
-		if cfg.task == 'mt80':
-			metaworld_reward = np.nanmean(metaworld_reward)
-			metaworld_success = np.nanmean(metaworld_success)
-			d['episode_reward+avg_metaworld'] = metaworld_reward
-			d['episode_success+avg_metaworld'] = metaworld_success
-			print(colored(f'  {"metaworld":<22}\tR: {metaworld_reward:.01f}', 'yellow', attrs=['bold']))
-			print(colored(f'  {"metaworld":<22}\tS: {metaworld_success:.02f}', 'yellow', attrs=['bold']))
-
-	def log(self, d, category="train"):
-		assert category in CAT_TO_COLOR.keys(), f"invalid category: {category}"
-		if self._wandb:
-			if category in {"train", "eval"}:
-				xkey = "step"
-			elif category == "pretrain":
-				xkey = "iteration"
-			_d = dict()
-			for k, v in d.items():
-				_d[category + "/" + k] = v
-			self._wandb.log(_d, step=d[xkey])
-		if category == "eval" and self._save_csv:
-			keys = ["step", "episode_reward"]
-			self._eval.append(np.array([d[keys[0]], d[keys[1]]]))
-			pd.DataFrame(np.array(self._eval)).to_csv(
-				self._log_dir / "eval.csv", header=keys, index=None
-			)
-		self._print(d, category)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/math.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/math.py
deleted file mode 100644
index ca1d6a7..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/math.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import torch
-import torch.nn.functional as F
-
-
-def soft_ce(pred, target, cfg):
-	"""Computes the cross entropy loss between predictions and soft targets."""
-	pred = F.log_softmax(pred, dim=-1)
-	target = two_hot(target, cfg)
-	return -(target * pred).sum(-1, keepdim=True)
-
-
-def log_std(x, low, dif):
-	return low + 0.5 * dif * (torch.tanh(x) + 1)
-
-
-def gaussian_logprob(eps, log_std):
-	"""Compute Gaussian log probability."""
-	residual = -0.5 * eps.pow(2) - log_std
-	log_prob = residual - 0.9189385175704956
-	return log_prob.sum(-1, keepdim=True)
-
-
-def squash(mu, pi, log_pi):
-	"""Apply squashing function."""
-	mu = torch.tanh(mu)
-	pi = torch.tanh(pi)
-	squashed_pi = torch.log(F.relu(1 - pi.pow(2)) + 1e-6)
-	log_pi = log_pi - squashed_pi.sum(-1, keepdim=True)
-	return mu, pi, log_pi
-
-
-def int_to_one_hot(x, num_classes):
-	"""
-	Converts an integer tensor to a one-hot tensor.
-	Supports batched inputs.
-	"""
-	one_hot = torch.zeros(*x.shape, num_classes, device=x.device)
-	one_hot.scatter_(-1, x.unsqueeze(-1), 1)
-	return one_hot
-
-
-def symlog(x):
-	"""
-	Symmetric logarithmic function.
-	Adapted from https://github.com/danijar/dreamerv3.
-	"""
-	return torch.sign(x) * torch.log(1 + torch.abs(x))
-
-
-def symexp(x):
-	"""
-	Symmetric exponential function.
-	Adapted from https://github.com/danijar/dreamerv3.
-	"""
-	return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)
-
-
-def two_hot(x, cfg):
-	"""Converts a batch of scalars to soft two-hot encoded targets for discrete regression."""
-	if cfg.num_bins == 0:
-		return x
-	elif cfg.num_bins == 1:
-		return symlog(x)
-	x = torch.clamp(symlog(x), cfg.vmin, cfg.vmax).squeeze(1)
-	bin_idx = torch.floor((x - cfg.vmin) / cfg.bin_size)
-	bin_offset = ((x - cfg.vmin) / cfg.bin_size - bin_idx).unsqueeze(-1)
-	soft_two_hot = torch.zeros(x.shape[0], cfg.num_bins, device=x.device, dtype=x.dtype)
-	bin_idx = bin_idx.long()
-	soft_two_hot = soft_two_hot.scatter(1, bin_idx.unsqueeze(1), 1 - bin_offset)
-	soft_two_hot = soft_two_hot.scatter(1, (bin_idx.unsqueeze(1) + 1) % cfg.num_bins, bin_offset)
-	return soft_two_hot
-
-
-def two_hot_inv(x, cfg):
-	"""Converts a batch of soft two-hot encoded vectors to scalars."""
-	if cfg.num_bins == 0:
-		return x
-	elif cfg.num_bins == 1:
-		return symexp(x)
-	dreg_bins = torch.linspace(cfg.vmin, cfg.vmax, cfg.num_bins, device=x.device, dtype=x.dtype)
-	x = F.softmax(x, dim=-1)
-	x = torch.sum(x * dreg_bins, dim=-1, keepdim=True)
-	return symexp(x)
-
-
-def gumbel_softmax_sample(p, temperature=1.0, dim=0):
-	"""Sample from the Gumbel-Softmax distribution."""
-	logits = p.log()
-	gumbels = (
-		-torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
-	)  # ~Gumbel(0,1)
-	gumbels = (logits + gumbels) / temperature  # ~Gumbel(logits,tau)
-	y_soft = gumbels.softmax(dim)
-	return y_soft.argmax(-1)
-
-
-def termination_statistics(pred, target, eps=1e-9):
-	"""Compute episode termination statistics."""
-	pred = pred.squeeze(-1)
-	target = target.squeeze(-1)
-	rate = target.sum() / len(target)
-	tp = ((pred > 0.5) & (target == 1)).sum()
-	fn = ((pred <= 0.5) & (target == 1)).sum()
-	fp = ((pred > 0.5) & (target == 0)).sum()
-	recall = tp / (tp + fn + eps)
-	precision = tp / (tp + fp + eps)
-	f1 = 2 * (precision * recall) / (precision + recall + eps)
-	return {
-		'termination_rate': rate,
-		'termination_f1': f1,
-	}
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/parser.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/parser.py
deleted file mode 100755
index a8d9f25..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/parser.py
+++ /dev/null
@@ -1,80 +0,0 @@
-import dataclasses
-import re
-from pathlib import Path
-from typing import Any
-
-import hydra
-from omegaconf import OmegaConf
-
-from common import MODEL_SIZE, TASK_SET
-
-
-def cfg_to_dataclass(cfg, frozen=False):
-	"""
-	Converts an OmegaConf config to a dataclass object.
-	This prevents graph breaks when used with torch.compile.
-	"""
-	cfg_dict = OmegaConf.to_container(cfg)
-	fields = []
-	for key, value in cfg_dict.items():
-		fields.append((key, Any, dataclasses.field(default_factory=lambda value_=value: value_)))
-	dataclass_name = "Config"
-	dataclass = dataclasses.make_dataclass(dataclass_name, fields, frozen=frozen)
-	def get(self, val, default=None):
-		return getattr(self, val, default)
-	dataclass.get = get
-	return dataclass()
-
-
-def parse_cfg(cfg: OmegaConf) -> OmegaConf:
-	"""
-	Parses a Hydra config. Mostly for convenience.
-	"""
-
-	# Logic
-	for k in cfg.keys():
-		try:
-			v = cfg[k]
-			if v == None:
-				v = True
-		except:
-			pass
-
-	# Algebraic expressions
-	for k in cfg.keys():
-		try:
-			v = cfg[k]
-			if isinstance(v, str):
-				match = re.match(r"(\d+)([+\-*/])(\d+)", v)
-				if match:
-					cfg[k] = eval(match.group(1) + match.group(2) + match.group(3))
-					if isinstance(cfg[k], float) and cfg[k].is_integer():
-						cfg[k] = int(cfg[k])
-		except:
-			pass
-
-	# Convenience
-	cfg.work_dir = Path(hydra.utils.get_original_cwd()) / 'logs' / cfg.task / str(cfg.seed) / cfg.exp_name
-	cfg.task_title = cfg.task.replace("-", " ").title()
-	cfg.bin_size = (cfg.vmax - cfg.vmin) / (cfg.num_bins-1) # Bin size for discrete regression
-
-	# Model size
-	if cfg.get('model_size', None) is not None:
-		assert cfg.model_size in MODEL_SIZE.keys(), \
-			f'Invalid model size {cfg.model_size}. Must be one of {list(MODEL_SIZE.keys())}'
-		for k, v in MODEL_SIZE[cfg.model_size].items():
-			cfg[k] = v
-		if cfg.task == 'mt30' and cfg.model_size == 19:
-			cfg.latent_dim = 512 # This checkpoint is slightly smaller
-
-	# Multi-task
-	cfg.multitask = cfg.task in TASK_SET.keys()
-	if cfg.multitask:
-		cfg.task_title = cfg.task.upper()
-		# Account for slight inconsistency in task_dim for the mt30 experiments
-		cfg.task_dim = 96 if cfg.task == 'mt80' or cfg.get('model_size', 5) in {1, 317} else 64
-	else:
-		cfg.task_dim = 0
-	cfg.tasks = TASK_SET.get(cfg.task, [cfg.task])
-
-	return cfg_to_dataclass(cfg)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/scale.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/scale.py
deleted file mode 100644
index 10e1378..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/scale.py
+++ /dev/null
@@ -1,53 +0,0 @@
-import torch
-from torch.nn import Buffer
-
-
-class RunningScale(torch.nn.Module):
-	"""Running trimmed scale estimator."""
-
-	def __init__(self, cfg, device=None):
-		super().__init__()
-		self.cfg = cfg
-		dev = device if device is not None else torch.device(getattr(cfg, 'device', 'cuda:0'))
-		if not isinstance(dev, torch.device):
-			dev = torch.device(dev)
-		self.value = Buffer(torch.ones(1, dtype=torch.float32, device=dev))
-		self._percentiles = Buffer(torch.tensor([5, 95], dtype=torch.float32, device=dev))
-
-	def state_dict(self):
-		return dict(value=self.value, percentiles=self._percentiles)
-
-	def load_state_dict(self, state_dict):
-		self.value.copy_(state_dict['value'])
-		self._percentiles.copy_(state_dict['percentiles'])
-
-	def _positions(self, x_shape):
-		positions = self._percentiles * (x_shape-1) / 100
-		floored = torch.floor(positions)
-		ceiled = floored + 1
-		ceiled = torch.where(ceiled > x_shape - 1, x_shape - 1, ceiled)
-		weight_ceiled = positions-floored
-		weight_floored = 1.0 - weight_ceiled
-		return floored.long(), ceiled.long(), weight_floored.unsqueeze(1), weight_ceiled.unsqueeze(1)
-
-	def _percentile(self, x):
-		x_dtype, x_shape = x.dtype, x.shape
-		x = x.flatten(1, x.ndim-1)
-		in_sorted = torch.sort(x, dim=0).values
-		floored, ceiled, weight_floored, weight_ceiled = self._positions(x.shape[0])
-		d0 = in_sorted[floored] * weight_floored
-		d1 = in_sorted[ceiled] * weight_ceiled
-		return (d0+d1).reshape(-1, *x_shape[1:]).to(x_dtype)
-
-	def update(self, x):
-		percentiles = self._percentile(x.detach())
-		value = torch.clamp(percentiles[1] - percentiles[0], min=1.)
-		self.value.data.lerp_(value, self.cfg.tau)
-
-	def forward(self, x, update=False):
-		if update:
-			self.update(x)
-		return x / self.value
-
-	def __repr__(self):
-		return f'RunningScale(S: {self.value})'
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/seed.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/seed.py
deleted file mode 100644
index 5c8972e..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/seed.py
+++ /dev/null
@@ -1,12 +0,0 @@
-import random
-
-import numpy as np
-import torch
-
-
-def set_seed(seed):
-	"""Set seed for reproducibility."""
-	random.seed(seed)
-	np.random.seed(seed)
-	torch.manual_seed(seed)
-	torch.cuda.manual_seed_all(seed)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/world_model.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/world_model.py
deleted file mode 100644
index f748f0c..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/common/world_model.py
+++ /dev/null
@@ -1,216 +0,0 @@
-from copy import deepcopy
-
-import torch
-import torch.nn as nn
-
-from common import layers, math, init
-from tensordict import TensorDict
-from tensordict.nn import TensorDictParams
-
-
-class WorldModel(nn.Module):
-	"""
-	TD-MPC2 implicit world model architecture.
-	Can be used for both single-task and multi-task experiments.
-	"""
-
-	def __init__(self, cfg):
-		super().__init__()
-		self.cfg = cfg
-		if cfg.multitask:
-			self._task_emb = nn.Embedding(len(cfg.tasks), cfg.task_dim, max_norm=1)
-			self.register_buffer("_action_masks", torch.zeros(len(cfg.tasks), cfg.action_dim))
-			for i in range(len(cfg.tasks)):
-				self._action_masks[i, :cfg.action_dims[i]] = 1.
-		self._encoder = layers.enc(cfg)
-		self._dynamics = layers.mlp(cfg.latent_dim + cfg.action_dim + cfg.task_dim, 2*[cfg.mlp_dim], cfg.latent_dim, act=layers.SimNorm(cfg))
-		self._reward = layers.mlp(cfg.latent_dim + cfg.action_dim + cfg.task_dim, 2*[cfg.mlp_dim], max(cfg.num_bins, 1))
-		self._termination = layers.mlp(cfg.latent_dim + cfg.task_dim, 2*[cfg.mlp_dim], 1) if cfg.episodic else None
-		self._pi = layers.mlp(cfg.latent_dim + cfg.task_dim, 2*[cfg.mlp_dim], 2*cfg.action_dim)
-		self._Qs = layers.Ensemble([layers.mlp(cfg.latent_dim + cfg.action_dim + cfg.task_dim, 2*[cfg.mlp_dim], max(cfg.num_bins, 1), dropout=cfg.dropout) for _ in range(cfg.num_q)])
-		self.apply(init.weight_init)
-		init.zero_([self._reward[-1].weight, self._Qs.params["2", "weight"]])
-
-		self.register_buffer("log_std_min", torch.tensor(cfg.log_std_min))
-		self.register_buffer("log_std_dif", torch.tensor(cfg.log_std_max) - self.log_std_min)
-		self.init()
-
-	def init(self):
-		# Create params
-		self._detach_Qs_params = TensorDictParams(self._Qs.params.data, no_convert=True)
-		self._target_Qs_params = TensorDictParams(self._Qs.params.data.clone(), no_convert=True)
-
-		# Create modules
-		with self._detach_Qs_params.data.to("meta").to_module(self._Qs.module):
-			self._detach_Qs = deepcopy(self._Qs)
-			self._target_Qs = deepcopy(self._Qs)
-
-		# Assign params to modules
-		# We do this strange assignment to avoid having duplicated tensors in the state-dict -- working on a better API for this
-		delattr(self._detach_Qs, "params")
-		self._detach_Qs.__dict__["params"] = self._detach_Qs_params
-		delattr(self._target_Qs, "params")
-		self._target_Qs.__dict__["params"] = self._target_Qs_params
-
-	def __repr__(self):
-		repr = 'TD-MPC2 World Model\n'
-		modules = ['Encoder', 'Dynamics', 'Reward', 'Termination', 'Policy prior', 'Q-functions']
-		for i, m in enumerate([self._encoder, self._dynamics, self._reward, self._termination, self._pi, self._Qs]):
-			if m == self._termination and not self.cfg.episodic:
-				continue
-			repr += f"{modules[i]}: {m}\n"
-		repr += "Learnable parameters: {:,}".format(self.total_params)
-		return repr
-
-	@property
-	def total_params(self):
-		return sum(p.numel() for p in self.parameters() if p.requires_grad)
-
-	def to(self, *args, **kwargs):
-		super().to(*args, **kwargs)
-		self.init()
-		return self
-
-	def train(self, mode=True):
-		"""
-		Overriding `train` method to keep target Q-networks in eval mode.
-		"""
-		super().train(mode)
-		self._target_Qs.train(False)
-		return self
-
-	def soft_update_target_Q(self):
-		"""
-		Soft-update target Q-networks using Polyak averaging.
-		"""
-		self._target_Qs_params.lerp_(self._detach_Qs_params, self.cfg.tau)
-
-	def task_emb(self, x, task):
-		"""
-		Continuous task embedding for multi-task experiments.
-		Retrieves the task embedding for a given task ID `task`
-		and concatenates it to the input `x`.
-		"""
-		if isinstance(task, int):
-			task = torch.tensor([task], device=x.device)
-		emb = self._task_emb(task.long())
-		if x.ndim == 3:
-			emb = emb.unsqueeze(0).repeat(x.shape[0], 1, 1)
-		elif emb.shape[0] == 1:
-			emb = emb.repeat(x.shape[0], 1)
-		return torch.cat([x, emb], dim=-1)
-
-	def encode(self, obs, task):
-		"""
-		Encodes an observation into its latent representation.
-		This implementation assumes a single state-based observation.
-		"""
-		if self.cfg.multitask:
-			obs = self.task_emb(obs, task)
-		if self.cfg.obs == 'rgb' and obs.ndim == 5:
-			return torch.stack([self._encoder[self.cfg.obs](o) for o in obs])
-		return self._encoder[self.cfg.obs](obs)
-
-	def next(self, z, a, task):
-		"""
-		Predicts the next latent state given the current latent state and action.
-		"""
-		if self.cfg.multitask:
-			z = self.task_emb(z, task)
-		z = torch.cat([z, a], dim=-1)
-		return self._dynamics(z)
-
-	def reward(self, z, a, task):
-		"""
-		Predicts instantaneous (single-step) reward.
-		"""
-		if self.cfg.multitask:
-			z = self.task_emb(z, task)
-		z = torch.cat([z, a], dim=-1)
-		return self._reward(z)
-	
-	def termination(self, z, task, unnormalized=False):
-		"""
-		Predicts termination signal.
-		"""
-		assert task is None
-		if self.cfg.multitask:
-			z = self.task_emb(z, task)
-		if unnormalized:
-			return self._termination(z)
-		return torch.sigmoid(self._termination(z))
-		
-
-	def pi(self, z, task):
-		"""
-		Samples an action from the policy prior.
-		The policy prior is a Gaussian distribution with
-		mean and (log) std predicted by a neural network.
-		"""
-		if self.cfg.multitask:
-			z = self.task_emb(z, task)
-
-		# Gaussian policy prior
-		mean, log_std = self._pi(z).chunk(2, dim=-1)
-		log_std = math.log_std(log_std, self.log_std_min, self.log_std_dif)
-		eps = torch.randn_like(mean)
-
-		if self.cfg.multitask: # Mask out unused action dimensions
-			mean = mean * self._action_masks[task]
-			log_std = log_std * self._action_masks[task]
-			eps = eps * self._action_masks[task]
-			action_dims = self._action_masks.sum(-1)[task].unsqueeze(-1)
-		else: # No masking
-			action_dims = None
-
-		log_prob = math.gaussian_logprob(eps, log_std)
-
-		# Scale log probability by action dimensions
-		size = eps.shape[-1] if action_dims is None else action_dims
-		scaled_log_prob = log_prob * size
-
-		# Reparameterization trick
-		action = mean + eps * log_std.exp()
-		mean, action, log_prob = math.squash(mean, action, log_prob)
-
-		entropy_scale = scaled_log_prob / (log_prob + 1e-8)
-		info = TensorDict({
-			"mean": mean,
-			"log_std": log_std,
-			"action_prob": 1.,
-			"entropy": -log_prob,
-			"scaled_entropy": -log_prob * entropy_scale,
-		})
-		return action, info
-
-	def Q(self, z, a, task, return_type='min', target=False, detach=False):
-		"""
-		Predict state-action value.
-		`return_type` can be one of [`min`, `avg`, `all`]:
-			- `min`: return the minimum of two randomly subsampled Q-values.
-			- `avg`: return the average of two randomly subsampled Q-values.
-			- `all`: return all Q-values.
-		`target` specifies whether to use the target Q-networks or not.
-		"""
-		assert return_type in {'min', 'avg', 'all'}
-
-		if self.cfg.multitask:
-			z = self.task_emb(z, task)
-
-		z = torch.cat([z, a], dim=-1)
-		if target:
-			qnet = self._target_Qs
-		elif detach:
-			qnet = self._detach_Qs
-		else:
-			qnet = self._Qs
-		out = qnet(z)
-
-		if return_type == 'all':
-			return out
-
-		qidx = torch.randperm(self.cfg.num_q, device=out.device)[:2]
-		Q = math.two_hot_inv(out[qidx], self.cfg)
-		if return_type == "min":
-			return Q.min(0).values
-		return Q.sum(0) / 2
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/config.yaml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/config.yaml
deleted file mode 100755
index ff15dbb..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/config.yaml
+++ /dev/null
@@ -1,91 +0,0 @@
-defaults:
-    - override hydra/launcher: submitit_local
-
-# environment
-task: dog-run
-obs: state
-episodic: false
-
-# evaluation
-checkpoint: ???
-eval_episodes: 10
-eval_freq: 50000
-
-# training
-steps: 10_000_000
-batch_size: 256
-reward_coef: 0.1
-value_coef: 0.1
-termination_coef: 1
-consistency_coef: 20
-rho: 0.5
-lr: 3e-4
-enc_lr_scale: 0.3
-grad_clip_norm: 20
-tau: 0.01
-discount_denom: 5
-discount_min: 0.95
-discount_max: 0.995
-buffer_size: 1_000_000
-exp_name: default
-data_dir: ???
-
-# planning
-mpc: true
-iterations: 6
-num_samples: 512
-num_elites: 64
-num_pi_trajs: 24
-horizon: 3
-min_std: 0.05
-max_std: 2
-temperature: 0.5
-
-# actor
-log_std_min: -10
-log_std_max: 2
-entropy_coef: 1e-4
-
-# critic
-num_bins: 101
-vmin: -10
-vmax: +10
-
-# architecture
-model_size: ???
-num_enc_layers: 2
-enc_dim: 256
-num_channels: 32
-mlp_dim: 512
-latent_dim: 512
-task_dim: 96
-num_q: 5
-dropout: 0.01
-simnorm_dim: 8
-
-# logging
-wandb_project: ???
-wandb_entity: ???
-wandb_silent: false
-enable_wandb: true
-save_csv: true
-
-# misc
-compile: true
-save_video: true
-save_agent: true
-seed: 1
-
-# convenience
-work_dir: ???
-task_title: ???
-multitask: ???
-tasks: ???
-obs_shape: ???
-action_dim: ???
-episode_length: ???
-obs_shapes: ???
-action_dims: ???
-episode_lengths: ???
-seed_steps: ???
-bin_size: ???
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/__init__.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/__init__.py
deleted file mode 100644
index 46f99a8..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/__init__.py
+++ /dev/null
@@ -1,83 +0,0 @@
-from copy import deepcopy
-import warnings
-
-import gymnasium as gym
-
-from envs.wrappers.multitask import MultitaskWrapper
-from envs.wrappers.tensor import TensorWrapper
-
-def missing_dependencies(task):
-	raise ValueError(f'Missing dependencies for task {task}; install dependencies to use this environment.')
-
-try:
-	from envs.dmcontrol import make_env as make_dm_control_env
-except:
-	make_dm_control_env = missing_dependencies
-try:
-	from envs.maniskill import make_env as make_maniskill_env
-except:
-	make_maniskill_env = missing_dependencies
-try:
-	from envs.metaworld import make_env as make_metaworld_env
-except:
-	make_metaworld_env = missing_dependencies
-try:
-	from envs.myosuite import make_env as make_myosuite_env
-except:
-	make_myosuite_env = missing_dependencies
-try:
-	from envs.mujoco import make_env as make_mujoco_env
-except:
-	make_mujoco_env = missing_dependencies
-
-
-warnings.filterwarnings('ignore', category=DeprecationWarning)
-
-
-def make_multitask_env(cfg):
-	"""
-	Make a multi-task environment for TD-MPC2 experiments.
-	"""
-	print('Creating multi-task environment with tasks:', cfg.tasks)
-	envs = []
-	for task in cfg.tasks:
-		_cfg = deepcopy(cfg)
-		_cfg.task = task
-		_cfg.multitask = False
-		env = make_env(_cfg)
-		if env is None:
-			raise ValueError('Unknown task:', task)
-		envs.append(env)
-	env = MultitaskWrapper(cfg, envs)
-	cfg.obs_shapes = env._obs_dims
-	cfg.action_dims = env._action_dims
-	cfg.episode_lengths = env._episode_lengths
-	return env
-	
-
-def make_env(cfg):
-	"""
-	Make an environment for TD-MPC2 experiments.
-	"""
-	gym.logger.set_level(40)
-	if cfg.multitask:
-		env = make_multitask_env(cfg)
-
-	else:
-		env = None
-		for fn in [make_dm_control_env, make_maniskill_env, make_metaworld_env, make_myosuite_env, make_mujoco_env]:
-			try:
-				env = fn(cfg)
-			except ValueError:
-				pass
-		if env is None:
-			raise ValueError(f'Failed to make environment "{cfg.task}": please verify that dependencies are installed and that the task exists.')
-		env = TensorWrapper(env)
-	try: # Dict
-		cfg.obs_shape = {k: v.shape for k, v in env.observation_space.spaces.items()}
-	except: # Box
-		cfg.obs_shape = {cfg.get('obs', 'state'): env.observation_space.shape}
-	cfg.action_dim = env.action_space.shape[0]
-	cfg.episode_length = env.max_episode_steps
-	cfg.seed_steps = max(1000, 5*cfg.episode_length)
-	return env
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/dmcontrol.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/dmcontrol.py
deleted file mode 100644
index a6e21b3..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/dmcontrol.py
+++ /dev/null
@@ -1,111 +0,0 @@
-from collections import defaultdict, deque
-
-import gymnasium as gym
-import numpy as np
-import torch
-
-from envs.tasks import cheetah, walker, hopper, reacher, ball_in_cup, pendulum, fish
-from dm_control import suite
-suite.ALL_TASKS = suite.ALL_TASKS + suite._get_tasks('custom')
-suite.TASKS_BY_DOMAIN = suite._get_tasks_by_domain(suite.ALL_TASKS)
-from dm_control.suite.wrappers import action_scale
-
-from envs.wrappers.timeout import Timeout
-
-
-def get_obs_shape(env):
-	obs_shp = []
-	for v in env.observation_spec().values():
-		try:
-			shp = np.prod(v.shape)
-		except:
-			shp = 1
-		obs_shp.append(shp)
-	return (int(np.sum(obs_shp)),)
-
-
-class DMControlWrapper:
-	def __init__(self, env, domain):
-		self.env = env
-		self.camera_id = 2 if domain == 'quadruped' else 0
-		obs_shape = get_obs_shape(env)
-		action_shape = env.action_spec().shape
-		self.observation_space = gym.spaces.Box(
-			low=np.full(obs_shape, -np.inf, dtype=np.float32),
-			high=np.full(obs_shape, np.inf, dtype=np.float32),
-			dtype=np.float32)
-		self.action_space = gym.spaces.Box(
-			low=np.full(action_shape, env.action_spec().minimum),
-			high=np.full(action_shape, env.action_spec().maximum),
-			dtype=env.action_spec().dtype)
-		self.action_spec_dtype = env.action_spec().dtype
-
-	@property
-	def unwrapped(self):
-		return self.env
-	
-	def _obs_to_array(self, obs):
-		return torch.from_numpy(
-			np.concatenate([v.flatten() for v in obs.values()], dtype=np.float32))
-	
-	def reset(self):
-		return self._obs_to_array(self.env.reset().observation)
-
-	def step(self, action):
-		reward = 0
-		action = action.astype(self.action_spec_dtype)
-		for _ in range(2):
-			step = self.env.step(action)
-			reward += step.reward
-		return self._obs_to_array(step.observation), reward, False, defaultdict(float)
-	
-	def render(self, width=384, height=384, camera_id=None):
-		return self.env.physics.render(height, width, camera_id or self.camera_id)
-
-
-class Pixels(gym.Wrapper):
-	def __init__(self, env, cfg, num_frames=3, size=64):
-		super().__init__(env)
-		self.cfg = cfg
-		self.env = env
-		self.observation_space = gym.spaces.Box(
-			low=0, high=255, shape=(num_frames*3, size, size), dtype=np.uint8)
-		self._frames = deque([], maxlen=num_frames)
-		self._size = size
-
-	def _get_obs(self, is_reset=False):
-		frame = self.env.render(width=self._size, height=self._size).transpose(2, 0, 1)
-		num_frames = self._frames.maxlen if is_reset else 1
-		for _ in range(num_frames):
-			self._frames.append(frame)
-		return torch.from_numpy(np.concatenate(self._frames))
-
-	def reset(self):
-		self.env.reset()
-		return self._get_obs(is_reset=True)
-
-	def step(self, action):
-		_, reward, done, info = self.env.step(action)
-		return self._get_obs(), reward, done, info
-
-
-def make_env(cfg):
-	"""
-	Make DMControl environment.
-	Adapted from https://github.com/facebookresearch/drqv2
-	"""
-	domain, task = cfg.task.replace('-', '_').split('_', 1)
-	domain = dict(cup='ball_in_cup', pointmass='point_mass').get(domain, domain)
-	if (domain, task) not in suite.ALL_TASKS:
-		raise ValueError('Unknown task:', task)
-	assert cfg.obs in {'state', 'rgb'}, 'This task only supports state and rgb observations.'
-	env = suite.load(domain,
-					 task,
-					 task_kwargs={'random': cfg.seed},
-					 visualize_reward=False)
-	env = action_scale.Wrapper(env, minimum=-1., maximum=1.)
-	env = DMControlWrapper(env, domain)
-	if cfg.obs == 'rgb':
-		env = Pixels(env, cfg)
-	env = Timeout(env, max_episode_steps=500)
-	return env
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/maniskill.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/maniskill.py
deleted file mode 100644
index 25d5937..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/maniskill.py
+++ /dev/null
@@ -1,82 +0,0 @@
-import gymnasium as gym
-import numpy as np
-from envs.wrappers.timeout import Timeout
-
-import mani_skill2.envs
-
-
-MANISKILL_TASKS = {
-	'lift-cube': dict(
-		env='LiftCube-v0',
-		control_mode='pd_ee_delta_pos',
-	),
-	'pick-cube': dict(
-		env='PickCube-v0',
-		control_mode='pd_ee_delta_pos',
-	),
-	'stack-cube': dict(
-		env='StackCube-v0',
-		control_mode='pd_ee_delta_pos',
-	),
-	'pick-ycb': dict(
-		env='PickSingleYCB-v0',
-		control_mode='pd_ee_delta_pose',
-	),
-	'turn-faucet': dict(
-		env='TurnFaucet-v0',
-		control_mode='pd_ee_delta_pose',
-	),
-}
-
-
-class ManiSkillWrapper(gym.Wrapper):
-	def __init__(self, env, cfg):
-		super().__init__(env)
-		self.env = env
-		self.cfg = cfg
-		self.observation_space = self.env.observation_space
-		self.action_space = gym.spaces.Box(
-			low=np.full(self.env.action_space.shape, self.env.action_space.low.min()),
-			high=np.full(self.env.action_space.shape, self.env.action_space.high.max()),
-			dtype=self.env.action_space.dtype,
-		)
-
-	def reset(self):
-		return self.env.reset()
-	
-	def step(self, action):
-		reward = 0
-		for _ in range(2):
-			obs, r, done, info = self.env.step(action)
-			reward += r
-			info['terminated'] = done
-			if done:
-				break
-		return obs, reward, done, info
-
-	@property
-	def unwrapped(self):
-		return self.env.unwrapped
-
-	def render(self, args, **kwargs):
-		return self.env.render(mode='cameras')
-
-
-def make_env(cfg):
-	"""
-	Make ManiSkill2 environment.
-	"""
-	if cfg.task not in MANISKILL_TASKS:
-		raise ValueError('Unknown task:', cfg.task)
-	assert cfg.obs == 'state', 'This task only supports state observations.'
-	task_cfg = MANISKILL_TASKS[cfg.task]
-	env = gym.make(
-		task_cfg['env'],
-		obs_mode='state',
-		control_mode=task_cfg['control_mode'],
-		render_camera_cfgs=dict(width=384, height=384),
-	)
-	env = ManiSkillWrapper(env, cfg)
-	env = Timeout(env, max_episode_steps=100)
-	env.max_episode_steps = env._max_episode_steps
-	return env
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/metaworld.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/metaworld.py
deleted file mode 100644
index f9b3513..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/metaworld.py
+++ /dev/null
@@ -1,52 +0,0 @@
-import numpy as np
-import gym
-from envs.wrappers.timeout import Timeout
-
-from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE
-
-
-class MetaWorldWrapper(gym.Wrapper):
-	def __init__(self, env, cfg):
-		super().__init__(env)
-		self.env = env
-		self.cfg = cfg
-		self.camera_name = "corner2"
-		self.env.model.cam_pos[2] = [0.75, 0.075, 0.7]
-		self.env._freeze_rand_vec = False
-
-	def reset(self, **kwargs):
-		obs = super().reset(**kwargs).astype(np.float32)
-		self.env.step(np.zeros(self.env.action_space.shape))
-		return obs
-
-	def step(self, action):
-		reward = 0
-		for _ in range(2):
-			obs, r, _, info = self.env.step(action.copy())
-			reward += r
-		obs = obs.astype(np.float32)
-		return obs, reward, False, info
-
-	@property
-	def unwrapped(self):
-		return self.env.unwrapped
-
-	def render(self, *args, **kwargs):
-		return self.env.render(
-			offscreen=True, resolution=(384, 384), camera_name=self.camera_name
-		).copy()
-
-
-def make_env(cfg):
-	"""
-	Make Meta-World environment.
-	"""
-	env_id = cfg.task.split("-", 1)[-1] + "-v2-goal-observable"
-	if not cfg.task.startswith('mw-') or env_id not in ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE:
-		raise ValueError('Unknown task:', cfg.task)
-	assert cfg.obs == 'state', 'This task only supports state observations.'
-	env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[env_id](seed=cfg.seed)
-	env = MetaWorldWrapper(env, cfg)
-	env = Timeout(env, max_episode_steps=100)
-	env.max_episode_steps = env._max_episode_steps
-	return env
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/mujoco.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/mujoco.py
deleted file mode 100644
index 358775c..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/mujoco.py
+++ /dev/null
@@ -1,59 +0,0 @@
-import numpy as np
-import gymnasium as gym
-from envs.wrappers.timeout import Timeout
-
-
-MUJOCO_TASKS = {
-	'mujoco-walker': 'Walker2d-v4',
-	'mujoco-halfcheetah': 'HalfCheetah-v4',
-	'bipedal-walker': 'BipedalWalker-v3',
-	'lunarlander-continuous': 'LunarLander-v2',
-}
-
-class MuJoCoWrapper(gym.Wrapper):
-	def __init__(self, env, cfg):
-		super().__init__(env)
-		self.env = env
-		self.cfg = cfg
-		self._cumulative_reward = 0
-
-	def reset(self):
-		self._cumulative_reward = 0
-		return self.env.reset()[0]
-
-	def step(self, action):
-		obs, reward, terminated, truncated, info = self.env.step(action.copy())
-		self._cumulative_reward += reward
-		done = terminated or truncated
-		info['terminated'] = terminated
-		if self.cfg.task == 'lunarlander-continuous':
-			info['success'] = self._cumulative_reward > 200
-		return obs, reward, done, info
-
-	@property
-	def unwrapped(self):
-		return self.env.unwrapped
-	
-	def render(self, **kwargs):
-		return self.env.render(**kwargs)
-
-
-def make_env(cfg):
-	"""
-	Make classic/MuJoCo environment.
-	"""
-	if not cfg.task in MUJOCO_TASKS:
-		raise ValueError('Unknown task:', cfg.task)
-	assert cfg.obs == 'state', 'This task only supports state observations.'
-	if cfg.task == 'lunarlander-continuous':
-		env = gym.make(MUJOCO_TASKS[cfg.task], continuous=True, render_mode='rgb_array')
-	else:
-		env = gym.make(MUJOCO_TASKS[cfg.task], render_mode='rgb_array')
-	env = MuJoCoWrapper(env, cfg)
-	env = Timeout(env, max_episode_steps={
-		'lunarlander-continuous': 500,
-		'bipedal-walker': 1600,
-	}.get(cfg.task, 1000)) # Default max episode steps for other tasks
-	cfg.discount_max = 0.99 # TODO: temporarily hardcode for these envs, makes comparison to other codebases easier
-	cfg.rho = 0.7 # TODO: increase rho for episodic tasks since termination always happens at the end of a sequence
-	return env
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/myosuite.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/myosuite.py
deleted file mode 100644
index ee35fd1..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/myosuite.py
+++ /dev/null
@@ -1,58 +0,0 @@
-import numpy as np
-import gymnasium as gym
-from envs.wrappers.timeout import Timeout
-
-
-MYOSUITE_TASKS = {
-	'myo-reach': 'myoHandReachFixed-v0',
-	'myo-reach-hard': 'myoHandReachRandom-v0',
-	'myo-pose': 'myoHandPoseFixed-v0',
-	'myo-pose-hard': 'myoHandPoseRandom-v0',
-	'myo-obj-hold': 'myoHandObjHoldFixed-v0',
-	'myo-obj-hold-hard': 'myoHandObjHoldRandom-v0',
-	'myo-key-turn': 'myoHandKeyTurnFixed-v0',
-	'myo-key-turn-hard': 'myoHandKeyTurnRandom-v0',
-	'myo-pen-twirl': 'myoHandPenTwirlFixed-v0',
-	'myo-pen-twirl-hard': 'myoHandPenTwirlRandom-v0',
-}
-
-
-class MyoSuiteWrapper(gym.Wrapper):
-	def __init__(self, env, cfg):
-		super().__init__(env)
-		self.env = env
-		self.cfg = cfg
-		self.camera_id = 'hand_side_inter'
-
-	def reset(self):
-		return self.env.reset()[0]
-
-	def step(self, action):
-		obs, reward, _, _, info = self.env.step(action.copy())
-		info['success'] = info['solved']
-		return obs, reward, False, info
-
-	@property
-	def unwrapped(self):
-		return self.env.unwrapped
-
-	def render(self, *args, **kwargs):
-		return self.env.sim.renderer.render_offscreen(
-			width=384, height=384, camera_id=self.camera_id
-		).copy()
-
-
-def make_env(cfg):
-	"""
-	Make Myosuite environment.
-	"""
-	if not cfg.task in MYOSUITE_TASKS:
-		raise ValueError('Unknown task:', cfg.task)
-	assert cfg.obs == 'state', 'This task only supports state observations.'
-	import myosuite
-	from myosuite.utils import gym as gym_utils
-	env = gym_utils.make(MYOSUITE_TASKS[cfg.task])
-	env = MyoSuiteWrapper(env, cfg)
-	env = Timeout(env, max_episode_steps=100)
-	env.max_episode_steps = env._max_episode_steps
-	return env
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/__init__.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/ball_in_cup.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/ball_in_cup.py
deleted file mode 100644
index fea86f6..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/ball_in_cup.py
+++ /dev/null
@@ -1,99 +0,0 @@
-import collections
-import os
-
-from dm_control import mujoco
-from dm_control.rl import control
-from dm_control.suite import base
-from dm_control.suite import ball_in_cup
-from dm_control.suite import common
-from dm_control.utils import rewards
-from dm_control.utils import io as resources
-import numpy as np
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_DIST_TARGET = 0.5
-_TARGET_SPEED = 6.
-
-_DEFAULT_TIME_LIMIT = 20  # (seconds)
-_CONTROL_TIMESTEP = .02   # (seconds)
-
-
-def get_model_and_assets():
-    """Returns a tuple containing the model XML string and a dict of assets."""
-    return resources.GetResource(os.path.join(_TASKS_DIR, 'ball_in_cup.xml')), common.ASSETS
-
-
-@ball_in_cup.SUITE.add('custom')
-def spin(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Ball-in-Cup Spin task."""
-  physics = Physics.from_xml_string(*get_model_and_assets())
-  task = CustomBallInCup(random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=_CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-class Physics(mujoco.Physics):
-  """Physics with additional features for the Ball-in-Cup domain."""
-
-  def ball_to_target(self):
-    """Returns the vector from the ball to the target."""
-    target = self.named.data.site_xpos['target', ['x', 'z']]
-    ball = self.named.data.xpos['ball', ['x', 'z']]
-    return target - ball
-
-  def in_target(self):
-    """Returns 1 if the ball is in the target, 0 otherwise."""
-    ball_to_target = abs(self.ball_to_target())
-    target_size = self.named.model.site_size['target', [0, 2]]
-    ball_size = self.named.model.geom_size['ball', 0]
-    return float(all(ball_to_target < target_size - ball_size))
-
-
-class CustomBallInCup(ball_in_cup.BallInCup):
-  """Custom Ball-in-Cup tasks."""
-
-  def initialize_episode(self, physics):
-    # Find a collision-free random initial position of the ball.
-    penetrating = True
-    valid_pos = False
-    init_out_of_target = self.random.uniform() < 0.1
-    while penetrating or not valid_pos:
-      # Assign a random ball position.
-      physics.named.data.qpos['ball_x'] = self.random.uniform(-.2, .2)
-      physics.named.data.qpos['ball_z'] = self.random.uniform(.2, .5)
-      # Check for collisions.
-      physics.after_reset()
-      penetrating = physics.data.ncon > 0
-      valid_pos = bool(physics.in_target()) or init_out_of_target
-    base.Task.initialize_episode(self, physics)
-
-  def get_observation(self, physics):
-    """Returns an observation of the state."""
-    obs = collections.OrderedDict()
-    obs['position'] = physics.position()
-    obs['velocity'] = physics.velocity()
-    return obs
-
-  def get_reward(self, physics):
-    dist = np.linalg.norm(physics.ball_to_target())
-    ball_vel_x = abs(physics.named.data.qvel['ball_x'])
-    ball_vel_z = abs(physics.named.data.qvel['ball_z'])
-    ball_vel = np.linalg.norm([ball_vel_x, ball_vel_z])
-
-    # reward: spin around target (maximize distance to target + ball velocity)
-    dist_reward = rewards.tolerance(dist,
-                                    bounds=(_DIST_TARGET, float('inf')),
-                                    margin=_DIST_TARGET/2,
-                                    value_at_margin=0.5,
-                                    sigmoid='linear')
-    not_in_target = 1 - physics.in_target()
-    vel_reward = rewards.tolerance(ball_vel,
-                                   bounds=(_TARGET_SPEED, float('inf')),
-                                   margin=_TARGET_SPEED/2,
-                                   value_at_margin=0.5,
-                                   sigmoid='linear')
-    spin_reward = not_in_target * (dist_reward + 2*vel_reward) / 3
-    return spin_reward
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/ball_in_cup.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/ball_in_cup.xml
deleted file mode 100644
index 32708c1..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/ball_in_cup.xml
+++ /dev/null
@@ -1,53 +0,0 @@
-<mujoco model="ball in cup">
-
-  <include file="./common/visual.xml"/>
-  <include file="./common/skybox.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <default>
-    <motor ctrllimited="true" ctrlrange="-1 1" gear="5"/>
-    <default class="cup">
-      <joint type="slide" damping="3" stiffness="20"/>
-      <geom type="capsule" size=".008" material="self"/>
-    </default>
-  </default>
-
-  <worldbody>
-    <light name="light" directional="true" diffuse=".6 .6 .6" pos="0 0 2" specular=".3 .3 .3"/>
-    <geom name="ground" type="plane" pos="0 0 0" size=".6 .2 10" material="grid"/>
-    <camera name="cam0" pos="0 -1 .8" xyaxes="1 0 0 0 1 2"/>
-    <camera name="cam1" pos="0 -1 .4" xyaxes="1 0 0 0 0 1" />
-
-    <body name="cup" pos="0 0 .6" childclass="cup">
-      <joint name="cup_x" axis="1 0 0"/>
-      <joint name="cup_z" axis="0 0 1"/>
-      <geom name="cup_part_0" fromto="-.05 0 0 -.05 0 -.075" />
-      <geom name="cup_part_1" fromto="-.05 0 -.075 -.025 0 -.1" />
-      <geom name="cup_part_2" fromto="-.025 0 -.1 .025 0 -.1" />
-      <geom name="cup_part_3" fromto=".025 0 -.1 .05 0 -.075" />
-      <geom name="cup_part_4" fromto=".05 0 -.075 .05 0 0" />
-      <site name="cup" pos="0 0 -.108" size=".005"/>
-      <site name="target" type="box" pos="0 0 -.05" size=".05 .006 .05" group="4"/>
-    </body>
-
-    <body name="ball" pos="0 0 .2">
-      <joint name="ball_x" type="slide" axis="1 0 0"/>
-      <joint name="ball_z" type="slide" axis="0 0 1"/>
-      <geom name="ball" type="sphere" size=".025" material="effector"/>
-      <site name="ball" size=".005"/>
-    </body>
-  </worldbody>
-
-  <actuator>
-    <motor name="x" joint="cup_x"/>
-    <motor name="z" joint="cup_z"/>
-  </actuator>
-
-  <tendon>
-    <spatial name="string" limited="true" range="0 0.3" width="0.003">
-      <site site="ball"/>
-      <site site="cup"/>
-    </spatial>
-  </tendon>
-
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/cheetah.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/cheetah.py
deleted file mode 100644
index f24d2f6..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/cheetah.py
+++ /dev/null
@@ -1,268 +0,0 @@
-import os
-
-from dm_control.rl import control
-from dm_control.suite import common
-from dm_control.suite import cheetah
-from dm_control.utils import rewards
-from dm_control.utils import io as resources
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_CHEETAH_JUMP_HEIGHT = 1.2
-_CHEETAH_LIE_HEIGHT = 0.25
-_CHEETAH_SPIN_SPEED = 8
-
-
-def get_model_and_assets():
-    """Returns a tuple containing the model XML string and a dict of assets."""
-    return resources.GetResource(os.path.join(_TASKS_DIR, 'cheetah.xml')), common.ASSETS
-
-
-@cheetah.SUITE.add('custom')
-def run_backwards(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Run Backwards task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='run-backwards', move_speed=cheetah._RUN_SPEED*0.8, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def stand_front(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Stand Front task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='stand-front', move_speed=0.5, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def stand_back(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Stand Back task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='stand-back', move_speed=0.5, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def jump(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Jump task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='jump', move_speed=0.5, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def run_front(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Run Front task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='run-front', move_speed=cheetah._RUN_SPEED*0.6, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def run_back(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Run Back task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='run-back', move_speed=cheetah._RUN_SPEED*0.6, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def lie_down(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Lie Down task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='lie-down', random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def legs_up(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Legs Up task."""
-    physics = cheetah.Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='legs-up', random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def flip(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Flip task."""
-    physics = Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='flip', move_speed=cheetah._RUN_SPEED, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-@cheetah.SUITE.add('custom')
-def flip_backwards(time_limit=cheetah._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-    """Returns the Flip Backwards task."""
-    physics = Physics.from_xml_string(*get_model_and_assets())
-    task = CustomCheetah(goal='flip-backwards', move_speed=cheetah._RUN_SPEED*0.8, random=random)
-    environment_kwargs = environment_kwargs or {}
-    return control.Environment(physics, task, time_limit=time_limit,
-                               **environment_kwargs)
-
-
-class Physics(cheetah.Physics):
-    """Physics simulation with additional features for the Cheetah domain."""
-
-    def angmomentum(self):
-        """Returns the angular momentum of torso of the Cheetah about Y axis."""
-        return self.named.data.subtree_angmom['torso'][1]
-
-
-class CustomCheetah(cheetah.Cheetah):
-    """Custom Cheetah tasks."""
-    
-    def __init__(self, goal='run-backwards', move_speed=0, random=None):
-        super().__init__(random)
-        self._goal = goal
-        self._move_speed = move_speed
-
-    def _run_backwards_reward(self, physics):
-        return rewards.tolerance(physics.speed(),
-                            bounds=(-float('inf'), -self._move_speed),
-                            margin=self._move_speed,
-                            value_at_margin=0,
-                            sigmoid='linear')
-       
-    def _stand_one_foot_reward(self, physics, foot):
-        """Note: `foot` is the foot that is *not* on the ground."""
-        torso_height = physics.named.data.xpos['torso', 'z']
-        foot_height = physics.named.data.xpos[foot, 'z']
-        height_reward = rewards.tolerance((torso_height + foot_height)/2,
-                            bounds=(_CHEETAH_JUMP_HEIGHT, float('inf')),
-                            margin=_CHEETAH_JUMP_HEIGHT/2)
-        horizontal_speed_reward = rewards.tolerance(physics.speed(),
-                            bounds=(-self._move_speed, self._move_speed),
-                            margin=self._move_speed,
-                            value_at_margin=0,
-                            sigmoid='linear')
-        stand_reward = (5*height_reward + horizontal_speed_reward) / 6
-        return stand_reward
-
-    def _stand_front_reward(self, physics):
-        return self._stand_one_foot_reward(physics, 'bfoot')
-    
-    def _stand_back_reward(self, physics):
-        return self._stand_one_foot_reward(physics, 'ffoot')
-    
-    def _jump_reward(self, physics):
-        front_reward = self._stand_front_reward(physics)
-        back_reward = self._stand_back_reward(physics)
-        jump_reward = (front_reward + back_reward) / 2
-        return jump_reward
-
-    def _run_one_foot_reward(self, physics, foot):
-        """Note: `foot` is the foot that is *not* on the ground."""
-        torso_height = physics.named.data.xpos['torso', 'z']
-        foot_height = physics.named.data.xpos[foot, 'z']
-        torso_up = rewards.tolerance(torso_height,
-                            bounds=(_CHEETAH_JUMP_HEIGHT, float('inf')),
-                            margin=_CHEETAH_JUMP_HEIGHT/2)
-        foot_up = rewards.tolerance(foot_height,
-                            bounds=(_CHEETAH_JUMP_HEIGHT, float('inf')),
-                            margin=_CHEETAH_JUMP_HEIGHT/2)
-        up_reward = (3*foot_up + 2*torso_up) / 5
-        if self._move_speed == 0:
-            return up_reward
-        horizontal_speed_reward = rewards.tolerance(physics.speed(),
-                            bounds=(self._move_speed, float('inf')),
-                            margin=self._move_speed,
-                            value_at_margin=0,
-                            sigmoid='linear')
-        return up_reward * (5*horizontal_speed_reward + 1) / 6
-
-    def _run_front_reward(self, physics):
-        return self._run_one_foot_reward(physics, 'bfoot')
-    
-    def _run_back_reward(self, physics):
-        return self._run_one_foot_reward(physics, 'ffoot')
-
-    def _lie_down_reward(self, physics):
-        torso_height = physics.named.data.xpos['torso', 'z']
-        feet_height = (physics.named.data.xpos['ffoot', 'z'] + physics.named.data.xpos['bfoot', 'z']) / 2
-        torso_down = rewards.tolerance(torso_height,
-                            bounds=(-float('inf'), _CHEETAH_LIE_HEIGHT),
-                            margin=_CHEETAH_LIE_HEIGHT,
-                            value_at_margin=0,
-                            sigmoid='linear')
-        feet_down = rewards.tolerance(feet_height,
-                            bounds=(-float('inf'), _CHEETAH_LIE_HEIGHT),
-                            margin=_CHEETAH_LIE_HEIGHT,
-                            value_at_margin=0,
-                            sigmoid='linear')
-        lie_down_reward = (3*torso_down + feet_down) / 4
-        return lie_down_reward
-
-    def _legs_up_reward(self, physics):
-        torso_height = physics.named.data.xpos['torso', 'z']
-        torso_down = rewards.tolerance(torso_height,
-                            bounds=(-float('inf'), _CHEETAH_LIE_HEIGHT),
-                            margin=_CHEETAH_LIE_HEIGHT/2)
-        get_up = self._run_one_foot_reward(physics, 'bfoot')
-        legs_up_reward = (5*torso_down + get_up) / 6
-        return legs_up_reward
-    
-    def _flip_reward(self, physics, forward=True):
-        spin_reward = rewards.tolerance(
-                            (1. if forward else -1.) * physics.angmomentum(),
-                            bounds=(_CHEETAH_SPIN_SPEED, float('inf')),
-                            margin=_CHEETAH_SPIN_SPEED,
-                            value_at_margin=0,
-                            sigmoid='linear')
-        horizontal_speed_reward = rewards.tolerance(
-                            (1. if forward else -1.) * physics.speed(),
-                            bounds=(self._move_speed, float('inf')),
-                            margin=self._move_speed,
-                            value_at_margin=0,
-                            sigmoid='linear')
-        flip_reward = (2*spin_reward + horizontal_speed_reward) / 3
-        return flip_reward
-
-    def get_reward(self, physics):
-        if self._goal == 'run-backwards':
-            return self._run_backwards_reward(physics)
-        elif self._goal == 'stand-front':
-            return self._stand_front_reward(physics)
-        elif self._goal == 'stand-back':
-            return self._stand_back_reward(physics)
-        elif self._goal == 'jump':
-            return self._jump_reward(physics)
-        elif self._goal == 'run-front':
-            return self._run_front_reward(physics)
-        elif self._goal == 'run-back':
-            return self._run_back_reward(physics)
-        elif self._goal == 'lie-down':
-            return self._lie_down_reward(physics)
-        elif self._goal == 'legs-up':
-            return self._legs_up_reward(physics)
-        elif self._goal == 'flip':
-            return self._flip_reward(physics, forward=True)
-        elif self._goal == 'flip-backwards':
-            return self._flip_reward(physics, forward=False)
-        else:
-            raise NotImplementedError(f'Goal {self._goal} is not implemented.')
-
-
-if __name__ == '__main__':
-    env = jump()
-    obs = env.reset()
-    import numpy as np
-    next_obs, reward, done, info = env.step(np.zeros(6))
-    print(reward)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/cheetah.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/cheetah.xml
deleted file mode 100644
index 1a7f6fd..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/cheetah.xml
+++ /dev/null
@@ -1,73 +0,0 @@
-<mujoco model="cheetah">
-  <include file="./common/skybox.xml"/>
-  <include file="./common/visual.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <compiler settotalmass="14"/>
-
-  <default>
-    <default class="cheetah">
-      <joint limited="true" damping=".01" armature=".1" stiffness="8" type="hinge" axis="0 1 0"/>
-      <geom contype="1" conaffinity="1" condim="3" friction=".4 .1 .1" material="self"/>
-    </default>
-    <default class="free">
-      <joint limited="false" damping="0" armature="0" stiffness="0"/>
-    </default>
-    <motor ctrllimited="true" ctrlrange="-1 1"/>
-  </default>
-
-  <statistic center="0 0 .7" extent="2"/>
-
-  <option timestep="0.01"/>
-
-  <worldbody>
-    <geom name="ground" type="plane" conaffinity="1" pos="98 0 0" size="200 .8 .5" material="grid"/>
-    <body name="torso" pos="0 0 .7" childclass="cheetah">
-      <light name="light" pos="0 0 2" mode="trackcom"/>
-      <camera name="side" pos="0 -3 0" quat="0.707 0.707 0 0" mode="trackcom"/>
-      <camera name="back" pos="-1.8 -1.3 0.8" xyaxes="0.45 -0.9 0 0.3 0.15 0.94" mode="trackcom"/>
-      <joint name="rootx" type="slide" axis="1 0 0" class="free"/>
-      <joint name="rootz" type="slide" axis="0 0 1" class="free"/>
-      <joint name="rooty" type="hinge" axis="0 1 0" class="free"/>
-      <geom name="torso" type="capsule" fromto="-.5 0 0 .5 0 0" size="0.046"/>
-      <geom name="head" type="capsule" pos=".6 0 .1" euler="0 50 0" size="0.046 .15"/>
-      <body name="bthigh" pos="-.5 0 0">
-        <joint name="bthigh" range="-30 60" stiffness="240" damping="6"/>
-        <geom name="bthigh" type="capsule" pos=".1 0 -.13" euler="0 -218 0" size="0.046 .145"/>
-        <body name="bshin" pos=".16 0 -.25">
-          <joint name="bshin" range="-50 50" stiffness="180" damping="4.5"/>
-          <geom name="bshin" type="capsule" pos="-.14 0 -.07" euler="0 -116 0" size="0.046 .15"/>
-          <body name="bfoot" pos="-.28 0 -.14">
-            <joint name="bfoot" range="-230 50" stiffness="120" damping="3"/>
-            <geom name="bfoot" type="capsule" pos=".03 0 -.097" euler="0 -15 0" size="0.046 .094"/>
-          </body>
-        </body>
-      </body>
-      <body name="fthigh" pos=".5 0 0">
-        <joint name="fthigh" range="-57 .40" stiffness="180" damping="4.5"/>
-        <geom name="fthigh" type="capsule" pos="-.07 0 -.12" euler="0 30 0" size="0.046 .133"/>
-        <body name="fshin" pos="-.14 0 -.24">
-          <joint name="fshin" range="-70 50" stiffness="120" damping="3"/>
-          <geom name="fshin" type="capsule" pos=".065 0 -.09" euler="0 -34 0" size="0.046 .106"/>
-          <body name="ffoot" pos=".13 0 -.18">
-            <joint name="ffoot" range="-28 28" stiffness="60" damping="1.5"/>
-            <geom name="ffoot" type="capsule" pos=".045 0 -.07" euler="0 -34 0" size="0.046 .07"/>
-          </body>
-        </body>
-      </body>
-    </body>
-  </worldbody>
-
-  <sensor>
-    <subtreelinvel name="torso_subtreelinvel" body="torso"/>
-  </sensor>
-
-  <actuator>
-    <motor name="bthigh" joint="bthigh" gear="120" />
-    <motor name="bshin" joint="bshin" gear="90" />
-    <motor name="bfoot" joint="bfoot" gear="60" />
-    <motor name="fthigh" joint="fthigh" gear="90" />
-    <motor name="fshin" joint="fshin" gear="60" />
-    <motor name="ffoot" joint="ffoot" gear="30" />
-  </actuator>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/fish.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/fish.py
deleted file mode 100644
index 59bed4b..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/fish.py
+++ /dev/null
@@ -1,79 +0,0 @@
-import collections
-import os
-
-from dm_control import mujoco
-from dm_control.rl import control
-from dm_control.suite import base
-from dm_control.suite import common
-from dm_control.suite import fish
-from dm_control.utils import rewards
-from dm_control.utils import io as resources
-import numpy as np
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_DEFAULT_TIME_LIMIT = 40
-_CONTROL_TIMESTEP = .04
-_JOINTS = ['tail1',
-           'tail_twist',
-           'tail2',
-           'finright_roll',
-           'finright_pitch',
-           'finleft_roll',
-           'finleft_pitch']
-
-
-def get_model_and_assets():
-    """Returns a tuple containing the model XML string and a dict of assets."""
-    return resources.GetResource(os.path.join(_TASKS_DIR, 'fish.xml')), common.ASSETS
-
-
-@fish.SUITE.add('custom')
-def obstacles(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Fish Obstacles task."""
-  physics = fish.Physics.from_xml_string(*get_model_and_assets())
-  task = Obstacles(random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, control_timestep=_CONTROL_TIMESTEP, time_limit=time_limit,
-      **environment_kwargs)
-
-
-class Obstacles(fish.Swim):
-  """A custom Fish Obstacles task."""
-
-  def __init__(self, random=None):
-    super().__init__(random=random)
-
-  def in_wall(self, physics, name, min_distance=0.08):
-    """Returns True if the given body is too close to a wall."""
-    for wall in ['wall0', 'wall1', 'wall2', 'wall3']:
-      l1_dist = np.min(np.abs(physics.named.data.geom_xpos[name][:2] - physics.named.data.geom_xpos[wall][:2]))
-      if l1_dist < min_distance:
-        return True
-    return False
-
-  def initialize_episode(self, physics):
-    in_wall = True
-    while in_wall:
-        # Randomize fish position.
-        quat = self.random.randn(4)
-        physics.named.data.qpos['root'][3:7] = quat / np.linalg.norm(quat)
-        for joint in _JOINTS:
-            physics.named.data.qpos[joint] = self.random.uniform(-.2, .2)
-        # Randomize target position.
-        physics.named.model.geom_pos['target', 'x'] = self.random.uniform(-.4, .4)
-        physics.named.model.geom_pos['target', 'y'] = self.random.uniform(-.4, .4)
-        physics.named.model.geom_pos['target', 'z'] = self.random.uniform(.1, .3)
-        # Make sure target is not too close to a wall.
-        physics.after_reset()
-        in_wall = self.in_wall(physics, 'target')
-    base.Task.initialize_episode(self, physics)
-
-  def get_reward(self, physics):
-    radii = physics.named.model.geom_size[['mouth', 'target'], 0].sum()
-    in_target = rewards.tolerance(np.linalg.norm(physics.mouth_to_target()),
-                                  bounds=(0, radii), margin=2*radii)
-    is_upright = 0.5 * (physics.upright() + 1)
-    is_not_in_wall = 1. - self.in_wall(physics, 'torso', min_distance=0.06)
-    return is_not_in_wall * (7*in_target + is_upright) / 8
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/fish.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/fish.xml
deleted file mode 100644
index 82c9ede..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/fish.xml
+++ /dev/null
@@ -1,93 +0,0 @@
-<mujoco model="fish">
-  <include file="./common/visual.xml"/>
-  <include file="./common/materials.xml"/>
-  <asset>
-      <texture name="skybox" type="skybox" builtin="gradient" rgb1=".4 .6 .8" rgb2="0 0 0" width="800" height="800" mark="random" markrgb="1 1 1"/>
-  </asset>
-
-
-  <option timestep="0.004" density="5000">
-    <flag gravity="disable" constraint="disable"/>
-  </option>
-
-  <default>
-    <general ctrllimited="true"/>
-    <default class="fish">
-      <joint type="hinge" limited="false" range="-60 60" damping="2e-5" solreflimit=".1 1" solimplimit="0 .8 .1"/>
-      <geom material="self"/>
-    </default>
-    <default class="wall">
-      <geom type="box" material="self"/>
-    </default>
-  </default>
-
-  <worldbody>
-    <camera name="tracking_top" pos="0 0 1" xyaxes="1 0 0 0 1 0" mode="trackcom"/>
-    <camera name="tracking_x" pos="-.3 0 .2" xyaxes="0 -1 0 0.342 0 0.940" fovy="60" mode="trackcom"/>
-    <camera name="tracking_y" pos="0 -.3 .2" xyaxes="1 0 0 0 0.342 0.940" fovy="60" mode="trackcom"/>
-    <camera name="fixed_top" pos="0 0 5.5" fovy="10"/>
-    <geom name="ground" type="plane" size=".5 .5 .1" material="grid"/>
-
-    <geom name="wall0" class="wall" pos="-.15 -.15 .1" size=".05 .05 .1"/>
-    <geom name="wall1" class="wall" pos=".15 -.15 .1" size=".05 .05 .1"/>
-    <geom name="wall2" class="wall" pos=".15 .15 .1" size=".05 .05 .1"/>
-    <geom name="wall3" class="wall" pos="-.15 .15 .1" size=".05 .05 .1"/>
-
-    <geom name="target" type="sphere" pos="0 .4 .1" size=".04" material="target"/>
-    <body name="torso" pos="0 0 .1" childclass="fish">
-      <light name="light" diffuse=".6 .6 .6" pos="0 0 0.5" dir="0 0 -1" specular=".3 .3 .3" mode="track"/>
-      <joint name="root" type="free" damping="0" limited="false"/>
-      <site name="torso" size=".01" rgba="0 0 0 0"/>
-      <geom name="eye" type="ellipsoid" pos="0 .055 .015" size=".008 .012 .008" euler="-10 0 0" material="eye" mass="0"/>
-      <camera name="eye" pos="0 .06 .02" xyaxes="1 0 0 0 0 1"/>
-      <geom name="mouth" type="capsule" fromto="0 .079 0 0 .07 0" size=".005" material="effector" mass="0"/>
-      <geom name="lower_mouth" type="capsule" fromto="0 .079 -.004 0 .07 -.003" size=".0045" material="effector" mass="0"/>
-      <geom name="torso" type="ellipsoid" size=".01 .08 .04" mass="0"/>
-      <geom name="back_fin" type="ellipsoid" size=".001 .03 .015" pos="0 -.03 .03" material="effector" mass="0"/>
-      <geom name="torso_massive" type="box" size=".002 .06 .03" group="4"/>
-      <body name="tail1" pos="0 -.09 0">
-        <joint name="tail1" axis="0 0 1" pos="0 .01 0"/>
-        <joint name="tail_twist" axis="0 1 0" pos="0 .01 0" range="-30 30"/>
-        <geom name="tail1" type="ellipsoid" size=".001 .008 .016"/>
-        <body name="tail2" pos="0 -.028 0">
-          <joint name="tail2" axis="0 0 1" pos="0 .02 0" stiffness="8e-5"/>
-          <geom name="tail2" type="ellipsoid" size=".001 .018 .035"/>
-        </body>
-      </body>
-      <body name="finright" pos=".01 0 0">
-        <joint name="finright_roll" axis="0 1 0"/>
-        <joint name="finright_pitch" axis="1 0 0" pos="0 .005 0"/>
-        <geom name="finright" type="ellipsoid" pos=".015 0 0" size=".02 .015 .001"  />
-      </body>
-      <body name="finleft" pos="-.01 0 0">
-        <joint name="finleft_roll" axis="0 1 0"/>
-        <joint name="finleft_pitch" axis="1 0 0" pos="0 .005 0"/>
-        <geom name="finleft" type="ellipsoid"  pos="-.015 0 0" size=".02 .015 .001"/>
-      </body>
-    </body>
-  </worldbody>
-
-  <tendon>
-    <fixed name="fins_flap">
-      <joint joint="finleft_roll"  coef="-.5"/>
-      <joint joint="finright_roll" coef=".5"/>
-    </fixed>
-    <fixed name="fins_sym" stiffness="1e-4">
-      <joint joint="finleft_roll"  coef=".5"/>
-      <joint joint="finright_roll" coef=".5"/>
-    </fixed>
-  </tendon>
-
-  <actuator>
-    <position name="tail"           joint="tail1"           ctrlrange="-1 1"    kp="5e-4"/>
-    <position name="tail_twist"     joint="tail_twist"      ctrlrange="-1 1"    kp="1e-4"/>
-    <position name="fins_flap"      tendon="fins_flap"      ctrlrange="-1 1"    kp="3e-4"/>
-    <position name="finleft_pitch"  joint="finleft_pitch"   ctrlrange="-1 1"    kp="1e-4"/>
-    <position name="finright_pitch" joint="finright_pitch"  ctrlrange="-1 1"    kp="1e-4"/>
-  </actuator>
-
-  <sensor>
-    <velocimeter name="velocimeter" site="torso"/>
-    <gyro name="gyro" site="torso"/>
-  </sensor>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/hopper.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/hopper.py
deleted file mode 100644
index 3e19b1c..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/hopper.py
+++ /dev/null
@@ -1,114 +0,0 @@
-import os
-
-from dm_control import mujoco
-from dm_control.rl import control
-from dm_control.suite import common
-from dm_control.suite import hopper
-from dm_control.utils import rewards
-from dm_control.utils import io as resources
-import numpy as np
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_CONTROL_TIMESTEP = .02  # (Seconds)
-
-# Default duration of an episode, in seconds.
-_DEFAULT_TIME_LIMIT = 20
-
-# Minimal height of torso over foot above which stand reward is 1.
-_STAND_HEIGHT = 0.6
-
-# Hopping speed above which hop reward is 1.
-_HOP_SPEED = 2
-
-# Angular momentum above which reward is 1.
-_SPIN_SPEED = 5
-
-
-def get_model_and_assets():
-	"""Returns a tuple containing the model XML string and a dict of assets."""
-	return resources.GetResource(os.path.join(_TASKS_DIR, 'hopper.xml')), common.ASSETS
-
-
-@hopper.SUITE.add('custom')
-def hop_backwards(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-	"""Returns the Hop Backwards task."""
-	physics = Physics.from_xml_string(*get_model_and_assets())
-	task = CustomHopper(goal='hop-backwards', random=random)
-	environment_kwargs = environment_kwargs or {}
-	return control.Environment(
-		physics, task, time_limit=time_limit, control_timestep=_CONTROL_TIMESTEP,
-		**environment_kwargs)
-
-
-@hopper.SUITE.add('custom')
-def flip(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-	"""Returns the Flip task."""
-	physics = Physics.from_xml_string(*get_model_and_assets())
-	task = CustomHopper(goal='flip', random=random)
-	environment_kwargs = environment_kwargs or {}
-	return control.Environment(
-		physics, task, time_limit=time_limit, control_timestep=_CONTROL_TIMESTEP,
-		**environment_kwargs)
-
-
-@hopper.SUITE.add('custom')
-def flip_backwards(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-	"""Returns the Flip Backwards task."""
-	physics = Physics.from_xml_string(*get_model_and_assets())
-	task = CustomHopper(goal='flip-backwards', random=random)
-	environment_kwargs = environment_kwargs or {}
-	return control.Environment(
-		physics, task, time_limit=time_limit, control_timestep=_CONTROL_TIMESTEP,
-		**environment_kwargs)
-
-
-class Physics(hopper.Physics):
-
-	def angmomentum(self):
-		"""Returns the angular momentum of torso of the Cheetah about Y axis."""
-		return self.named.data.subtree_angmom['torso'][1]
-
-
-class CustomHopper(hopper.Hopper):
-	"""Custom Hopper tasks."""
-
-	def __init__(self, goal='hop-backwards', random=None):
-		super().__init__(None, random)
-		self._goal = goal
-	
-	def _hop_backwards_reward(self, physics):
-		standing = rewards.tolerance(physics.height(), (_STAND_HEIGHT, 2))
-		hopping = rewards.tolerance(physics.speed(),
-									bounds=(-float('inf'), -_HOP_SPEED/2),
-									margin=_HOP_SPEED/4,
-									value_at_margin=0.5,
-									sigmoid='linear')
-		return standing * hopping
-	
-	def _flip_reward(self, physics, forward=True):
-		reward = rewards.tolerance((1. if forward else -1.) * physics.angmomentum(),
-								   bounds=(_SPIN_SPEED, float('inf')),
-								   margin=_SPIN_SPEED/2,
-								   value_at_margin=0,
-								   sigmoid='linear')
-		return reward
-
-
-	def get_reward(self, physics):
-		if self._goal == 'hop-backwards':
-			return self._hop_backwards_reward(physics)
-		elif self._goal == 'flip':
-			return self._flip_reward(physics, forward=True)
-		elif self._goal == 'flip-backwards':
-			return self._flip_reward(physics, forward=False)
-		else:
-			raise NotImplementedError(f'Goal {self._goal} is not implemented.')
-
-
-if __name__ == '__main__':
-	env = hop_backwards()
-	obs = env.reset()
-	import numpy as np
-	next_obs, reward, done, info = env.step(np.zeros(2))
-	print(reward)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/hopper.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/hopper.xml
deleted file mode 100644
index 84ad72e..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/hopper.xml
+++ /dev/null
@@ -1,66 +0,0 @@
-<mujoco model="planar hopper">
-  <include file="./common/skybox.xml"/>
-  <include file="./common/visual.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <statistic extent="2" center="0 0 .5"/>
-
-  <default>
-    <default class="hopper">
-      <joint type="hinge" axis="0 1 0" limited="true" damping=".05" armature=".2"/>
-      <geom type="capsule" material="self"/>
-      <site type="sphere" size="0.05" group="3"/>
-    </default>
-    <default class="free">
-      <joint limited="false" damping="0" armature="0" stiffness="0"/>
-    </default>
-    <motor ctrlrange="-1 1" ctrllimited="true"/>
-  </default>
-
-  <option timestep="0.005"/>
-
-  <worldbody>
-    <camera name="cam0" pos="0 -2.8 0.8" euler="90 0 0" mode="trackcom"/>
-    <camera name="back" pos="-2 -.2 1.2" xyaxes="0.2 -1 0 .5 0 2" mode="trackcom"/>
-    <geom name="floor" type="plane" conaffinity="1" pos="48 0 0" size="50 1 .2" material="grid"/>
-    <body name="torso" pos="0 0 1" childclass="hopper">
-      <light name="top" pos="0 0 2" mode="trackcom"/>
-      <joint name="rootx" type="slide" axis="1 0 0" class="free"/>
-      <joint name="rootz" type="slide" axis="0 0 1" class="free"/>
-      <joint name="rooty" type="hinge" axis="0 1 0" class="free"/>
-      <geom name="torso" fromto="0 0 -.05 0 0 .2" size="0.0653"/>
-      <geom name="nose" fromto=".08 0 .13 .15 0 .14" size="0.03"/>
-      <body name="pelvis" pos="0 0 -.05">
-        <joint name="waist" range="-30 30"/>
-        <geom name="pelvis" fromto="0 0 0 0 0 -.15" size="0.065"/>
-        <body name="thigh" pos="0 0 -.2">
-          <joint name="hip" range="-170 10"/>
-          <geom name="thigh" fromto="0 0 0 0 0 -.33" size="0.04"/>
-          <body name="calf" pos="0 0 -.33">
-            <joint name="knee" range="5 150"/>
-            <geom name="calf" fromto="0 0 0 0 0 -.32" size="0.03"/>
-            <body name="foot" pos="0 0 -.32">
-              <joint name="ankle" range="-45 45"/>
-              <geom name="foot" fromto="-.08 0 0 .17 0 0" size="0.04"/>
-              <site name="touch_toe" pos=".17 0 0"/>
-              <site name="touch_heel" pos="-.08 0 0"/>
-            </body>
-          </body>
-        </body>
-      </body>
-    </body>
-  </worldbody>
-
-  <sensor>
-    <subtreelinvel name="torso_subtreelinvel" body="torso"/>
-    <touch name="touch_toe" site="touch_toe"/>
-    <touch name="touch_heel" site="touch_heel"/>
-  </sensor>
-
-  <actuator>
-    <motor name="waist" joint="waist" gear="30"/>
-    <motor name="hip" joint="hip" gear="40"/>
-    <motor name="knee" joint="knee" gear="30"/>
-    <motor name="ankle" joint="ankle" gear="10"/>
-  </actuator>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/pendulum.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/pendulum.py
deleted file mode 100644
index 3a5b636..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/pendulum.py
+++ /dev/null
@@ -1,43 +0,0 @@
-import os
-
-from dm_control.rl import control
-from dm_control.suite import pendulum
-from dm_control.suite import common
-from dm_control.utils import rewards
-from dm_control.utils import io as resources
-import numpy as np
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_DEFAULT_TIME_LIMIT = 20
-_TARGET_SPEED = 9.
-
-
-def get_model_and_assets():
-    """Returns a tuple containing the model XML string and a dict of assets."""
-    return resources.GetResource(os.path.join(_TASKS_DIR, 'pendulum.xml')), common.ASSETS
-
-
-@pendulum.SUITE.add('custom')
-def spin(time_limit=_DEFAULT_TIME_LIMIT, random=None,
-            environment_kwargs=None):
-  """Returns pendulum spin task."""
-  physics = pendulum.Physics.from_xml_string(*get_model_and_assets())
-  task = Spin(random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, **environment_kwargs)
-
-
-class Spin(pendulum.SwingUp):
-  """A custom Pendulum Spin task."""
-
-  def __init__(self, random=None):
-    super().__init__(random=random)
-
-  def get_reward(self, physics):
-    return rewards.tolerance(np.linalg.norm(physics.angular_velocity()),
-                             bounds=(_TARGET_SPEED, float('inf')),
-                             margin=_TARGET_SPEED/2,
-                             value_at_margin=0.5,
-                            sigmoid='linear')
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/pendulum.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/pendulum.xml
deleted file mode 100644
index 14377ae..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/pendulum.xml
+++ /dev/null
@@ -1,26 +0,0 @@
-<mujoco model="pendulum">
-  <include file="./common/visual.xml"/>
-  <include file="./common/skybox.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <option timestep="0.02">
-    <flag contact="disable" energy="enable"/>
-  </option>
-
-  <worldbody>
-    <light name="light" pos="0 0 2"/>
-    <geom name="floor" size="2 2 .2" type="plane" material="grid"/>
-    <camera name="fixed" pos="0 -1.5 2" xyaxes='1 0 0 0 1 1'/>
-    <camera name="lookat" mode="targetbodycom" target="pole" pos="0 -2 1"/>
-    <body name="pole" pos="0 0 .6">
-      <joint name="hinge" type="hinge" axis="0 1 0" damping="0.1"/>
-      <geom name="base" material="decoration" type="cylinder" fromto="0 -.03 0 0 .03 0" size="0.021" mass="0"/>
-      <geom name="pole" material="self" type="capsule" fromto="0 0 0 0 0 0.5" size="0.02" mass="0"/>
-      <geom name="mass" material="effector" type="sphere" pos="0 0 0.5" size="0.05" mass="1"/>
-    </body>
-  </worldbody>
-
-  <actuator>
-    <motor name="torque" joint="hinge" gear="1" ctrlrange="-1 1" ctrllimited="true"/>
-  </actuator>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher.py
deleted file mode 100644
index 4c1778e..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher.py
+++ /dev/null
@@ -1,89 +0,0 @@
-import collections
-import os
-
-from dm_control import mujoco
-from dm_control.rl import control
-from dm_control.suite import common
-from dm_control.suite import reacher
-from dm_control.utils import io as resources
-import numpy as np
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_DEFAULT_TIME_LIMIT = 20
-_BIG_TARGET = .05
-_SMALL_TARGET = .015
-
-
-def get_model_and_assets(links):
-    """Returns a tuple containing the model XML string and a dict of assets."""
-    assert links in {3, 4}, 'Only 3 or 4 links are supported.'
-    fn = 'reacher_three_links.xml' if links == 3 else 'reacher_four_links.xml'
-    return resources.GetResource(os.path.join(_TASKS_DIR, fn)), common.ASSETS
-
-
-@reacher.SUITE.add('custom')
-def three_easy(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns three-link reacher with sparse reward with 5e-2 tol and randomized target."""
-  physics = Physics.from_xml_string(*get_model_and_assets(links=3))
-  task = CustomThreeLinkReacher(target_size=_BIG_TARGET, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, **environment_kwargs)
-
-
-@reacher.SUITE.add('custom')
-def three_hard(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns three-link reacher with sparse reward with 1e-2 tol and randomized target."""
-  physics = Physics.from_xml_string(*get_model_and_assets(links=3))
-  task = CustomThreeLinkReacher(target_size=_SMALL_TARGET, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, **environment_kwargs)
-
-
-@reacher.SUITE.add('custom')
-def four_easy(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns three-link reacher with sparse reward with 5e-2 tol and randomized target."""
-  physics = Physics.from_xml_string(*get_model_and_assets(links=4))
-  task = CustomThreeLinkReacher(target_size=_BIG_TARGET, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, **environment_kwargs)
-
-
-@reacher.SUITE.add('custom')
-def four_hard(time_limit=_DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns three-link reacher with sparse reward with 1e-2 tol and randomized target."""
-  physics = Physics.from_xml_string(*get_model_and_assets(links=4))
-  task = CustomThreeLinkReacher(target_size=_SMALL_TARGET, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, **environment_kwargs)
-
-
-class Physics(mujoco.Physics):
-  """Physics simulation with additional features for the Reacher domain."""
-
-  def finger_to_target(self):
-    """Returns the vector from target to finger in global coordinates."""
-    return (self.named.data.geom_xpos['target', :2] -
-            self.named.data.geom_xpos['finger', :2])
-
-  def finger_to_target_dist(self):
-    """Returns the signed distance between the finger and target surface."""
-    return np.linalg.norm(self.finger_to_target())
-
-
-class CustomThreeLinkReacher(reacher.Reacher):
-  """Custom Reacher tasks."""
-
-  def __init__(self, target_size, random=None):
-    super().__init__(target_size, random)
-
-  def get_observation(self, physics):
-    obs = collections.OrderedDict()
-    obs['position'] = physics.position()
-    obs['to_target'] = physics.finger_to_target()
-    obs['velocity'] = physics.velocity()
-    return obs
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher_four_links.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher_four_links.xml
deleted file mode 100644
index d5aa8e5..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher_four_links.xml
+++ /dev/null
@@ -1,57 +0,0 @@
-<mujoco model="two-link planar reacher">
-  <include file="./common/skybox.xml"/>
-  <include file="./common/visual.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <option timestep="0.02">
-    <flag contact="disable"/>
-  </option>
-
-  <default>
-    <joint type="hinge" axis="0 0 1" damping="0.01"/>
-    <motor gear=".05" ctrlrange="-1 1" ctrllimited="true"/>
-  </default>
-
-  <worldbody>
-    <light name="light" pos="0 0 1"/>
-    <camera name="fixed" pos="0 0 .75" quat="1 0 0 0"/>
-    <!-- Arena -->
-    <geom name="ground" type="plane" pos="0 0 0" size=".3 .3 10" material="grid"/>
-    <geom name="wall_x" type="plane" pos="-.3 0 .02" zaxis="1 0 0"  size=".02 .3 .02" material="decoration"/>
-    <geom name="wall_y" type="plane" pos="0 -.3 .02" zaxis="0 1 0"  size=".3 .02 .02" material="decoration"/>
-    <geom name="wall_neg_x" type="plane" pos=".3 0 .02" zaxis="-1 0 0"  size=".02 .3 .02" material="decoration"/>
-    <geom name="wall_neg_y" type="plane" pos="0 .3 .02" zaxis="0 -1 0"  size=".3 .02 .02" material="decoration"/>
-
-    <!-- Arm -->
-    <geom name="root" type="cylinder" fromto="0 0 0 0 0 0.02" size=".011" material="decoration"/>
-    <body name="arm0" pos="0 0 .01">
-      <geom name="arm0" type="capsule" fromto="0 0 0 0.06 0 0" size=".01" material="self"/>
-      <joint name="shoulder0"/>
-      <body name="arm1" pos=".06 0 0">
-        <geom name="arm1" type="capsule" fromto="0 0 0 0.06 0 0" size=".01" material="self"/>
-        <joint name="shoulder1" limited="true" range="-80 80"/>
-        <body name="arm2" pos=".06 0 0">
-            <geom name="arm2" type="capsule" fromto="0 0 0 0.06 0 0" size=".01" material="self"/>
-            <joint name="shoulder2" limited="true" range="-80 80"/>
-            <body name="hand" pos=".06 0 0">
-                <geom name="hand" type="capsule" fromto="0 0 0 0.1 0 0" size=".01" material="self"/>
-                <joint name="wrist" limited="true" range="-80 80"/>
-                <body name="finger" pos=".06 0 0">
-                    <camera name="hand" pos="0 0 .2" mode="track"/>
-                    <geom name="finger" type="sphere" size=".01" material="effector"/>
-                </body>
-            </body>
-        </body>
-      </body>
-    </body>
-    <!-- Target -->
-    <geom name="target" pos="0 0 .01" material="target" type="sphere" size=".05"/>
-  </worldbody>
-
-  <actuator>
-    <motor name="shoulder0" joint="shoulder0"/>
-    <motor name="shoulder1" joint="shoulder1"/>
-    <motor name="shoulder2" joint="shoulder2"/>
-    <motor name="wrist" joint="wrist"/>
-  </actuator>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher_three_links.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher_three_links.xml
deleted file mode 100644
index f32f4bc..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/reacher_three_links.xml
+++ /dev/null
@@ -1,52 +0,0 @@
-<mujoco model="two-link planar reacher">
-  <include file="./common/skybox.xml"/>
-  <include file="./common/visual.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <option timestep="0.02">
-    <flag contact="disable"/>
-  </option>
-
-  <default>
-    <joint type="hinge" axis="0 0 1" damping="0.01"/>
-    <motor gear=".05" ctrlrange="-1 1" ctrllimited="true"/>
-  </default>
-
-  <worldbody>
-    <light name="light" pos="0 0 1"/>
-    <camera name="fixed" pos="0 0 .75" quat="1 0 0 0"/>
-    <!-- Arena -->
-    <geom name="ground" type="plane" pos="0 0 0" size=".3 .3 10" material="grid"/>
-    <geom name="wall_x" type="plane" pos="-.3 0 .02" zaxis="1 0 0"  size=".02 .3 .02" material="decoration"/>
-    <geom name="wall_y" type="plane" pos="0 -.3 .02" zaxis="0 1 0"  size=".3 .02 .02" material="decoration"/>
-    <geom name="wall_neg_x" type="plane" pos=".3 0 .02" zaxis="-1 0 0"  size=".02 .3 .02" material="decoration"/>
-    <geom name="wall_neg_y" type="plane" pos="0 .3 .02" zaxis="0 -1 0"  size=".3 .02 .02" material="decoration"/>
-
-    <!-- Arm -->
-    <geom name="root" type="cylinder" fromto="0 0 0 0 0 0.02" size=".011" material="decoration"/>
-    <body name="arm0" pos="0 0 .01">
-      <geom name="arm0" type="capsule" fromto="0 0 0 0.09 0 0" size=".01" material="self"/>
-      <joint name="shoulder0"/>
-      <body name="arm1" pos=".09 0 0">
-        <geom name="arm1" type="capsule" fromto="0 0 0 0.09 0 0" size=".01" material="self"/>
-        <joint name="shoulder1" limited="true" range="-80 80"/>
-        <body name="hand" pos=".09 0 0">
-            <geom name="hand" type="capsule" fromto="0 0 0 0.1 0 0" size=".01" material="self"/>
-            <joint name="wrist" limited="true" range="-80 80"/>
-            <body name="finger" pos=".09 0 0">
-              <camera name="hand" pos="0 0 .2" mode="track"/>
-              <geom name="finger" type="sphere" size=".01" material="effector"/>
-            </body>
-        </body>
-      </body>
-    </body>
-    <!-- Target -->
-    <geom name="target" pos="0 0 .01" material="target" type="sphere" size=".05"/>
-  </worldbody>
-
-  <actuator>
-    <motor name="shoulder0" joint="shoulder0"/>
-    <motor name="shoulder1" joint="shoulder1"/>
-    <motor name="wrist" joint="wrist"/>
-  </actuator>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/walker.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/walker.py
deleted file mode 100644
index d04c404..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/walker.py
+++ /dev/null
@@ -1,223 +0,0 @@
-import os
-
-from dm_control.rl import control
-from dm_control.suite import common
-from dm_control.suite import walker
-from dm_control.utils import rewards
-from dm_control.utils import io as resources
-
-_TASKS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tasks')
-
-_YOGA_STAND_HEIGHT = 1.0
-_YOGA_LIE_DOWN_HEIGHT = 0.08
-_YOGA_LEGS_UP_HEIGHT = 1.1
-
-
-def get_model_and_assets():
-    """Returns a tuple containing the model XML string and a dict of assets."""
-    return resources.GetResource(os.path.join(_TASKS_DIR, 'walker.xml')), common.ASSETS
-
-
-@walker.SUITE.add('custom')
-def walk_backwards(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Walk Backwards task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = BackwardsPlanarWalker(move_speed=walker._WALK_SPEED, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def run_backwards(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Run Backwards task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = BackwardsPlanarWalker(move_speed=walker._RUN_SPEED, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def arabesque(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Arabesque task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = YogaPlanarWalker(goal='arabesque', random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def lie_down(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Lie Down task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = YogaPlanarWalker(goal='lie_down', random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def legs_up(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Legs Up task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = YogaPlanarWalker(goal='legs_up', random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def headstand(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Headstand task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = YogaPlanarWalker(goal='flip', move_speed=0, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def flip(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Flip task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = YogaPlanarWalker(goal='flip', move_speed=walker._RUN_SPEED*0.75, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-@walker.SUITE.add('custom')
-def backflip(time_limit=walker._DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):
-  """Returns the Backflip task."""
-  physics = walker.Physics.from_xml_string(*get_model_and_assets())
-  task = YogaPlanarWalker(goal='flip', move_speed=-walker._RUN_SPEED*0.75, random=random)
-  environment_kwargs = environment_kwargs or {}
-  return control.Environment(
-      physics, task, time_limit=time_limit, control_timestep=walker._CONTROL_TIMESTEP,
-      **environment_kwargs)
-
-
-class BackwardsPlanarWalker(walker.PlanarWalker):
-    """Backwards PlanarWalker task."""
-    def __init__(self, move_speed, random=None):
-        super().__init__(move_speed, random)
-    
-    def get_reward(self, physics):
-        standing = rewards.tolerance(physics.torso_height(),
-                                 bounds=(walker._STAND_HEIGHT, float('inf')),
-                                 margin=walker._STAND_HEIGHT/2)
-        upright = (1 + physics.torso_upright()) / 2
-        stand_reward = (3*standing + upright) / 4
-        if self._move_speed == 0:
-            return stand_reward
-        else:
-            move_reward = rewards.tolerance(physics.horizontal_velocity(),
-                                            bounds=(-float('inf'), -self._move_speed),
-                                            margin=self._move_speed/2,
-                                            value_at_margin=0.5,
-                                            sigmoid='linear')
-            return stand_reward * (5*move_reward + 1) / 6
-
-
-class YogaPlanarWalker(walker.PlanarWalker):
-    """Yoga PlanarWalker tasks."""
-    
-    def __init__(self, goal='arabesque', move_speed=0, random=None):
-        super().__init__(0, random)
-        self._goal = goal
-        self._move_speed = move_speed
-    
-    def _arabesque_reward(self, physics):
-        standing = rewards.tolerance(physics.torso_height(),
-                                bounds=(_YOGA_STAND_HEIGHT, float('inf')),
-                                margin=_YOGA_STAND_HEIGHT/2)
-        left_foot_height = physics.named.data.xpos['left_foot', 'z']
-        right_foot_height = physics.named.data.xpos['right_foot', 'z']
-        left_foot_down = rewards.tolerance(left_foot_height,
-                                bounds=(-float('inf'), _YOGA_LIE_DOWN_HEIGHT),
-                                margin=_YOGA_STAND_HEIGHT/2)
-        right_foot_up = rewards.tolerance(right_foot_height,
-                                bounds=(_YOGA_STAND_HEIGHT, float('inf')),
-                                margin=_YOGA_STAND_HEIGHT/2)
-        upright = (1 - physics.torso_upright()) / 2
-        arabesque_reward = (3*standing + left_foot_down + right_foot_up + upright) / 6
-        return arabesque_reward
-    
-    def _lie_down_reward(self, physics):
-        torso_down = rewards.tolerance(physics.torso_height(),
-                                bounds=(-float('inf'), _YOGA_LIE_DOWN_HEIGHT),
-                                margin=_YOGA_LIE_DOWN_HEIGHT/2)
-        thigh_height = (physics.named.data.xpos['left_thigh', 'z'] + physics.named.data.xpos['right_thigh', 'z']) / 2
-        thigh_down = rewards.tolerance(thigh_height,
-                                bounds=(-float('inf'), _YOGA_LIE_DOWN_HEIGHT),
-                                margin=_YOGA_LIE_DOWN_HEIGHT/2)
-        feet_height = (physics.named.data.xpos['left_foot', 'z'] + physics.named.data.xpos['right_foot', 'z']) / 2
-        feet_down = rewards.tolerance(feet_height,
-                                bounds=(-float('inf'), _YOGA_LIE_DOWN_HEIGHT),
-                                margin=_YOGA_LIE_DOWN_HEIGHT/2)
-        upright = (1 - physics.torso_upright()) / 2
-        lie_down_reward = (3*torso_down + thigh_down + upright) / 5
-        return lie_down_reward
-    
-    def _legs_up_reward(self, physics):
-        torso_down = rewards.tolerance(physics.torso_height(),
-                                bounds=(-float('inf'), _YOGA_LIE_DOWN_HEIGHT),
-                                margin=_YOGA_LIE_DOWN_HEIGHT/2)
-        thigh_height = (physics.named.data.xpos['left_thigh', 'z'] + physics.named.data.xpos['right_thigh', 'z']) / 2
-        thigh_down = rewards.tolerance(thigh_height,
-                                bounds=(-float('inf'), _YOGA_LIE_DOWN_HEIGHT),
-                                margin=_YOGA_LIE_DOWN_HEIGHT/2)
-        feet_height = (physics.named.data.xpos['left_foot', 'z'] + physics.named.data.xpos['right_foot', 'z']) / 2
-        legs_up = rewards.tolerance(feet_height,
-                                bounds=(_YOGA_LEGS_UP_HEIGHT, float('inf')),
-                                margin=_YOGA_LEGS_UP_HEIGHT/2)
-        upright = (1 - physics.torso_upright()) / 2
-        legs_up_reward = (3*torso_down + 2*legs_up + thigh_down + upright) / 7
-        return legs_up_reward
-    
-    def _flip_reward(self, physics):
-        thigh_height = (physics.named.data.xpos['left_thigh', 'z'] + physics.named.data.xpos['right_thigh', 'z']) / 2
-        thigh_up = rewards.tolerance(thigh_height,
-                                bounds=(_YOGA_STAND_HEIGHT, float('inf')),
-                                margin=_YOGA_STAND_HEIGHT/2)
-        feet_height = (physics.named.data.xpos['left_foot', 'z'] + physics.named.data.xpos['right_foot', 'z']) / 2
-        legs_up = rewards.tolerance(feet_height,
-                                bounds=(_YOGA_LEGS_UP_HEIGHT, float('inf')),
-                                margin=_YOGA_LEGS_UP_HEIGHT/2)
-        upside_down_reward = (3*legs_up + 2*thigh_up) / 5
-        if self._move_speed == 0:
-            return upside_down_reward
-        move_reward = rewards.tolerance(physics.horizontal_velocity(),
-                                    bounds=(self._move_speed, float('inf')) if self._move_speed > 0 else (-float('inf'), self._move_speed),
-                                    margin=abs(self._move_speed)/2,
-                                    value_at_margin=0.5,
-                                    sigmoid='linear')
-        return upside_down_reward * (5*move_reward + 1) / 6
-    
-    def get_reward(self, physics):
-        if self._goal == 'arabesque':
-            return self._arabesque_reward(physics)
-        elif self._goal == 'lie_down':
-            return self._lie_down_reward(physics)
-        elif self._goal == 'legs_up':
-            return self._legs_up_reward(physics)
-        elif self._goal == 'flip':
-            return self._flip_reward(physics)
-        else:
-            raise NotImplementedError(f'Goal {self._goal} is not implemented.')
-
-
-if __name__ == '__main__':
-    env = legs_up()
-    obs = env.reset()
-    import numpy as np
-    next_obs, reward, done, info = env.step(np.zeros(6))
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/walker.xml b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/walker.xml
deleted file mode 100644
index 1d17637..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/tasks/walker.xml
+++ /dev/null
@@ -1,70 +0,0 @@
-<mujoco model="planar walker">
-  <include file="./common/visual.xml"/>
-  <include file="./common/skybox.xml"/>
-  <include file="./common/materials.xml"/>
-
-  <option timestep="0.0025"/>
-
-  <statistic extent="2" center="0 0 1"/>
-
-  <default>
-    <joint damping=".1" armature="0.01" limited="true" solimplimit="0 .99 .01"/>
-    <geom contype="1" conaffinity="0" friction=".7 .1 .1"/>
-    <motor ctrlrange="-1 1" ctrllimited="true"/>
-    <site size="0.01"/>
-    <default class="walker">
-      <geom material="self" type="capsule"/>
-      <joint axis="0 -1 0"/>
-    </default>
-  </default>
-
-  <worldbody>
-    <geom name="floor" type="plane" conaffinity="1" pos="248 0 0" size="500 .8 .2" material="grid" zaxis="0 0 1"/>
-    <body name="torso" pos="0 0 1.3" childclass="walker">
-      <light name="light" pos="0 0 2" mode="trackcom"/>
-      <camera name="side" pos="0 -2 .7" euler="60 0 0" mode="trackcom"/>
-      <camera name="back" pos="-2 0 .5" xyaxes="0 -1 0 1 0 3" mode="trackcom"/>
-      <joint name="rootz" axis="0 0 1" type="slide" limited="false" armature="0" damping="0"/>
-      <joint name="rootx" axis="1 0 0" type="slide" limited="false" armature="0" damping="0"/>
-      <joint name="rooty" axis="0 1 0" type="hinge" limited="false" armature="0" damping="0"/>
-      <geom name="torso" size="0.07 0.3"/>
-      <body name="right_thigh" pos="0 -.05 -0.3">
-        <joint name="right_hip" range="-20 100"/>
-        <geom name="right_thigh" pos="0 0 -0.225" size="0.05 0.225"/>
-        <body name="right_leg" pos="0 0 -0.7">
-          <joint name="right_knee" pos="0 0 0.25" range="-150 0"/>
-          <geom name="right_leg" size="0.04 0.25"/>
-          <body name="right_foot" pos="0.06 0 -0.25">
-            <joint name="right_ankle" pos="-0.06 0 0" range="-45 45"/>
-            <geom name="right_foot" zaxis="1 0 0" size="0.05 0.1"/>
-          </body>
-        </body>
-      </body>
-      <body name="left_thigh" pos="0 .05 -0.3" >
-        <joint name="left_hip" range="-20 100"/>
-        <geom name="left_thigh" pos="0 0 -0.225" size="0.05 0.225"/>
-        <body name="left_leg" pos="0 0 -0.7">
-          <joint name="left_knee" pos="0 0 0.25" range="-150 0"/>
-          <geom name="left_leg" size="0.04 0.25"/>
-          <body name="left_foot" pos="0.06 0 -0.25">
-            <joint name="left_ankle" pos="-0.06 0 0" range="-45 45"/>
-            <geom name="left_foot" zaxis="1 0 0" size="0.05 0.1"/>
-          </body>
-        </body>
-      </body>
-    </body>
-  </worldbody>
-
-  <sensor>
-    <subtreelinvel name="torso_subtreelinvel" body="torso"/>
-  </sensor>
-
-  <actuator>
-    <motor name="right_hip" joint="right_hip" gear="100"/>
-    <motor name="right_knee" joint="right_knee" gear="50"/>
-    <motor name="right_ankle" joint="right_ankle" gear="20"/>
-    <motor name="left_hip" joint="left_hip" gear="100"/>
-    <motor name="left_knee" joint="left_knee" gear="50"/>
-    <motor name="left_ankle" joint="left_ankle" gear="20"/>
-  </actuator>
-</mujoco>
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/__init__.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/multitask.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/multitask.py
deleted file mode 100644
index 529295a..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/multitask.py
+++ /dev/null
@@ -1,57 +0,0 @@
-import gymnasium as gym
-import numpy as np
-import torch
-
-
-class MultitaskWrapper(gym.Wrapper):
-	"""
-	Wrapper for multi-task environments.
-	"""
-
-	def __init__(self, cfg, envs):
-		super().__init__(envs[0])
-		self.cfg = cfg
-		self.envs = envs
-		self._task = cfg.tasks[0]
-		self._task_idx = 0
-		self._obs_dims = [env.observation_space.shape[0] for env in self.envs]
-		self._action_dims = [env.action_space.shape[0] for env in self.envs]
-		self._episode_lengths = [env.max_episode_steps for env in self.envs]
-		self._obs_shape = (max(self._obs_dims),)
-		self._action_dim = max(self._action_dims)
-		self.observation_space = gym.spaces.Box(
-			low=-np.inf, high=np.inf, shape=self._obs_shape, dtype=np.float32
-		)
-		self.action_space = gym.spaces.Box(
-			low=-1, high=1, shape=(self._action_dim,), dtype=np.float32
-		)
-	
-	@property
-	def task(self):
-		return self._task
-	
-	@property
-	def task_idx(self):
-		return self._task_idx
-	
-	@property
-	def _env(self):
-		return self.envs[self.task_idx]
-
-	def rand_act(self):
-		return torch.from_numpy(self.action_space.sample().astype(np.float32))
-
-	def _pad_obs(self, obs):
-		if obs.shape != self._obs_shape:
-			obs = torch.cat((obs, torch.zeros(self._obs_shape[0]-obs.shape[0], dtype=obs.dtype, device=obs.device)))
-		return obs
-	
-	def reset(self, task_idx=-1):
-		self._task_idx = task_idx
-		self._task = self.cfg.tasks[task_idx]
-		self.env = self._env
-		return self._pad_obs(self.env.reset())
-
-	def step(self, action):
-		obs, reward, done, info = self.env.step(action[:self.env.action_space.shape[0]])
-		return self._pad_obs(obs), reward, done, info
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/tensor.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/tensor.py
deleted file mode 100644
index 4a6819a..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/tensor.py
+++ /dev/null
@@ -1,42 +0,0 @@
-from collections import defaultdict
-
-import gymnasium as gym
-import numpy as np
-import torch
-
-
-class TensorWrapper(gym.Wrapper):
-	"""
-	Wrapper for converting numpy arrays to torch tensors.
-	"""
-
-	def __init__(self, env):
-		super().__init__(env)
-	
-	def rand_act(self):
-		return torch.from_numpy(self.action_space.sample().astype(np.float32))
-
-	def _try_f32_tensor(self, x):
-		if isinstance(x, np.ndarray):
-			x = torch.from_numpy(x)
-			if x.dtype == torch.float64:
-				x = x.float()
-		return x
-
-	def _obs_to_tensor(self, obs):
-		if isinstance(obs, dict):
-			for k in obs.keys():
-				obs[k] = self._try_f32_tensor(obs[k])
-		else:
-			obs = self._try_f32_tensor(obs)
-		return obs
-
-	def reset(self, task_idx=None):
-		return self._obs_to_tensor(self.env.reset())
-
-	def step(self, action):
-		obs, reward, done, info = self.env.step(action.numpy())
-		info = defaultdict(float, info)
-		info['success'] = float(info['success'])
-		info['terminated'] = torch.tensor(float(info['terminated']))
-		return self._obs_to_tensor(obs), torch.tensor(reward, dtype=torch.float32), done, info
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/timeout.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/timeout.py
deleted file mode 100644
index cc2081c..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/envs/wrappers/timeout.py
+++ /dev/null
@@ -1,25 +0,0 @@
-import gymnasium as gym
-
-
-class Timeout(gym.Wrapper):
-	"""
-	Wrapper for enforcing a time limit on the environment.
-	"""
-
-	def __init__(self, env, max_episode_steps):
-		super().__init__(env)
-		self._max_episode_steps = max_episode_steps
-	
-	@property
-	def max_episode_steps(self):
-		return self._max_episode_steps
-
-	def reset(self, **kwargs):
-		self._t = 0
-		return self.env.reset(**kwargs)
-
-	def step(self, action):
-		obs, reward, done, info = self.env.step(action)
-		self._t += 1
-		done = done or self._t >= self.max_episode_steps
-		return obs, reward, done, info
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/evaluate.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/evaluate.py
deleted file mode 100755
index 9c788a2..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/evaluate.py
+++ /dev/null
@@ -1,103 +0,0 @@
-import os
-os.environ['MUJOCO_GL'] = os.getenv("MUJOCO_GL", 'egl')
-import warnings
-warnings.filterwarnings('ignore')
-
-import hydra
-import imageio
-import numpy as np
-import torch
-from termcolor import colored
-
-from common.parser import parse_cfg
-from common.seed import set_seed
-from envs import make_env
-from tdmpc2 import TDMPC2
-
-torch.backends.cudnn.benchmark = True
-
-
-@hydra.main(config_name='config', config_path='.')
-def evaluate(cfg: dict):
-	"""
-	Script for evaluating a single-task / multi-task TD-MPC2 checkpoint.
-
-	Most relevant args:
-		`task`: task name (or mt30/mt80 for multi-task evaluation)
-		`model_size`: model size, must be one of `[1, 5, 19, 48, 317]` (default: 5)
-		`checkpoint`: path to model checkpoint to load
-		`eval_episodes`: number of episodes to evaluate on per task (default: 10)
-		`save_video`: whether to save a video of the evaluation (default: True)
-		`seed`: random seed (default: 1)
-	
-	See config.yaml for a full list of args.
-
-	Example usage:
-	````
-		$ python evaluate.py task=mt80 model_size=48 checkpoint=/path/to/mt80-48M.pt
-		$ python evaluate.py task=mt30 model_size=317 checkpoint=/path/to/mt30-317M.pt
-		$ python evaluate.py task=dog-run checkpoint=/path/to/dog-1.pt save_video=true
-	```
-	"""
-	assert torch.cuda.is_available()
-	assert cfg.eval_episodes > 0, 'Must evaluate at least 1 episode.'
-	cfg = parse_cfg(cfg)
-	set_seed(cfg.seed)
-	print(colored(f'Task: {cfg.task}', 'blue', attrs=['bold']))
-	print(colored(f'Model size: {cfg.get("model_size", "default")}', 'blue', attrs=['bold']))
-	print(colored(f'Checkpoint: {cfg.checkpoint}', 'blue', attrs=['bold']))
-	if not cfg.multitask and ('mt80' in cfg.checkpoint or 'mt30' in cfg.checkpoint):
-		print(colored('Warning: single-task evaluation of multi-task models is not currently supported.', 'red', attrs=['bold']))
-		print(colored('To evaluate a multi-task model, use task=mt80 or task=mt30.', 'red', attrs=['bold']))
-
-	# Make environment
-	env = make_env(cfg)
-
-	# Load agent
-	agent = TDMPC2(cfg)
-	assert os.path.exists(cfg.checkpoint), f'Checkpoint {cfg.checkpoint} not found! Must be a valid filepath.'
-	agent.load(cfg.checkpoint)
-	
-	# Evaluate
-	if cfg.multitask:
-		print(colored(f'Evaluating agent on {len(cfg.tasks)} tasks:', 'yellow', attrs=['bold']))
-	else:
-		print(colored(f'Evaluating agent on {cfg.task}:', 'yellow', attrs=['bold']))
-	if cfg.save_video:
-		video_dir = os.path.join(cfg.work_dir, 'videos')
-		os.makedirs(video_dir, exist_ok=True)
-	scores = []
-	tasks = cfg.tasks if cfg.multitask else [cfg.task]
-	for task_idx, task in enumerate(tasks):
-		if not cfg.multitask:
-			task_idx = None
-		ep_rewards, ep_successes = [], []
-		for i in range(cfg.eval_episodes):
-			obs, done, ep_reward, t = env.reset(task_idx=task_idx), False, 0, 0
-			if cfg.save_video:
-				frames = [env.render()]
-			while not done:
-				action = agent.act(obs, t0=t==0, task=task_idx)
-				obs, reward, done, info = env.step(action)
-				ep_reward += reward
-				t += 1
-				if cfg.save_video:
-					frames.append(env.render())
-			ep_rewards.append(ep_reward)
-			ep_successes.append(info['success'])
-			if cfg.save_video:
-				imageio.mimsave(
-					os.path.join(video_dir, f'{task}-{i}.mp4'), frames, fps=15)
-		ep_rewards = np.mean(ep_rewards)
-		ep_successes = np.mean(ep_successes)
-		if cfg.multitask:
-			scores.append(ep_successes*100 if task.startswith('mw-') else ep_rewards/10)
-		print(colored(f'  {task:<22}' \
-			f'\tR: {ep_rewards:.01f}  ' \
-			f'\tS: {ep_successes:.02f}', 'yellow'))
-	if cfg.multitask:
-		print(colored(f'Normalized score: {np.mean(scores):.02f}', 'yellow', attrs=['bold']))
-
-
-if __name__ == '__main__':
-	evaluate()
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/tdmpc2.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/tdmpc2.py
deleted file mode 100755
index e37945b..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/tdmpc2.py
+++ /dev/null
@@ -1,458 +0,0 @@
-import torch
-import torch.nn.functional as F
-from common import math
-from common.layers import api_model_conversion
-from common.scale import RunningScale
-from common.world_model import WorldModel
-
-
-class TDMPC2(torch.nn.Module):
-    """
-    TD-MPC2 agent. Implements training + inference.
-    Can be used for both single-task and multi-task experiments,
-    and supports both state and pixel observations.
-    """
-
-    def __init__(self, cfg):
-        super().__init__()
-        self.cfg = cfg
-        dev = getattr(cfg, "device", "cuda:0")
-        self.device = torch.device(dev)
-        self.model = WorldModel(cfg).to(self.device)
-        self.optim = torch.optim.Adam(
-            [
-                {
-                    "params": self.model._encoder.parameters(),
-                    "lr": self.cfg.lr * self.cfg.enc_lr_scale,
-                },
-                {"params": self.model._dynamics.parameters()},
-                {"params": self.model._reward.parameters()},
-                {
-                    "params": self.model._termination.parameters()
-                    if self.cfg.episodic
-                    else []
-                },
-                {"params": self.model._Qs.parameters()},
-                {
-                    "params": self.model._task_emb.parameters()
-                    if self.cfg.multitask
-                    else []
-                },
-            ],
-            lr=self.cfg.lr,
-            capturable=True,
-        )
-        self.pi_optim = torch.optim.Adam(
-            self.model._pi.parameters(), lr=self.cfg.lr, eps=1e-5, capturable=True
-        )
-        self.model.eval()
-        self.scale = RunningScale(cfg, self.device)
-        self.cfg.iterations += 2 * int(
-            cfg.action_dim >= 20
-        )  # Heuristic for large action spaces
-        self.discount = (
-            torch.tensor(
-                [self._get_discount(ep_len) for ep_len in cfg.episode_lengths],
-                device=self.device,
-            )
-            if self.cfg.multitask
-            else self._get_discount(cfg.episode_length)
-        )
-        print("Episode length:", cfg.episode_length)
-        print("Discount factor:", self.discount)
-        self._prev_mean = torch.nn.Buffer(
-            torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)
-        )
-        if cfg.compile:
-            print("Compiling update function with torch.compile...")
-            self._update = torch.compile(self._update, mode="reduce-overhead")
-
-    @property
-    def plan(self):
-        _plan_val = getattr(self, "_plan_val", None)
-        if _plan_val is not None:
-            return _plan_val
-        if self.cfg.compile:
-            plan = torch.compile(self._plan, mode="reduce-overhead")
-        else:
-            plan = self._plan
-        self._plan_val = plan
-        return self._plan_val
-
-    def _get_discount(self, episode_length):
-        """
-        Returns discount factor for a given episode length.
-        Simple heuristic that scales discount linearly with episode length.
-        Default values should work well for most tasks, but can be changed as needed.
-
-        Args:
-                episode_length (int): Length of the episode. Assumes episodes are of fixed length.
-
-        Returns:
-                float: Discount factor for the task.
-        """
-        frac = episode_length / self.cfg.discount_denom
-        return min(
-            max((frac - 1) / (frac), self.cfg.discount_min), self.cfg.discount_max
-        )
-
-    def save(self, fp):
-        """
-        Save state dict of the agent to filepath.
-
-        Args:
-                fp (str): Filepath to save state dict to.
-        """
-        torch.save({"model": self.model.state_dict()}, fp)
-
-    def load(self, fp):
-        """
-        Load a saved state dict from filepath (or dictionary) into current agent.
-
-        Args:
-                fp (str or dict): Filepath or state dict to load.
-        """
-        if isinstance(fp, dict):
-            state_dict = fp
-        else:
-            state_dict = torch.load(fp, map_location=self.device, weights_only=False)
-        state_dict = state_dict["model"] if "model" in state_dict else state_dict
-        state_dict = api_model_conversion(self.model.state_dict(), state_dict)
-        self.model.load_state_dict(state_dict)
-        return
-
-    @torch.no_grad()
-    def act(self, obs, t0=False, eval_mode=False, task=None):
-        """
-        Select an action by planning in the latent space of the world model.
-
-        Args:
-                obs (torch.Tensor): Observation from the environment.
-                t0 (bool): Whether this is the first observation in the episode.
-                eval_mode (bool): Whether to use the mean of the action distribution.
-                task (int): Task index (only used for multi-task experiments).
-
-        Returns:
-                torch.Tensor: Action to take in the environment.
-        """
-        obs = obs.to(self.device, non_blocking=True).unsqueeze(0)
-        if task is not None:
-            task = torch.tensor([task], device=self.device)
-        if self.cfg.mpc:
-            return self.plan(obs, t0=t0, eval_mode=eval_mode, task=task).cpu()
-        z = self.model.encode(obs, task)
-        action, info = self.model.pi(z, task)
-        if eval_mode:
-            action = info["mean"]
-        return action[0].cpu()
-
-    @torch.no_grad()
-    def _estimate_value(self, z, actions, task):
-        """Estimate value of a trajectory starting at latent state z and executing given actions."""
-        G, discount = 0, 1
-        termination = torch.zeros(
-            self.cfg.num_samples, 1, dtype=torch.float32, device=z.device
-        )
-        for t in range(self.cfg.horizon):
-            reward = math.two_hot_inv(self.model.reward(z, actions[t], task), self.cfg)
-            z = self.model.next(z, actions[t], task)
-            G = G + discount * (1 - termination) * reward
-            discount_update = (
-                self.discount[torch.tensor(task)]
-                if self.cfg.multitask
-                else self.discount
-            )
-            discount = discount * discount_update
-            if self.cfg.episodic:
-                termination = torch.clip(
-                    termination + (self.model.termination(z, task) > 0.5).float(),
-                    max=1.0,
-                )
-        action, _ = self.model.pi(z, task)
-        return G + discount * (1 - termination) * self.model.Q(
-            z, action, task, return_type="avg"
-        )
-
-    @torch.no_grad()
-    def _plan(self, obs, t0=False, eval_mode=False, task=None):
-        """
-        Plan a sequence of actions using the learned world model.
-
-        Args:
-                z (torch.Tensor): Latent state from which to plan.
-                t0 (bool): Whether this is the first observation in the episode.
-                eval_mode (bool): Whether to use the mean of the action distribution.
-                task (Torch.Tensor): Task index (only used for multi-task experiments).
-
-        Returns:
-                torch.Tensor: Action to take in the environment.
-        """
-        # Sample policy trajectories
-        z = self.model.encode(obs, task)
-        if self.cfg.num_pi_trajs > 0:
-            pi_actions = torch.empty(
-                self.cfg.horizon,
-                self.cfg.num_pi_trajs,
-                self.cfg.action_dim,
-                device=self.device,
-            )
-            _z = z.repeat(self.cfg.num_pi_trajs, 1)
-            for t in range(self.cfg.horizon - 1):
-                pi_actions[t], _ = self.model.pi(_z, task)
-                _z = self.model.next(_z, pi_actions[t], task)
-            pi_actions[-1], _ = self.model.pi(_z, task)
-
-        # Initialize state and parameters
-        z = z.repeat(self.cfg.num_samples, 1)
-        mean = torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)
-        std = torch.full(
-            (self.cfg.horizon, self.cfg.action_dim),
-            self.cfg.max_std,
-            dtype=torch.float,
-            device=self.device,
-        )
-        if not t0:
-            mean[:-1] = self._prev_mean[1:]
-        actions = torch.empty(
-            self.cfg.horizon,
-            self.cfg.num_samples,
-            self.cfg.action_dim,
-            device=self.device,
-        )
-        if self.cfg.num_pi_trajs > 0:
-            actions[:, : self.cfg.num_pi_trajs] = pi_actions
-
-        # Iterate MPPI
-        for _ in range(self.cfg.iterations):
-            # Sample actions
-            r = torch.randn(
-                self.cfg.horizon,
-                self.cfg.num_samples - self.cfg.num_pi_trajs,
-                self.cfg.action_dim,
-                device=std.device,
-            )
-            actions_sample = mean.unsqueeze(1) + std.unsqueeze(1) * r
-            actions_sample = actions_sample.clamp(-1, 1)
-            actions[:, self.cfg.num_pi_trajs :] = actions_sample
-            if self.cfg.multitask:
-                actions = actions * self.model._action_masks[task]
-
-            # Compute elite actions
-            value = self._estimate_value(z, actions, task).nan_to_num(0)
-            elite_idxs = torch.topk(
-                value.squeeze(1), self.cfg.num_elites, dim=0
-            ).indices
-            elite_value, elite_actions = value[elite_idxs], actions[:, elite_idxs]
-
-            # Update parameters
-            max_value = elite_value.max(0).values
-            score = torch.exp(self.cfg.temperature * (elite_value - max_value))
-            score = score / score.sum(0)
-            mean = (score.unsqueeze(0) * elite_actions).sum(dim=1) / (
-                score.sum(0) + 1e-9
-            )
-            std = (
-                (score.unsqueeze(0) * (elite_actions - mean.unsqueeze(1)) ** 2).sum(
-                    dim=1
-                )
-                / (score.sum(0) + 1e-9)
-            ).sqrt()
-            std = std.clamp(self.cfg.min_std, self.cfg.max_std)
-            if self.cfg.multitask:
-                mean = mean * self.model._action_masks[task]
-                std = std * self.model._action_masks[task]
-
-        # Select action
-        rand_idx = math.gumbel_softmax_sample(score.squeeze(1))
-        actions = torch.index_select(elite_actions, 1, rand_idx).squeeze(1)
-        a, std = actions[0], std[0]
-        if not eval_mode:
-            a = a + std * torch.randn(self.cfg.action_dim, device=std.device)
-        self._prev_mean.copy_(mean)
-        return a.clamp(-1, 1)
-
-    def update_pi(self, zs, task):
-        """
-        Update policy using a sequence of latent states.
-
-        Args:
-                zs (torch.Tensor): Sequence of latent states.
-                task (torch.Tensor): Task index (only used for multi-task experiments).
-
-        Returns:
-                float: Loss of the policy update.
-        """
-        action, info = self.model.pi(zs, task)
-        qs = self.model.Q(zs, action, task, return_type="avg", detach=True)
-        self.scale.update(qs[0])
-        qs = self.scale(qs)
-
-        # Loss is a weighted sum of Q-values
-        rho = torch.pow(self.cfg.rho, torch.arange(len(qs), device=self.device))
-        pi_loss = (
-            -(self.cfg.entropy_coef * info["scaled_entropy"] + qs).mean(dim=(1, 2))
-            * rho
-        ).mean()
-        pi_loss.backward()
-        pi_grad_norm = torch.nn.utils.clip_grad_norm_(
-            self.model._pi.parameters(), self.cfg.grad_clip_norm
-        )
-        self.pi_optim.step()
-        self.pi_optim.zero_grad(set_to_none=True)
-
-        return {
-            "pi_loss": pi_loss,
-            "pi_grad_norm": pi_grad_norm,
-            "pi_entropy": info["entropy"],
-            "pi_scaled_entropy": info["scaled_entropy"],
-            "pi_scale": self.scale.value,
-        }
-
-    @torch.no_grad()
-    def _td_target(self, next_z, reward, terminated, task):
-        """
-        Compute the TD-target from a reward and the observation at the following time step.
-
-        Args:
-                next_z (torch.Tensor): Latent state at the following time step.
-                reward (torch.Tensor): Reward at the current time step.
-                terminated (torch.Tensor): Termination signal at the current time step.
-                task (torch.Tensor): Task index (only used for multi-task experiments).
-
-        Returns:
-                torch.Tensor: TD-target.
-        """
-        action, _ = self.model.pi(next_z, task)
-        discount = (
-            self.discount[task].unsqueeze(-1) if self.cfg.multitask else self.discount
-        )
-        return reward + discount * (1 - terminated) * self.model.Q(
-            next_z, action, task, return_type="min", target=True
-        )
-
-    def _update(self, obs, action, reward, terminated, task=None):
-        # Compute targets
-        with torch.no_grad():
-            next_z = self.model.encode(obs[1:], task)
-            td_targets = self._td_target(next_z, reward, terminated, task)
-
-        # Prepare for update
-        self.model.train()
-
-        # Latent rollout
-        zs = torch.empty(
-            self.cfg.horizon + 1,
-            self.cfg.batch_size,
-            self.cfg.latent_dim,
-            device=self.device,
-        )
-        z = self.model.encode(obs[0], task)
-        zs[0] = z
-        consistency_loss = 0
-        for t, (_action, _next_z) in enumerate(zip(action.unbind(0), next_z.unbind(0))):
-            z = self.model.next(z, _action, task)
-            consistency_loss = (
-                consistency_loss + F.mse_loss(z, _next_z) * self.cfg.rho**t
-            )
-            zs[t + 1] = z
-
-        # Predictions
-        _zs = zs[:-1]
-        qs = self.model.Q(_zs, action, task, return_type="all")
-        reward_preds = self.model.reward(_zs, action, task)
-        if self.cfg.episodic:
-            termination_pred = self.model.termination(zs[1:], task, unnormalized=True)
-
-        # Compute losses
-        reward_loss, value_loss = 0, 0
-        for t, (rew_pred_unbind, rew_unbind, td_targets_unbind, qs_unbind) in enumerate(
-            zip(
-                reward_preds.unbind(0),
-                reward.unbind(0),
-                td_targets.unbind(0),
-                qs.unbind(1),
-            )
-        ):
-            reward_loss = (
-                reward_loss
-                + math.soft_ce(rew_pred_unbind, rew_unbind, self.cfg).mean()
-                * self.cfg.rho**t
-            )
-            for _, qs_unbind_unbind in enumerate(qs_unbind.unbind(0)):
-                value_loss = (
-                    value_loss
-                    + math.soft_ce(qs_unbind_unbind, td_targets_unbind, self.cfg).mean()
-                    * self.cfg.rho**t
-                )
-
-        consistency_loss = consistency_loss / self.cfg.horizon
-        reward_loss = reward_loss / self.cfg.horizon
-        if self.cfg.episodic:
-            termination_loss = F.binary_cross_entropy_with_logits(
-                termination_pred, terminated
-            )
-        else:
-            termination_loss = 0.0
-        value_loss = value_loss / (self.cfg.horizon * self.cfg.num_q)
-        total_loss = (
-            self.cfg.consistency_coef * consistency_loss
-            + self.cfg.reward_coef * reward_loss
-            + self.cfg.termination_coef * termination_loss
-            + self.cfg.value_coef * value_loss
-        )
-
-        # Update model
-        total_loss.backward()
-        grad_norm = torch.nn.utils.clip_grad_norm_(
-            self.model.parameters(), self.cfg.grad_clip_norm
-        )
-        self.optim.step()
-        self.optim.zero_grad(set_to_none=True)
-
-        # Update policy
-        pi_info = self.update_pi(zs.detach(), task)
-
-        # Update target Q-functions
-        self.model.soft_update_target_Q()
-
-        # Return training statistics
-        self.model.eval()
-        info = {
-            "consistency_loss": consistency_loss,
-            "reward_loss": reward_loss,
-            "value_loss": value_loss,
-            "termination_loss": termination_loss,
-            "total_loss": total_loss,
-            "grad_norm": grad_norm,
-        }
-        if self.cfg.episodic:
-            info.update(
-                math.termination_statistics(
-                    torch.sigmoid(termination_pred[-1]), terminated[-1]
-                )
-            )
-        info.update(pi_info)
-        detached = {}
-        for key, value in info.items():
-            if isinstance(value, torch.Tensor):
-                detached[key] = value.detach()
-            else:
-                detached[key] = value
-        return detached
-
-    def update(self, buffer):
-        """
-        Main update function. Corresponds to one iteration of model learning.
-
-        Args:
-                buffer (common.buffer.Buffer): Replay buffer.
-
-        Returns:
-                dict: Dictionary of training statistics.
-        """
-        obs, action, reward, terminated, task = buffer.sample()
-        kwargs = {}
-        if task is not None:
-            kwargs["task"] = task
-        torch.compiler.cudagraph_mark_step_begin()
-        return self._update(obs, action, reward, terminated, **kwargs)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/train.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/train.py
deleted file mode 100755
index 5676349..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/train.py
+++ /dev/null
@@ -1,65 +0,0 @@
-import os
-os.environ['MUJOCO_GL'] = os.getenv("MUJOCO_GL", 'egl')
-os.environ['LAZY_LEGACY_OP'] = '0'
-os.environ['TORCHDYNAMO_INLINE_INBUILT_NN_MODULES'] = "1"
-os.environ['TORCH_LOGS'] = "+recompiles"
-import warnings
-warnings.filterwarnings('ignore')
-import torch
-
-import hydra
-from termcolor import colored
-
-from common.parser import parse_cfg
-from common.seed import set_seed
-from common.buffer import Buffer
-from envs import make_env
-from tdmpc2 import TDMPC2
-from trainer.offline_trainer import OfflineTrainer
-from trainer.online_trainer import OnlineTrainer
-from common.logger import Logger
-
-torch.backends.cudnn.benchmark = True
-torch.set_float32_matmul_precision('high')
-
-
-@hydra.main(config_name='config', config_path='.')
-def train(cfg: dict):
-	"""
-	Script for training single-task / multi-task TD-MPC2 agents.
-
-	Most relevant args:
-		`task`: task name (or mt30/mt80 for multi-task training)
-		`model_size`: model size, must be one of `[1, 5, 19, 48, 317]` (default: 5)
-		`steps`: number of training/environment steps (default: 10M)
-		`seed`: random seed (default: 1)
-
-	See config.yaml for a full list of args.
-
-	Example usage:
-	```
-		$ python train.py task=mt80 model_size=48
-		$ python train.py task=mt30 model_size=317
-		$ python train.py task=dog-run steps=7000000
-	```
-	"""
-	assert torch.cuda.is_available()
-	assert cfg.steps > 0, 'Must train for at least 1 step.'
-	cfg = parse_cfg(cfg)
-	set_seed(cfg.seed)
-	print(colored('Work dir:', 'yellow', attrs=['bold']), cfg.work_dir)
-
-	trainer_cls = OfflineTrainer if cfg.multitask else OnlineTrainer
-	trainer = trainer_cls(
-		cfg=cfg,
-		env=make_env(cfg),
-		agent=TDMPC2(cfg),
-		buffer=Buffer(cfg),
-		logger=Logger(cfg),
-	)
-	trainer.train()
-	print('\nTraining completed successfully')
-
-
-if __name__ == '__main__':
-	train()
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/__init__.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/base.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/base.py
deleted file mode 100755
index 6d14783..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/base.py
+++ /dev/null
@@ -1,18 +0,0 @@
-class Trainer:
-	"""Base trainer class for TD-MPC2."""
-
-	def __init__(self, cfg, env, agent, buffer, logger):
-		self.cfg = cfg
-		self.env = env
-		self.agent = agent
-		self.buffer = buffer
-		self.logger = logger
-		print('Architecture:', self.agent.model)
-
-	def eval(self):
-		"""Evaluate a TD-MPC2 agent."""
-		raise NotImplementedError
-
-	def train(self):
-		"""Train a TD-MPC2 agent."""
-		raise NotImplementedError
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/offline_trainer.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/offline_trainer.py
deleted file mode 100755
index a64761b..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/offline_trainer.py
+++ /dev/null
@@ -1,94 +0,0 @@
-import os
-from copy import deepcopy
-from time import time
-from pathlib import Path
-from glob import glob
-
-import numpy as np
-import torch
-from tqdm import tqdm
-
-from common.buffer import Buffer
-from trainer.base import Trainer
-
-
-class OfflineTrainer(Trainer):
-	"""Trainer class for multi-task offline TD-MPC2 training."""
-
-	def __init__(self, *args, **kwargs):
-		super().__init__(*args, **kwargs)
-		self._start_time = time()
-	
-	def eval(self):
-		"""Evaluate a TD-MPC2 agent."""
-		results = dict()
-		for task_idx in tqdm(range(len(self.cfg.tasks)), desc='Evaluating'):
-			ep_rewards, ep_successes = [], []
-			for _ in range(self.cfg.eval_episodes):
-				obs, done, ep_reward, t = self.env.reset(task_idx), False, 0, 0
-				while not done:
-					torch.compiler.cudagraph_mark_step_begin()
-					action = self.agent.act(obs, t0=t==0, eval_mode=True, task=task_idx)
-					obs, reward, done, info = self.env.step(action)
-					ep_reward += reward
-					t += 1
-				ep_rewards.append(ep_reward)
-				ep_successes.append(info['success'])
-			results.update({
-				f'episode_reward+{self.cfg.tasks[task_idx]}': np.nanmean(ep_rewards),
-				f'episode_success+{self.cfg.tasks[task_idx]}': np.nanmean(ep_successes),})
-		return results
-	
-	def _load_dataset(self):
-		"""Load dataset for offline training."""
-		fp = Path(os.path.join(self.cfg.data_dir, '*.pt'))
-		fps = sorted(glob(str(fp)))
-		assert len(fps) > 0, f'No data found at {fp}'
-		print(f'Found {len(fps)} files in {fp}')
-		if len(fps) < (20 if self.cfg.task == 'mt80' else 4):
-			print(f'WARNING: expected 20 files for mt80 task set, 4 files for mt30 task set, found {len(fps)} files.')
-	
-		# Create buffer for sampling
-		_cfg = deepcopy(self.cfg)
-		_cfg.episode_length = 101 if self.cfg.task == 'mt80' else 501
-		_cfg.buffer_size = 550_450_000 if self.cfg.task == 'mt80' else 345_690_000
-		_cfg.steps = _cfg.buffer_size
-		self.buffer = Buffer(_cfg)
-		for fp in tqdm(fps, desc='Loading data'):
-			td = torch.load(fp, weights_only=False)
-			assert td.shape[1] == _cfg.episode_length, \
-				f'Expected episode length {td.shape[1]} to match config episode length {_cfg.episode_length}, ' \
-				f'please double-check your config.'
-			self.buffer.load(td)
-		expected_episodes = _cfg.buffer_size // _cfg.episode_length
-		if self.buffer.num_eps != expected_episodes:
-			print(f'WARNING: buffer has {self.buffer.num_eps} episodes, expected {expected_episodes} episodes for {self.cfg.task} task set.')
-
-	def train(self):
-		"""Train a TD-MPC2 agent."""
-		assert self.cfg.multitask and self.cfg.task in {'mt30', 'mt80'}, \
-			'Offline training only supports multitask training with mt30 or mt80 task sets.'
-		self._load_dataset()
-		
-		print(f'Training agent for {self.cfg.steps} iterations...')
-		metrics = {}
-		for i in range(self.cfg.steps):
-
-			# Update agent
-			train_metrics = self.agent.update(self.buffer)
-
-			# Evaluate agent periodically
-			if i % self.cfg.eval_freq == 0 or i % 10_000 == 0:
-				metrics = {
-					'iteration': i,
-					'elapsed_time': time() - self._start_time,
-				}
-				metrics.update(train_metrics)
-				if i % self.cfg.eval_freq == 0:
-					metrics.update(self.eval())
-					self.logger.pprint_multitask(metrics, self.cfg)
-					if i > 0:
-						self.logger.save_agent(self.agent, identifier=f'{i}')
-				self.logger.log(metrics, 'pretrain')
-			
-		self.logger.finish(self.agent)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/online_trainer.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/online_trainer.py
deleted file mode 100755
index 83128f7..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2/trainer/online_trainer.py
+++ /dev/null
@@ -1,127 +0,0 @@
-from time import time
-
-import numpy as np
-import torch
-from tensordict.tensordict import TensorDict
-from trainer.base import Trainer
-
-
-class OnlineTrainer(Trainer):
-	"""Trainer class for single-task online TD-MPC2 training."""
-
-	def __init__(self, *args, **kwargs):
-		super().__init__(*args, **kwargs)
-		self._step = 0
-		self._ep_idx = 0
-		self._start_time = time()
-
-	def common_metrics(self):
-		"""Return a dictionary of current metrics."""
-		elapsed_time = time() - self._start_time
-		return dict(
-			step=self._step,
-			episode=self._ep_idx,
-			elapsed_time=elapsed_time,
-			steps_per_second=self._step / elapsed_time
-		)
-
-	def eval(self):
-		"""Evaluate a TD-MPC2 agent."""
-		ep_rewards, ep_successes, ep_lengths = [], [], []
-		for i in range(self.cfg.eval_episodes):
-			obs, done, ep_reward, t = self.env.reset(), False, 0, 0
-			if self.cfg.save_video:
-				self.logger.video.init(self.env, enabled=(i==0))
-			while not done:
-				torch.compiler.cudagraph_mark_step_begin()
-				action = self.agent.act(obs, t0=t==0, eval_mode=True)
-				obs, reward, done, info = self.env.step(action)
-				ep_reward += reward
-				t += 1
-				if self.cfg.save_video:
-					self.logger.video.record(self.env)
-			ep_rewards.append(ep_reward)
-			ep_successes.append(info['success'])
-			ep_lengths.append(t)
-			if self.cfg.save_video:
-				self.logger.video.save(self._step)
-		return dict(
-			episode_reward=np.nanmean(ep_rewards),
-			episode_success=np.nanmean(ep_successes),
-			episode_length= np.nanmean(ep_lengths),
-		)
-
-	def to_td(self, obs, action=None, reward=None, terminated=None):
-		"""Creates a TensorDict for a new episode."""
-		if isinstance(obs, dict):
-			obs = TensorDict(obs, batch_size=(), device='cpu')
-		else:
-			obs = obs.unsqueeze(0).cpu()
-		if action is None:
-			action = torch.full_like(self.env.rand_act(), float('nan'))
-		if reward is None:
-			reward = torch.tensor(float('nan'))
-		if terminated is None:
-			terminated = torch.tensor(float('nan'))
-		td = TensorDict(
-			obs=obs,
-			action=action.unsqueeze(0),
-			reward=reward.unsqueeze(0),
-			terminated=terminated.unsqueeze(0),
-		batch_size=(1,))
-		return td
-
-	def train(self):
-		"""Train a TD-MPC2 agent."""
-		train_metrics, done, eval_next = {}, True, False
-		while self._step <= self.cfg.steps:
-			# Evaluate agent periodically
-			if self._step % self.cfg.eval_freq == 0:
-				eval_next = True
-
-			# Reset environment
-			if done:
-				if eval_next:
-					eval_metrics = self.eval()
-					eval_metrics.update(self.common_metrics())
-					self.logger.log(eval_metrics, 'eval')
-					eval_next = False
-
-				if self._step > 0:
-					if info['terminated'] and not self.cfg.episodic:
-						raise ValueError('Termination detected but you are not in episodic mode. ' \
-						'Set `episodic=true` to enable support for terminations.')
-					train_metrics.update(
-						episode_reward=torch.tensor([td['reward'] for td in self._tds[1:]]).sum(),
-						episode_success=info['success'],
-						episode_length=len(self._tds),
-						episode_terminated=info['terminated'])
-					train_metrics.update(self.common_metrics())
-					self.logger.log(train_metrics, 'train')
-					self._ep_idx = self.buffer.add(torch.cat(self._tds))
-
-				obs = self.env.reset()
-				self._tds = [self.to_td(obs)]
-
-			# Collect experience
-			if self._step > self.cfg.seed_steps:
-				action = self.agent.act(obs, t0=len(self._tds)==1)
-			else:
-				action = self.env.rand_act()
-			obs, reward, done, info = self.env.step(action)
-			self._tds.append(self.to_td(obs, action, reward, info['terminated']))
-
-			# Update agent
-			if self._step >= self.cfg.seed_steps:
-				if self._step == self.cfg.seed_steps:
-					num_updates = self.cfg.seed_steps
-					print('Pretraining agent on seed data...')
-				else:
-					num_updates = 1
-				for _ in range(num_updates):
-					_train_metrics = self.agent.update(self.buffer)
-				train_metrics.update(_train_metrics)
-
-			self._step += 1
-
-		self.logger.finish(self.agent)
diff --git a/src/rl_hockey/TD_MPC2_repo/tdmpc2_repo_wrapper.py b/src/rl_hockey/TD_MPC2_repo/tdmpc2_repo_wrapper.py
deleted file mode 100644
index 4fba770..0000000
--- a/src/rl_hockey/TD_MPC2_repo/tdmpc2_repo_wrapper.py
+++ /dev/null
@@ -1,762 +0,0 @@
-"""
-Wrapper for TD_MPC2_repo's TDMPC2 to integrate with the common Agent interface.
-
-Performance vs rl_hockey TD_MPC2 implementation:
-- act_batch loops over envs and calls tdmpc2.act() once per env (no batching).
-  The repo uses a single _prev_mean; batching would require per-env state.
-- Your impl batches encode and uses per-env planners, so it scales better with
-  num_envs > 1.
-- Repo compiles _plan and _update with torch.compile(reduce-overhead). If
-  "skipping cudagraphs due to cpu device" appears, compile adds overhead
-  without benefit. Set "compile": false in agent hyperparameters to avoid that.
-- Repo uses episodic + termination prediction and more structure; your impl
-  compiles only encoder/dynamics/reward and uses a simpler MPPI setup.
-"""
-
-import sys
-from pathlib import Path
-from typing import Optional
-
-import numpy as np
-import torch
-
-# Add parent directory to path to import rl_hockey modules
-current_dir = Path(__file__).parent
-parent_dir = current_dir.parent.parent
-if str(parent_dir) not in sys.path:
-    sys.path.insert(0, str(parent_dir))
-
-from rl_hockey.common.agent import Agent
-from rl_hockey.common.buffer import TDMPC2ReplayBuffer
-
-# Import TD_MPC2_repo modules
-# Add tdmpc2 directory to sys.path so relative imports work
-repo_path = current_dir / "tdmpc2"
-if str(repo_path) not in sys.path:
-    sys.path.insert(0, str(repo_path))
-
-# Lazy imports - only import TD_MPC2_repo modules when wrapper is actually used
-# This prevents import errors when TDMPC2_REPO is not being used
-_imported_modules = None
-
-
-def _import_tdmpc2_modules():
-    """Lazy import of TD_MPC2_repo modules. Called only when wrapper is instantiated."""
-    global _imported_modules
-    if _imported_modules is not None:
-        return _imported_modules
-
-    # Check for required dependencies first
-    try:
-        from omegaconf import OmegaConf
-    except ImportError:
-        raise ImportError(
-            "omegaconf is required for TDMPC2_REPO but is not installed.\n"
-            "Please install it with: pip install omegaconf\n"
-            "Or: conda install -c conda-forge omegaconf"
-        )
-
-    # Monkey-patch hydra before importing parser to avoid dependency
-    try:
-        import hydra  # noqa: F401
-    except ImportError:
-        # Create a minimal hydra mock
-        class HydraMock:
-            class utils:
-                @staticmethod
-                def get_original_cwd():
-                    import os
-
-                    return os.getcwd()
-
-        import sys as _sys
-
-        _sys.modules["hydra"] = HydraMock()
-        _sys.modules["hydra.utils"] = HydraMock.utils
-
-    try:
-        import common.math as math_module
-        from common.parser import cfg_to_dataclass, parse_cfg
-        from common.seed import set_seed
-        from tdmpc2 import TDMPC2 as TDMPC2Repo
-
-        # Patch TensorDict to handle version compatibility issues
-        try:
-            import torch
-            from tensordict import TensorDict
-
-            # Removed TensorDict.__init__ patch as it was causing object corruption
-            # (missing _tensordict attribute) in newer versions of tensordict.
-
-            # Patch TensorDict.update to handle version compatibility issues
-            if hasattr(TensorDict, "update"):
-                _original_tensordict_update = TensorDict.update
-
-                def _patched_tensordict_update(self, input_dict_or_td, **kwargs):
-                    """Patched update method that handles _batch_size attribute errors."""
-                    # If input is a regular dict, handle it directly
-                    if isinstance(input_dict_or_td, dict):
-                        # Regular dict - merge directly
-                        for key, value in input_dict_or_td.items():
-                            try:
-                                self[key] = value
-                            except (AttributeError, RuntimeError, TypeError):
-                                # If assignment fails, try to convert to tensor first
-                                if isinstance(value, torch.Tensor):
-                                    try:
-                                        self.set(key, value)
-                                    except (AttributeError, RuntimeError):
-                                        # Skip if all methods fail
-                                        pass
-                        return self
-
-                    # For TensorDict or other types, try original method first
-                    try:
-                        return _original_tensordict_update(
-                            self, input_dict_or_td, **kwargs
-                        )
-                    except AttributeError as e:
-                        error_str = str(e)
-                        if (
-                            "_batch_size" in error_str
-                            or "_device" in error_str
-                            or ("batch_size" in error_str and "TensorDict" in error_str)
-                        ):
-                            # Convert TensorDict to dict and merge manually
-                            update_dict = {}
-                            try:
-                                if isinstance(input_dict_or_td, dict):
-                                    update_dict = input_dict_or_td
-                                elif hasattr(input_dict_or_td, "to_dict"):
-                                    update_dict = input_dict_or_td.to_dict()
-                                else:
-                                    # Try to convert to dict safely
-                                    update_dict = dict(input_dict_or_td)
-                            except Exception:
-                                # Last resort: try to iterate keys if possible
-                                try:
-                                    if hasattr(input_dict_or_td, "keys"):
-                                        update_dict = {
-                                            k: input_dict_or_td[k]
-                                            for k in input_dict_or_td.keys()
-                                        }
-                                except Exception:
-                                    pass
-
-                            # Merge into self using direct assignment
-                            for key, value in update_dict.items():
-                                try:
-                                    self[key] = value
-                                except (AttributeError, RuntimeError, TypeError):
-                                    # If direct assignment fails, try alternative approach
-                                    if isinstance(value, torch.Tensor):
-                                        try:
-                                            self.set(key, value)
-                                        except (AttributeError, RuntimeError):
-                                            # Skip this key if all methods fail
-                                            pass
-                            return self
-                        raise
-
-                # Apply patch
-                TensorDict.update = _patched_tensordict_update
-
-            # Patch termination_statistics to return a regular dict instead of TensorDict
-            # This prevents TensorDict compatibility issues when updating info
-            _original_termination_statistics = math_module.termination_statistics
-
-            def _patched_termination_statistics(pred, target, eps=1e-9):
-                """Patched to return regular dict instead of TensorDict."""
-                # Compute statistics directly without creating TensorDict
-                pred = pred.squeeze(-1)
-                target = target.squeeze(-1)
-                rate = target.sum() / len(target)
-                tp = ((pred > 0.5) & (target == 1)).sum()
-                fn = ((pred <= 0.5) & (target == 1)).sum()
-                fp = ((pred > 0.5) & (target == 0)).sum()
-                eps_val = 1e-9
-                recall = tp / (tp + fn + eps_val)
-                precision = tp / (tp + fp + eps_val)
-                f1 = 2 * (precision * recall) / (precision + recall + eps_val)
-                # Return as regular dict - the update method will handle it
-                return {
-                    "termination_rate": rate,
-                    "termination_f1": f1,
-                }
-
-            math_module.termination_statistics = _patched_termination_statistics
-
-            # Patch update_pi to return a regular dict instead of TensorDict
-            _original_update_pi = TDMPC2Repo.update_pi
-
-            def _patched_update_pi(self, zs, task):
-                """Patched to return regular dict instead of TensorDict."""
-                # Call original method
-                result = _original_update_pi(self, zs, task)
-                # Convert TensorDict to regular dict
-                pi_dict = {}
-                try:
-                    if hasattr(result, "to_dict"):
-                        pi_dict = result.to_dict()
-                    elif isinstance(result, dict):
-                        pi_dict = result
-                    else:
-                        pi_dict = dict(result)
-                except Exception:
-                    # Fallback
-                    try:
-                        if hasattr(result, "keys"):
-                            pi_dict = {k: result[k] for k in result.keys()}
-                    except Exception:
-                        pass
-                return pi_dict if pi_dict else result
-
-            TDMPC2Repo.update_pi = _patched_update_pi
-
-        except (ImportError, AttributeError, TypeError) as patch_err:
-            # If patching fails, continue anyway - the error will be caught in train()
-            import warnings
-
-            warnings.warn(
-                f"Failed to patch TensorDict for compatibility: {patch_err}. "
-                f"Some features may not work correctly."
-            )
-
-    except ImportError as e:
-        # If imports fail, provide helpful error message
-        raise ImportError(
-            f"Failed to import TD_MPC2_repo modules.\n"
-            f"Error: {e}\n\n"
-            f"Required dependencies for TDMPC2_REPO:\n"
-            f"  - omegaconf (for config management)\n"
-            f"  - tensordict (for tensor operations)\n"
-            f"  - torch (PyTorch)\n\n"
-            f"Install missing dependencies with:\n"
-            f"  pip install omegaconf tensordict\n"
-            f"Or:\n"
-            f"  conda install -c conda-forge omegaconf tensordict\n\n"
-            f"Also ensure the TD_MPC2_repo/tdmpc2 directory exists and contains all required modules."
-        )
-
-    _imported_modules = {
-        "OmegaConf": OmegaConf,
-        "cfg_to_dataclass": cfg_to_dataclass,
-        "parse_cfg": parse_cfg,
-        "set_seed": set_seed,
-        "TDMPC2Repo": TDMPC2Repo,
-    }
-    return _imported_modules
-
-
-class TDMPC2RepoWrapper(Agent):
-    """
-    Wrapper for TD_MPC2_repo's TDMPC2 that implements the common Agent interface.
-
-    This wrapper allows using the reference TD-MPC2 implementation with the
-    existing training infrastructure.
-    """
-
-    def __init__(
-        self,
-        obs_dim: int,
-        action_dim: int,
-        latent_dim: int = 512,
-        hidden_dim: Optional[dict] = None,
-        num_q: int = 5,
-        lr: float = 3e-4,
-        enc_lr_scale: float = 0.3,
-        gamma: float = 0.99,
-        horizon: int = 5,
-        num_samples: int = 512,
-        num_iterations: int = 6,
-        num_elites: int = 64,
-        num_pi_trajs: int = 24,
-        capacity: int = 1000000,
-        temperature: float = 0.5,
-        batch_size: int = 256,
-        device: str = "cuda",
-        num_bins: int = 101,
-        vmin: float = -10.0,
-        vmax: float = 10.0,
-        tau: float = 0.01,
-        grad_clip_norm: float = 20.0,
-        consistency_coef: float = 20.0,
-        reward_coef: float = 0.1,
-        value_coef: float = 0.1,
-        termination_coef: float = 1.0,
-        rho: float = 0.5,
-        entropy_coef: float = 1e-4,
-        log_std_min: float = -10.0,
-        log_std_max: float = 2.0,
-        min_std: float = 0.05,
-        max_std: float = 2.0,
-        discount_denom: float = 5.0,
-        discount_min: float = 0.95,
-        discount_max: float = 0.995,
-        episodic: bool = False,
-        mpc: bool = True,
-        compile: bool = True,
-        episode_length: int = 500,
-        seed: int = 1,
-        win_reward_bonus: float = 10.0,
-        win_reward_discount: float = 0.92,
-        **kwargs,
-    ):
-        """
-        Initialize TDMPC2RepoWrapper.
-
-        Args:
-            obs_dim: Observation dimension
-            action_dim: Action dimension
-            latent_dim: Latent dimension for world model
-            hidden_dim: Optional dict with network-specific hidden dimensions
-            num_q: Number of Q-networks in ensemble
-            lr: Learning rate
-            enc_lr_scale: Encoder learning rate scale
-            gamma: Discount factor
-            horizon: Planning horizon
-            num_samples: Number of samples for MPPI
-            num_iterations: Number of MPPI iterations
-            num_elites: Number of elite samples in MPPI
-            num_pi_trajs: Number of policy trajectories in MPPI
-            capacity: Replay buffer capacity
-            temperature: MPPI temperature
-            batch_size: Training batch size
-            device: Device to use ('cuda' or 'cpu')
-            num_bins: Number of bins for discrete regression
-            vmin: Minimum value for value function
-            vmax: Maximum value for value function
-            tau: Soft update coefficient for target networks
-            grad_clip_norm: Gradient clipping norm
-            consistency_coef: Consistency loss coefficient
-            reward_coef: Reward prediction loss coefficient
-            value_coef: Value loss coefficient
-            termination_coef: Termination prediction loss coefficient
-            rho: Discount factor for multi-step losses
-            entropy_coef: Entropy coefficient for policy
-            log_std_min: Minimum log std for policy
-            log_std_max: Maximum log std for policy
-            min_std: Minimum std for MPPI
-            max_std: Maximum std for MPPI
-            discount_denom: Denominator for discount factor calculation
-            discount_min: Minimum discount factor
-            discount_max: Maximum discount factor
-            episodic: Whether environment is episodic
-            mpc: Whether to use MPC planning
-            compile: Whether to compile with torch.compile
-            episode_length: Expected episode length
-            seed: Random seed
-            win_reward_bonus: Bonus reward to add to each step in a winning episode.
-                Applied with discount factor backwards through the episode. Set to 0.0 to disable.
-            win_reward_discount: Discount factor for applying win reward bonus
-                backwards through the episode (1.0 = no discount, 0.99 = standard).
-        """
-        super().__init__()
-
-        # Lazy import TD_MPC2_repo modules (only when wrapper is actually used)
-        modules = _import_tdmpc2_modules()
-        OmegaConf = modules["OmegaConf"]
-        parse_cfg = modules["parse_cfg"]
-        set_seed = modules["set_seed"]
-        TDMPC2Repo = modules["TDMPC2Repo"]
-
-        # Normalize device: "cuda" -> "cuda:0" so wrapper and inner repo agree
-        if isinstance(device, str) and device == "cuda" and torch.cuda.is_available():
-            device = "cuda:0"
-        self.device_str = device
-        self.device = torch.device(device)
-
-        # Store parameters
-        self.obs_dim = obs_dim
-        self.action_dim = action_dim
-        self.latent_dim = latent_dim
-        self.horizon = horizon
-        self.batch_size = batch_size
-        self.episode_length = episode_length
-
-        # Set seed
-        set_seed(seed)
-
-        # Create config dict compatible with TD_MPC2_repo
-        # Use default model_size=5 parameters if hidden_dim not provided
-        if hidden_dim is None:
-            enc_dim = 256
-            mlp_dim = 512
-            num_enc_layers = 2
-        else:
-            # Extract dimensions from hidden_dim dict
-            enc_dim = (
-                hidden_dim.get("encoder", [256, 256, 256])[0]
-                if isinstance(hidden_dim.get("encoder"), list)
-                else 256
-            )
-            mlp_dim = (
-                hidden_dim.get("encoder", [512, 512, 512])[0]
-                if isinstance(hidden_dim.get("encoder"), list)
-                else 512
-            )
-            num_enc_layers = (
-                len(hidden_dim.get("encoder", [256, 256]))
-                if isinstance(hidden_dim.get("encoder"), list)
-                else 2
-            )
-
-        # Create OmegaConf config
-        cfg_dict = {
-            "task": "hockey",  # Dummy task name
-            "obs": "state",
-            "episodic": episodic,
-            "steps": 10_000_000,  # Dummy, not used
-            "batch_size": batch_size,
-            "reward_coef": reward_coef,
-            "value_coef": value_coef,
-            "termination_coef": termination_coef,
-            "consistency_coef": consistency_coef,
-            "rho": rho,
-            "lr": lr,
-            "enc_lr_scale": enc_lr_scale,
-            "grad_clip_norm": grad_clip_norm,
-            "tau": tau,
-            "discount_denom": discount_denom,
-            "discount_min": discount_min,
-            "discount_max": discount_max,
-            "buffer_size": capacity,
-            "exp_name": "default",
-            "data_dir": None,
-            "mpc": mpc,
-            "iterations": num_iterations,
-            "num_samples": num_samples,
-            "num_elites": num_elites,
-            "num_pi_trajs": num_pi_trajs,
-            "horizon": horizon,
-            "min_std": min_std,
-            "max_std": max_std,
-            "temperature": temperature,
-            "log_std_min": log_std_min,
-            "log_std_max": log_std_max,
-            "entropy_coef": entropy_coef,
-            "num_bins": num_bins,
-            "vmin": vmin,
-            "vmax": vmax,
-            "model_size": None,  # Will use explicit dimensions
-            "num_enc_layers": num_enc_layers,
-            "enc_dim": enc_dim,
-            "num_channels": 32,  # Not used for state obs
-            "mlp_dim": mlp_dim,
-            "latent_dim": latent_dim,
-            "task_dim": 0,  # Single task
-            "num_q": num_q,
-            "dropout": 0.01,
-            "simnorm_dim": 8,
-            "compile": compile,
-            "save_video": False,
-            "save_agent": True,
-            "seed": seed,
-            "work_dir": None,
-            "task_title": "Hockey",
-            "multitask": False,
-            "tasks": ["hockey"],
-            "obs_shape": {"state": (obs_dim,)},  # Must be a dict, not a tuple
-            "action_dim": action_dim,
-            "episode_length": episode_length,
-            "obs_shapes": [(obs_dim,)],
-            "action_dims": [action_dim],
-            "episode_lengths": [episode_length],
-            "seed_steps": 0,
-            "bin_size": (vmax - vmin) / (num_bins - 1),
-            "device": self.device_str,
-        }
-
-        # Convert to OmegaConf and parse
-        cfg = OmegaConf.create(cfg_dict)
-        self.cfg = parse_cfg(cfg)
-
-        # Initialize TD_MPC2_repo agent (uses cfg.device, no longer hardcoded cuda:0)
-        self.tdmpc2 = TDMPC2Repo(self.cfg)
-
-        # Log device so we can verify GPU is used when expected
-        print(f"[TDMPC2RepoWrapper] device={self.device} (model and buffer use this)")
-
-        # Store reference to modules for later use
-        self._modules = modules
-
-        # Buffer storage device: default to CPU for larger capacity and negligible transfer overhead
-        buffer_device = config.get("buffer_device", "cpu")
-        
-        # Replace buffer with TDMPC2ReplayBuffer (use same normalized device as model)
-        self.buffer = TDMPC2ReplayBuffer(
-            max_size=capacity,
-            horizon=horizon,
-            batch_size=batch_size,
-            use_torch_tensors=True,
-            device=self.device_str,  # Where sampled batches go (for training)
-            buffer_device=buffer_device,  # Where episode data is stored
-            pin_memory=(buffer_device == "cpu"),  # Pin memory for faster CPU->GPU transfer
-            multitask=False,
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-        )
-
-        # Track episode start for t0 flag
-        self._episode_step = 0
-        self._last_t0 = True
-
-    def act(self, state, deterministic=False, t0=None):
-        """
-        Returns the action for a given state according to the current policy.
-
-        Args:
-            state: Observation array
-            deterministic: If True, use mean action (no exploration)
-            t0: Optional bool indicating if this is the first step of an episode.
-                If None, uses internal tracking.
-
-        Returns:
-            Action array
-        """
-        if t0 is None:
-            t0 = self._last_t0
-
-        # After using t0, next step won't be t0 (unless explicitly set)
-        self._last_t0 = False
-
-        # Convert to torch tensor if needed
-        if isinstance(state, np.ndarray):
-            obs = torch.from_numpy(state).float()
-        else:
-            obs = torch.as_tensor(state, dtype=torch.float32)
-
-        obs = obs.to(self.device, non_blocking=True)
-        action = self.tdmpc2.act(obs, t0=t0, eval_mode=deterministic)
-
-        # Convert to numpy
-        if isinstance(action, torch.Tensor):
-            action = action.cpu().numpy()
-
-        # Ensure it's a 1D array
-        if action.ndim > 1:
-            action = action.flatten()
-
-        return action
-
-    def act_batch(self, states, deterministic=False, t0s=None):
-        """
-        Process a batch of states at once (for vectorized environments).
-
-        Fast path for batch size 1 (num_envs=1): single encode+plan, no loop.
-        For batch size > 1, loops over envs (repo uses single _prev_mean).
-
-        Args:
-            states: Batch of observations (batch_size, obs_dim)
-            deterministic: If True, use mean actions
-            t0s: Optional array of booleans indicating episode starts
-
-        Returns:
-            Batch of actions (batch_size, action_dim)
-        """
-        n = len(states) if hasattr(states, "__len__") else states.shape[0]
-        if t0s is None:
-            t0s = np.zeros(n, dtype=bool)
-
-        if isinstance(states, np.ndarray):
-            obs = torch.from_numpy(states).float()
-        else:
-            obs = torch.as_tensor(states, dtype=torch.float32)
-        obs = obs.to(self.device, non_blocking=True)
-
-        if n == 1:
-            action = self.tdmpc2.act(
-                obs[0], t0=bool(t0s[0]), eval_mode=deterministic
-            )
-            out = action.cpu().numpy()
-            return out[None, :] if out.ndim == 1 else out
-
-        actions = []
-        for obs_i, t0 in zip(obs, t0s):
-            a = self.tdmpc2.act(obs_i, t0=bool(t0), eval_mode=deterministic)
-            actions.append(a)
-        actions = torch.stack(actions)
-        return actions.cpu().numpy()
-
-    def evaluate(self, state):
-        """
-        Returns the value of a given state.
-
-        Args:
-            state: Observation array
-
-        Returns:
-            Estimated state value
-        """
-        # Convert to torch tensor if needed
-        if isinstance(state, np.ndarray):
-            obs = torch.from_numpy(state).float()
-        else:
-            obs = torch.as_tensor(state, dtype=torch.float32)
-
-        obs = obs.to(self.device, non_blocking=True)
-
-        with torch.no_grad():
-            z = self.tdmpc2.model.encode(obs.unsqueeze(0))
-            action, _ = self.tdmpc2.model.pi(z)
-            q_value = self.tdmpc2.model.Q(z, action, return_type="avg")
-            value = q_value.item()
-
-        return value
-
-    def train(self, steps=1):
-        """
-        Performs `steps` gradient steps.
-
-        Args:
-            steps: Number of training steps to perform
-
-        Returns:
-            Dictionary of training statistics (optional, for logging)
-        """
-        if self.buffer.size < self.horizon + 1:
-            # Buffer doesn't have enough data yet - return empty dict
-            # This is normal during warmup period
-            return {}
-
-        all_stats = []
-
-        for _ in range(steps):
-            try:
-                # Sample from buffer
-                obs, action, reward, terminated, task = self.buffer.sample(
-                    batch_size=self.batch_size, horizon=self.horizon
-                )
-
-                # Convert to torch tensors if needed
-                if isinstance(obs, np.ndarray):
-                    obs = torch.from_numpy(obs).float()
-                if isinstance(action, np.ndarray):
-                    action = torch.from_numpy(action).float()
-                if isinstance(reward, np.ndarray):
-                    reward = torch.from_numpy(reward).float()
-                if isinstance(terminated, np.ndarray):
-                    terminated = torch.from_numpy(terminated).float()
-
-                # Move to device
-                obs = obs.to(self.device, non_blocking=True)
-                action = action.to(self.device, non_blocking=True)
-                reward = reward.to(self.device, non_blocking=True)
-                terminated = terminated.to(self.device, non_blocking=True)
-
-                # TD_MPC2_repo expects (horizon+1, batch_size, obs_dim) format
-                # Our buffer returns (batch_size, horizon+1, obs_dim)
-                obs = obs.permute(1, 0, 2)  # (horizon+1, batch_size, obs_dim)
-                action = action.permute(1, 0, 2)  # (horizon, batch_size, action_dim)
-                reward = reward.permute(1, 0, 2)  # (horizon, batch_size, 1)
-                terminated = terminated.permute(1, 0, 2)  # (horizon, batch_size, 1)
-
-                # Update agent
-                try:
-                    stats = self.tdmpc2._update(
-                        obs, action, reward, terminated, task=None
-                    )
-                except (AttributeError, RuntimeError, TypeError) as e:
-                    # Handle TensorDict version compatibility issues
-                    error_str = str(e)
-                    if (
-                        any(
-                            attr in error_str
-                            for attr in [
-                                "_batch_size",
-                                "_device",
-                                "batch_size",
-                                "device",
-                            ]
-                        )
-                        and "TensorDict" in error_str
-                    ):
-                        # TensorDict version compatibility issue
-                        import warnings
-
-                        warnings.warn(
-                            f"TensorDict compatibility issue encountered: {e}. "
-                            f"Skipping this training step. The patches should have "
-                            f"prevented this - check tensordict version compatibility."
-                        )
-                        # Skip this training step
-                        continue
-                    else:
-                        # Re-raise if it's not a TensorDict compatibility issue
-                        raise
-
-                if stats is not None:
-                    # Convert TensorDict to regular dict and extract scalar values
-                    if hasattr(stats, "items"):
-                        stats_dict = {}
-                        for key, value in stats.items():
-                            # Convert tensor to Python scalar
-                            if isinstance(value, torch.Tensor):
-                                if value.numel() == 1:
-                                    stats_dict[key] = value.item()
-                                else:
-                                    # If it's a multi-element tensor, take mean
-                                    stats_dict[key] = value.mean().item()
-                            else:
-                                stats_dict[key] = value
-                        all_stats.append(stats_dict)
-                    else:
-                        all_stats.append({})
-
-            except RuntimeError as e:
-                # Buffer might not have enough data
-                if "no episode has length" in str(e):
-                    break
-                raise
-
-        # Aggregate stats
-        if all_stats:
-            aggregated = {}
-            for key in all_stats[0].keys():
-                values = [s.get(key) for s in all_stats if key in s]
-                if values:
-                    if isinstance(values[0], torch.Tensor):
-                        aggregated[key] = torch.stack(values).mean().item()
-                    else:
-                        aggregated[key] = np.mean(values)
-            return aggregated
-
-        return {}
-
-    def save(self, filepath):
-        """
-        Saves the agent's model to the specified filepath.
-
-        Args:
-            filepath: Path to save checkpoint
-        """
-        self.tdmpc2.save(filepath)
-
-    def load(self, filepath):
-        """
-        Loads the agent's model from the specified filepath.
-
-        Args:
-            filepath: Path to load checkpoint from
-        """
-        self.tdmpc2.load(filepath)
-
-    def on_episode_start(self, episode):
-        """
-        Hook that is called at the start of each episode.
-        """
-        self._episode_step = 0
-        self._last_t0 = True
-
-    def on_episode_end(self, episode):
-        """
-        Hook that is called at the end of each episode.
-        """
-        self._episode_step = 0
-        self._last_t0 = True
-
-    def log_architecture(self):
-        """
-        Logs the network architecture for the agent.
-        """
-        return str(self.tdmpc2.model)
diff --git a/src/rl_hockey/common/agent.py b/src/rl_hockey/common/agent.py
index 7dae3dd..3bef9e3 100644
--- a/src/rl_hockey/common/agent.py
+++ b/src/rl_hockey/common/agent.py
@@ -1,15 +1,12 @@
 from abc import ABC, abstractmethod
 
-from rl_hockey.common.prioritizedbuffer import PERMemory
 from rl_hockey.common.buffer import ReplayBuffer
 
 
 class Agent(ABC):
-    def __init__(self, priority_replay: bool = False, normalize_obs: bool = False):
-        if priority_replay:
-            self.buffer = PERMemory(normalize_obs=normalize_obs)
-        else:
-            self.buffer = ReplayBuffer(normalize_obs=normalize_obs)
+    def __init__(self):
+        self.buffer = ReplayBuffer()
+
     def store_transition(self, transition):
         """Stores a transition in the replay buffer."""
         self.buffer.store(transition)
@@ -46,11 +43,3 @@ class Agent(ABC):
     def on_episode_end(self, episode):
         """Hook that is called at the end of each episode."""
         pass
-
-    def log_architecture(self):
-        """
-        Logs the network architecture for the agent.
-        Override this method in subclasses to provide agent-specific architecture details.
-        Returns a formatted string describing the architecture.
-        """
-        return "Network architecture logging not implemented for this agent type."
diff --git a/src/rl_hockey/common/buffer.py b/src/rl_hockey/common/buffer.py
index a2840f5..e3becb5 100644
--- a/src/rl_hockey/common/buffer.py
+++ b/src/rl_hockey/common/buffer.py
@@ -1,867 +1,53 @@
-import random
-
 import numpy as np
-import torch
-
-from rl_hockey.common.reward_backprop import apply_win_reward_backprop
-from rl_hockey.common.segment_tree import MinSegmentTree, SumSegmentTree
 
 
 class ReplayBuffer:
-    def __init__(
-        self,
-        max_size=1_000_000,
-        use_torch_tensors=False,
-        pin_memory=False,
-        device="cpu",
-        normalize_obs = False
-    ):
-        """
-        Initialize replay buffer.
-
-        Args:
-            max_size: Maximum size of the buffer
-            use_torch_tensors: If True, use torch tensors instead of numpy arrays (faster for GPU transfers)
-            pin_memory: If True, pin memory for faster CPU->GPU transfers (only if use_torch_tensors=True and device="cpu")
-            device: Device to store buffer on ("cpu" or "cuda"). If "cuda", buffer is stored directly on GPU for zero-copy sampling.
-        """
+    def __init__(self, max_size=1_000_000):
         self.max_size = max_size
         self.current_idx = 0
         self.size = 0
-        self.use_torch_tensors = use_torch_tensors or (device != "cpu")
-        self.device = (
-            device
-            if isinstance(device, str)
-            else (device.type if hasattr(device, "type") else "cpu")
-        )
-        # Only use pinned memory if on CPU (pinned memory doesn't apply to GPU tensors)
-        self.pin_memory = (
-            pin_memory
-            and self.device == "cpu"
-            and self.use_torch_tensors
-            and torch.cuda.is_available()
-        )
 
         self.state = None
         self.action = None
         self.next_state = None
         self.reward = None
         self.done = None
-        self.normalize_obs = normalize_obs
-
-        self.obs_mean = None
-        self.obs_M2 = None
-
-    def normalize(self, state: np.ndarray):
-        normalized_state = (state - self.obs_mean) / (np.sqrt(self.obs_M2 / self.size) + 1e-8)
-        return normalized_state
-
-
-    def update_obs_stats(self, state: np.ndarray):
-        delta = state - self.obs_mean
-        self.obs_mean += delta / self.size
-        delta2 = state - self.obs_mean
-        self.obs_M2 += delta * delta2
-
 
     def store(self, transition):
         state, action, reward, next_state, done = transition
 
         if self.size == 0:
-            if self.use_torch_tensors:
-                # Use torch tensors - store directly on GPU if device="cuda" (zero-copy sampling)
-                # Or on CPU with pinned memory for faster transfers
-                device_kwargs = (
-                    {"device": self.device}
-                    if self.device != "cpu"
-                    else {"pin_memory": self.pin_memory}
-                )
-                self.state = torch.empty(
-                    (self.max_size, len(state)), dtype=torch.float32, **device_kwargs
-                )
-                self.action = torch.empty(
-                    (self.max_size, len(action)), dtype=torch.float32, **device_kwargs
-                )
-                self.next_state = torch.empty(
-                    (self.max_size, len(state)), dtype=torch.float32, **device_kwargs
-                )
-                self.reward = torch.empty(
-                    (self.max_size, 1), dtype=torch.float32, **device_kwargs
-                )
-                self.done = torch.empty(
-                    (self.max_size, 1), dtype=torch.float32, **device_kwargs
-                )
-            else:
-                # Original numpy arrays (for backward compatibility)
-                self.state = np.empty((self.max_size, len(state)), dtype=np.float32)
-                self.action = np.empty((self.max_size, len(action)), dtype=np.float32)
-                self.next_state = np.empty(
-                    (self.max_size, len(state)), dtype=np.float32
-                )
-                self.reward = np.empty((self.max_size, 1), dtype=np.float32)
-                self.done = np.empty((self.max_size, 1), dtype=np.float32)
+            self.state = np.empty((self.max_size, len(state)), dtype=np.float32)
+            self.action = np.empty((self.max_size, len(action)), dtype=np.float32)
+            self.next_state = np.empty((self.max_size, len(state)), dtype=np.float32)
+            self.reward = np.empty((self.max_size, 1), dtype=np.float32)
+            self.done = np.empty((self.max_size, 1), dtype=np.float32)
+
+        self.state[self.current_idx] = state
+        self.action[self.current_idx] = action
+        self.next_state[self.current_idx] = next_state
+        self.reward[self.current_idx] = reward
+        self.done[self.current_idx] = done 
 
-            self.obs_mean = np.zeros_like(state, dtype=np.float32)
-            self.obs_M2 = np.zeros_like(state, dtype=np.float32)
-
-        # Update observation statistics
         self.current_idx = (self.current_idx + 1) % self.max_size
         self.size = min(self.size + 1, self.max_size)
-        self.update_obs_stats(state)
-        self.update_obs_stats(next_state)
-
-        # Store data (convert to tensor if using torch tensors)
-        if self.use_torch_tensors:
-            # Convert to tensor and move to buffer device if needed
-            state_tensor = torch.as_tensor(state, dtype=torch.float32)
-            action_tensor = torch.as_tensor(action, dtype=torch.float32)
-            next_state_tensor = torch.as_tensor(next_state, dtype=torch.float32)
-            # Handle reward/done - can be scalar or array, ensure shape (1,)
-            reward_tensor = torch.as_tensor(reward, dtype=torch.float32)
-            if reward_tensor.dim() == 0:
-                reward_tensor = reward_tensor.unsqueeze(0)
-            reward_tensor = reward_tensor.reshape(1)
-            done_tensor = torch.as_tensor(done, dtype=torch.float32)
-            if done_tensor.dim() == 0:
-                done_tensor = done_tensor.unsqueeze(0)
-            done_tensor = done_tensor.reshape(1)
-
-            # Move to buffer device if buffer is on GPU
-            if self.device != "cpu":
-                state_tensor = state_tensor.to(self.device, non_blocking=True)
-                action_tensor = action_tensor.to(self.device, non_blocking=True)
-                next_state_tensor = next_state_tensor.to(self.device, non_blocking=True)
-                reward_tensor = reward_tensor.to(self.device, non_blocking=True)
-                done_tensor = done_tensor.to(self.device, non_blocking=True)
-
-            self.state[self.current_idx] = state_tensor
-            self.action[self.current_idx] = action_tensor
-            self.next_state[self.current_idx] = next_state_tensor
-            self.reward[self.current_idx] = reward_tensor
-            self.done[self.current_idx] = done_tensor
-        else:
-            self.state[self.current_idx] = state
-            self.action[self.current_idx] = action
-            self.next_state[self.current_idx] = next_state
-            self.reward[self.current_idx] = reward
-            self.done[self.current_idx] = done
-
 
     def sample(self, batch_size):
         if batch_size > self.size:
             batch_size = self.size
 
-        if self.use_torch_tensors:
-            # Use torch.randint for faster random sampling
-            # If buffer is on GPU, generate indices on same device for efficiency
-            if self.device != "cpu":
-                idx = torch.randint(0, self.size, (batch_size,), device=self.device)
-            else:
-                idx = torch.randint(0, self.size, (batch_size,))
-            
-            state = self.state[idx]
-            action = self.action[idx]
-            reward = self.reward[idx]
-            next_state = self.next_state[idx]
-            done = self.done[idx]
-            
-
-        else:
-            idx = np.random.randint(0, self.size, size=batch_size)
-            state = self.state[idx]
-            action = self.action[idx]
-            reward = self.reward[idx]
-            next_state = self.next_state[idx]
-            done = self.done[idx]
-
-        if self.normalize_obs:
-            state = self.normalize(state)
-            next_state = self.normalize(next_state)
-
-        return state, action, reward, next_state, done
-
-    def _is_valid_sequence_start(self, start_idx, horizon):
-        """Check if a sequence starting at start_idx is valid (no terminals, no wrap)."""
-        if self.size < horizon + 1:
-            return False
-
-        # If buffer not full, ensure we don't exceed current size
-        if self.size < self.max_size:
-            if start_idx + horizon >= self.size:
-                return False
-            idx_range = range(start_idx, start_idx + horizon)
-        else:
-            # Avoid sequences that cross the circular buffer boundary at current_idx
-            if start_idx < self.current_idx:
-                if start_idx + horizon >= self.current_idx:
-                    return False
-            else:
-                if start_idx + horizon >= self.max_size:
-                    return False
-            idx_range = range(start_idx, start_idx + horizon)
-
-        # Ensure no terminal transitions in the sequence
-        if self.use_torch_tensors:
-            dones = self.done[list(idx_range)]
-            return not torch.any(dones > 0.5).item()
-        return not np.any(self.done[list(idx_range)] > 0.5)
-
-    def sample_sequences(self, batch_size, horizon):
-        """
-        Sample contiguous sequences of transitions.
-
-        Returns
-        -------
-        tuple or None
-            (obs_seq, action_seq, reward_seq, done_seq) with shapes:
-            obs_seq: (batch, horizon+1, obs_dim)
-            action_seq: (batch, horizon, action_dim)
-            reward_seq: (batch, horizon, 1)
-            done_seq: (batch, horizon, 1)
-            Returns None if not enough data.
-        """
-        if self.size < horizon + 1:
-            return None
-
-        max_batch = self.size - horizon
-        batch_size = min(batch_size, max_batch)
-
-        starts = []
-        attempts = 0
-        max_attempts = batch_size * 20
-        max_index = self.size if self.size < self.max_size else self.max_size
-
-        while len(starts) < batch_size and attempts < max_attempts:
-            start_idx = random.randrange(0, max_index)
-            if self._is_valid_sequence_start(start_idx, horizon):
-                starts.append(start_idx)
-            attempts += 1
-
-        if len(starts) == 0:
-            return None
-
-        # Build index matrix without wrap (sequence validity ensures no wrap)
-        idx_matrix = [[s + t for t in range(horizon + 1)] for s in starts]
-
-        if self.use_torch_tensors:
-            idx_tensor = torch.as_tensor(
-                idx_matrix, dtype=torch.long, device=self.state.device
-            )
-            obs_seq = self.state[idx_tensor]
-            action_seq = self.action[idx_tensor[:, :horizon]]
-            reward_seq = self.reward[idx_tensor[:, :horizon]]
-            done_seq = self.done[idx_tensor[:, :horizon]]
-        else:
-            idx_array = np.asarray(idx_matrix, dtype=np.int64)
-            obs_seq = self.state[idx_array]
-            action_seq = self.action[idx_array[:, :horizon]]
-            reward_seq = self.reward[idx_array[:, :horizon]]
-            done_seq = self.done[idx_array[:, :horizon]]
-
-        return obs_seq, action_seq, reward_seq, done_seq
+        idx = np.random.randint(0, self.size, size=batch_size)
 
+        return (
+            self.state[idx],
+            self.action[idx],
+            self.reward[idx],
+            self.next_state[idx],
+            self.done[idx],
+        )
+    
     def clear(self):
         """Clear the replay buffer, resetting it to empty state."""
         self.current_idx = 0
         self.size = 0
-        # Keep arrays allocated but mark as empty
-
-
-class TDMPC2ReplayBuffer:
-    """Episode-based replay buffer for TD-MPC2.
-
-    Stores full episodes and samples contiguous subsequences of length horizon+1
-    for world model training. Functionally similar to the torchrl-based Buffer
-    in tdmpc2_repo, but implemented in the same style as ReplayBuffer (no
-    torchrl/tensordict, optional numpy/torch, explicit device/pin_memory).
-
-    Supports:
-    - add_episode: add one episode (obs, action, reward, terminated, task)
-    - add_episodes: add multiple episodes (batch load)
-    - store: append transitions; when done=True, the current trajectory is
-      closed and added as an episode (for compatibility with transition-based
-      env loops)
-    - sample: returns (obs, action, reward, terminated, task) with shapes
-      (B, H+1, obs_dim), (B, H, action_dim), (B, H, 1), (B, H, 1), (B, task_dim) or None
-    - sample_sequences: same as sample but returns (obs, action, reward, terminated)
-      for drop-in use with rl_hockey TD-MPC2 train (no task in the 4-tuple)
-    """
-
-    def __init__(
-        self,
-        max_size=1_000_000,
-        horizon=5,
-        batch_size=256,
-        use_torch_tensors=False,
-        pin_memory=False,
-        device="cpu",
-        buffer_device="cpu",
-        multitask=False,
-        win_reward_bonus=10.0,
-        win_reward_discount=0.92,
-    ):
-        """
-        Initialize TD-MPC2 replay buffer.
-
-        Args:
-            max_size: Maximum number of transitions across all episodes.
-                Oldest episodes are evicted when exceeded.
-            horizon: Length of sampled subsequences (number of transitions in
-                the chunk; obs will have horizon+1 steps).
-            batch_size: Number of subsequences returned per sample.
-            use_torch_tensors: If True, use torch tensors; else numpy.
-            pin_memory: If True, pin memory for faster CPU to GPU transfer
-                (only when use_torch_tensors=True and buffer_device="cpu").
-            device: Device to place sampled batches on ("cpu" or "cuda").
-                This is where data goes AFTER sampling for training.
-            buffer_device: Device to store episode data on ("cpu" or "cuda").
-                Default "cpu" is recommended - allows larger buffers and
-                transfer overhead is negligible (<1% of training time).
-                Only set to "cuda" if profiling shows transfer bottleneck.
-            multitask: If True, expect and return task in add_episode and sample.
-            win_reward_bonus: Bonus reward to add to each step in a winning episode.
-                Applied with discount factor backwards through the episode.
-            win_reward_discount: Discount factor for applying win reward bonus
-                backwards through the episode (1.0 = no discount, 0.99 = standard).
-        """
-        self.max_size = max_size
-        self.horizon = horizon
-        self.batch_size = batch_size
-        self.use_torch_tensors = use_torch_tensors or (device != "cpu") or (buffer_device != "cpu")
-        self.device = (
-            device
-            if isinstance(device, str)
-            else (device.type if hasattr(device, "type") else "cpu")
-        )
-        self.buffer_device = (
-            buffer_device
-            if isinstance(buffer_device, str)
-            else (buffer_device.type if hasattr(buffer_device, "type") else "cpu")
-        )
-        self.pin_memory = (
-            pin_memory
-            and self.buffer_device == "cpu"
-            and self.use_torch_tensors
-            and torch.cuda.is_available()
-        )
-        self.multitask = multitask
-        self.win_reward_bonus = win_reward_bonus
-        self.win_reward_discount = win_reward_discount
-
-        self._episodes = []
-        self._total_transitions = 0
-
-        # For store(transition): accumulate into current episode until done
-        self._current_obs = []
-        self._current_actions = []
-        self._current_rewards = []
-        self._current_dones = []
-        self._current_winner = None
-
-    @property
-    def capacity(self):
-        """Maximum number of transitions the buffer can hold."""
-        return self.max_size
-
-    @property
-    def num_eps(self):
-        """Number of episodes in the buffer."""
-        return len(self._episodes)
-
-    @property
-    def size(self):
-        """Current number of transitions in the buffer."""
-        return self._total_transitions
-
-    def _evict_if_needed(self, extra):
-        """Evict oldest episodes until _total_transitions + extra <= max_size."""
-        while self._episodes and self._total_transitions + extra > self.max_size:
-            ep = self._episodes.pop(0)
-            T = ep["action"].shape[0]
-            self._total_transitions -= T
-
-    def _add_episode_internal(
-        self,
-        obs,
-        action,
-        reward,
-        terminated,
-        task=None,
-        winner=None,
-        reward_original=None,
-    ):
-        """Append one episode to _episodes and update _total_transitions."""
-        T = action.shape[0]
-        self._evict_if_needed(T)
-        ep = {
-            "obs": obs,
-            "action": action,
-            "reward": reward,  # This is the backprop reward (used for training)
-            "terminated": terminated,
-            "task": task,
-            "winner": winner,
-            "reward_original": reward_original
-            if reward_original is not None
-            else reward,  # Original reward before backprop
-        }
-        self._episodes.append(ep)
-        self._total_transitions += T
-
-    def _to_buffer_dtype(self, x, is_numpy=None):
-        """Convert to torch or numpy to match buffer config.
-        
-        Data is stored on buffer_device (default CPU), not the training device.
-        """
-        if is_numpy is None:
-            is_numpy = not self.use_torch_tensors
-        if is_numpy:
-            if hasattr(x, "cpu") and callable(getattr(x, "cpu", None)):
-                return np.asarray(x.detach().cpu().numpy(), dtype=np.float32)
-            return np.asarray(x, dtype=np.float32)
-        t = torch.as_tensor(x, dtype=torch.float32)
-        if self.buffer_device != "cpu":
-            t = t.to(self.buffer_device, non_blocking=True)
-        elif self.pin_memory and t.is_cpu and torch.cuda.is_available():
-            t = t.pin_memory()
-        return t
-
-    def store(self, transition, winner=None):
-        """Store a transition. When done is True, the current trajectory is
-        closed and added as an episode.
-
-        Args:
-            transition: (state, action, reward, next_state, done).
-            winner: Optional winner information (1 for agent win, -1 for loss, 0 for draw).
-                Should be provided when done=True to enable reward shaping for wins.
-        """
-        state, action, reward, next_state, done = transition
-
-        if len(self._current_obs) == 0:
-            self._current_obs.append(
-                self._to_buffer_dtype(state)
-                if self.use_torch_tensors
-                else np.asarray(state, dtype=np.float32)
-            )
-        self._current_obs.append(
-            self._to_buffer_dtype(next_state)
-            if self.use_torch_tensors
-            else np.asarray(next_state, dtype=np.float32)
-        )
-        a = (
-            self._to_buffer_dtype(action)
-            if self.use_torch_tensors
-            else np.asarray(action, dtype=np.float32)
-        )
-        r = np.float32(reward)
-        d = np.float32(done)
-        if self.use_torch_tensors:
-            r = torch.as_tensor(r, dtype=torch.float32)
-            d = torch.as_tensor(d, dtype=torch.float32)
-        self._current_actions.append(a)
-        self._current_rewards.append(r)
-        self._current_dones.append(d)
-
-        if done:
-            if winner is not None:
-                self._current_winner = winner
-            self._flush_episode()
-
-    def _flush_episode(self):
-        """Build one episode from _current_* and add it; clear _current_*.
-        If the episode was a win and win_reward_bonus > 0, applies discounted
-        reward bonus backwards through the episode.
-        """
-        if len(self._current_actions) == 0:
-            self._current_obs = []
-            self._current_dones = []
-            self._current_winner = None
-            return
-
-        if self.use_torch_tensors:
-            obs = torch.stack(self._current_obs, dim=0)
-            action = torch.stack(self._current_actions, dim=0)
-            reward = torch.stack(self._current_rewards, dim=0)
-            terminated = torch.stack(self._current_dones, dim=0)
-        else:
-            obs = np.stack(self._current_obs, axis=0)
-            action = np.stack(self._current_actions, axis=0)
-            reward = np.stack(self._current_rewards, axis=0)
-            terminated = np.stack(self._current_dones, axis=0)
-
-        if reward.ndim == 1:
-            reward = reward.reshape(-1, 1)
-        if terminated.ndim == 1:
-            terminated = terminated.reshape(-1, 1)
-
-        # Save original reward before backpropagation
-        if self.use_torch_tensors:
-            reward_original = reward.clone()
-        else:
-            reward_original = reward.copy()
-
-        # Apply reward shaping for winning episodes using the backpropagation function
-        if (
-            self._current_winner == 1
-            and self.win_reward_bonus > 0.0
-            and len(reward) > 0
-        ):
-            # Flatten reward for the function (it expects 1D or will flatten)
-            reward_flat = reward.flatten()
-            # Apply backpropagation
-            reward_flat, _, _ = apply_win_reward_backprop(
-                reward_flat,
-                winner=self._current_winner,
-                win_reward_bonus=self.win_reward_bonus,
-                win_reward_discount=self.win_reward_discount,
-                use_torch=self.use_torch_tensors,
-            )
-            # Reshape back to original shape
-            reward = reward_flat.reshape(reward.shape)
-
-        self._add_episode_internal(
-            obs,
-            action,
-            reward,
-            terminated,
-            task=None,
-            winner=self._current_winner,
-            reward_original=reward_original,
-        )
-        self._current_obs = []
-        self._current_actions = []
-        self._current_rewards = []
-        self._current_dones = []
-        self._current_winner = None
-
-    def add_episode(self, obs, action, reward, terminated, task=None, winner=None):
-        """Add one episode to the buffer.
-
-        Args:
-            obs: (T+1, obs_dim) observations for steps 0..T.
-            action: (T, action_dim) actions for transitions 0..T-1.
-            reward: (T,) or (T, 1) rewards. This should be the backprop reward if backprop was applied.
-            terminated: (T,) or (T, 1) done flags.
-            task: Optional (T, task_dim) or (task_dim,) for multitask; ignored if multitask=False.
-            winner: Optional winner information (1 for agent win, -1 for loss, 0 for draw).
-        """
-        obs = self._to_buffer_dtype(obs)
-        action = self._to_buffer_dtype(action)
-        reward = self._to_buffer_dtype(reward)
-        terminated = self._to_buffer_dtype(terminated)
-        if reward.ndim == 1:
-            reward = reward.reshape(-1, 1)
-        if terminated.ndim == 1:
-            terminated = terminated.reshape(-1, 1)
-        t = task
-        if t is not None and self.multitask:
-            t = self._to_buffer_dtype(t)
-            if t.ndim == 1:
-                t = t.reshape(1, -1).expand(action.shape[0], -1)
-        else:
-            t = None
-        # For add_episode, assume reward is already backprop if needed, so use same as original
-        # (This method is typically used for batch loading where backprop may have been applied externally)
-        reward_original = reward.clone() if self.use_torch_tensors else reward.copy()
-        self._add_episode_internal(
-            obs,
-            action,
-            reward,
-            terminated,
-            task=t,
-            winner=winner,
-            reward_original=reward_original,
-        )
-
-    def add_episodes(self, episodes):
-        """Add multiple episodes (batch load).
-
-        Args:
-            episodes: List of dicts with keys obs, action, reward, terminated, task (optional), winner (optional).
-        """
-        for ep in episodes:
-            self.add_episode(
-                ep["obs"],
-                ep["action"],
-                ep["reward"],
-                ep["terminated"],
-                ep.get("task"),
-                ep.get("winner"),
-            )
-
-    def _episodes_with_length_at_least(self, H):
-        """Return list of (ep, start) for episodes that have at least H transitions
-        and a valid start index; for each, start is in [0, T-H].
-        """
-        out = []
-        for ep in self._episodes:
-            T = ep["action"].shape[0]
-            if T >= H:
-                for start in range(0, T - H + 1):
-                    out.append((ep, start))
-        return out
-
-    def sample(self, batch_size=None, horizon=None, return_original_reward=False):
-        """Sample a batch of subsequences.
-
-        Returns
-        -------
-        tuple
-            (obs, action, reward, terminated, task) with shapes
-            (B, H+1, obs_dim), (B, H, action_dim), (B, H, 1), (B, H, 1),
-            and (B, task_dim) or None if not multitask.
-
-        Raises
-        ------
-        RuntimeError
-            If no episode has length >= horizon or buffer is empty.
-        """
-        B = batch_size if batch_size is not None else self.batch_size
-        H = horizon if horizon is not None else self.horizon
-
-        candidates = self._episodes_with_length_at_least(H)
-        if len(candidates) == 0:
-            raise RuntimeError(
-                "TDMPC2ReplayBuffer.sample: no episode has length >= horizon. "
-                "Add episodes or reduce horizon."
-            )
-
-        # Sample B (ep, start) with replacement
-        inds = random.choices(range(len(candidates)), k=B)
-        chunks = [candidates[i] for i in inds]
-
-        obs_list = []
-        action_list = []
-        reward_list = []
-        terminated_list = []
-        task_list = []
-
-        for ep, start in chunks:
-            obs_list.append(ep["obs"][start : start + H + 1])
-            action_list.append(ep["action"][start : start + H])
-
-            if return_original_reward:
-                reward_list.append(ep["reward_original"][start : start + H])
-            else:
-                reward_list.append(ep["reward"][start : start + H])
-            terminated_list.append(ep["terminated"][start : start + H])
-            if self.multitask and ep.get("task") is not None:
-                task_list.append(ep["task"][start])
-
-        if self.use_torch_tensors:
-            obs = torch.stack(obs_list, dim=0)
-            action = torch.stack(action_list, dim=0)
-            reward = torch.stack(reward_list, dim=0)
-            terminated = torch.stack(terminated_list, dim=0)
-            if reward.dim() == 2:
-                reward = reward.unsqueeze(-1)
-            if terminated.dim() == 2:
-                terminated = terminated.unsqueeze(-1)
-            obs = obs.to(self.device, non_blocking=True)
-            action = action.to(self.device, non_blocking=True)
-            reward = reward.to(self.device, non_blocking=True)
-            terminated = terminated.to(self.device, non_blocking=True)
-            if task_list:
-                task = torch.stack(task_list, dim=0).to(self.device, non_blocking=True)
-            else:
-                task = None
-        else:
-            obs = np.stack(obs_list, axis=0)
-            action = np.stack(action_list, axis=0)
-            reward = np.stack(reward_list, axis=0)
-            terminated = np.stack(terminated_list, axis=0)
-            if reward.ndim == 2:
-                reward = reward.reshape(*reward.shape, 1)
-            if terminated.ndim == 2:
-                terminated = terminated.reshape(*terminated.shape, 1)
-            task = np.stack(task_list, axis=0) if task_list else None
-
-        return obs, action, reward, terminated, task
-
-    def sample_sequences(self, batch_size, horizon):
-        """Sample contiguous sequences for TD-MPC2 train. Same as sample but
-        returns a 4-tuple (obs, action, reward, terminated) so it can replace
-        ReplayBuffer.sample_sequences in the train loop.
-
-        Returns
-        -------
-        tuple or None
-            (obs_seq, action_seq, reward_seq, done_seq) with shapes
-            (batch, horizon+1, obs_dim), (batch, horizon, action_dim),
-            (batch, horizon, 1), (batch, horizon, 1). Returns None if no
-            episode has length >= horizon.
-        """
-        candidates = self._episodes_with_length_at_least(horizon)
-        if len(candidates) == 0:
-            return None
-        obs, action, reward, terminated, _ = self.sample(
-            batch_size=batch_size, horizon=horizon
-        )
-        return obs, action, reward, terminated
-
-    def clear(self):
-        """Clear the buffer and any in-progress episode from store()."""
-        self._episodes = []
-        self._total_transitions = 0
-        self._current_obs = []
-        self._current_actions = []
-        self._current_rewards = []
-        self._current_dones = []
-        self._current_winner = None
-
-
-class PrioritizedReplayBuffer(ReplayBuffer):
-    """Prioritized Experience Replay buffer.
-
-    Samples transitions based on their TD error priorities.
-    Uses importance sampling weights to correct for the bias introduced
-    by non-uniform sampling.
-    """
-
-    def __init__(
-        self,
-        max_size=1_000_000,
-        alpha=0.6,
-        beta=0.4,
-        beta_increment=0.0001,
-        max_beta=1.0,
-        eps=1e-6,
-    ):
-        """Initialize Prioritized Replay Buffer.
-
-        Parameters
-        ----------
-        max_size : int
-            Maximum size of the buffer
-        alpha : float
-            Priority exponent (0 = uniform, 1 = fully prioritized)
-        beta : float
-            Initial importance sampling exponent
-        beta_increment : float
-            How much to increment beta per sample
-        max_beta : float
-            Maximum value for beta
-        eps : float
-            Small constant to ensure priorities are never zero
-        """
-        super().__init__(max_size)
-        self.alpha = alpha
-        self.beta = beta
-        self.beta_increment = beta_increment
-        self.max_beta = max_beta
-        self.eps = eps
-        self.max_priority = 1.0
-
-        # Calculate tree capacity (must be power of 2)
-        tree_capacity = 1
-        while tree_capacity < max_size:
-            tree_capacity *= 2
-
-        self.sum_tree = SumSegmentTree(tree_capacity)
-        self.min_tree = MinSegmentTree(tree_capacity)
-
-    def store(self, transition):
-        """Store a transition with maximum priority."""
-        idx = self.current_idx
-        super().store(transition)
-
-        # Initialize with maximum priority
-        priority = (self.max_priority + self.eps) ** self.alpha
-        self.sum_tree[idx] = priority
-        self.min_tree[idx] = priority
-
-    def sample(self, batch_size):
-        """Sample a batch of transitions with priorities.
-
-        Returns
-        -------
-        tuple
-            (state, action, reward, next_state, done, weights, indices)
-            where weights are importance sampling weights and indices
-            are the buffer indices for updating priorities later.
-        """
-        if batch_size > self.size:
-            batch_size = self.size
-
-        if batch_size == 0:
-            # Return empty batch
-            return (
-                np.array([]),
-                np.array([]),
-                np.array([]),
-                np.array([]),
-                np.array([]),
-                np.array([]),
-                [],
-            )
-
-        # Increment beta
-        self.beta = min(self.max_beta, self.beta + self.beta_increment)
-
-        # Sample indices based on priorities
-        indices = []
-        priorities = []
-
-        total_priority = self.sum_tree.sum(0, self.size - 1)
-        # Safety check for zero priority
-        if total_priority <= 0:
-            total_priority = self.eps * self.size
-
-        segment_length = total_priority / batch_size
-
-        for i in range(batch_size):
-            prefixsum = (i + random.random()) * segment_length
-            idx = self.sum_tree.find_prefixsum_idx(prefixsum)
-            # Clamp to valid range (shouldn't be necessary but safety check)
-            idx = min(idx, self.size - 1)
-            indices.append(idx)
-            priorities.append(self.sum_tree[idx])
-
-        # Compute importance sampling weights
-        min_priority = self.min_tree.min(0, self.size - 1) / total_priority
-        max_weight = (min_priority * self.size) ** (-self.beta)
-
-        weights = []
-        for priority in priorities:
-            prob = priority / total_priority
-            weight = (prob * self.size) ** (-self.beta)
-            weights.append(weight / max_weight)
-
-        weights = np.array(weights, dtype=np.float32)
-
-        # Get the actual transitions
-        state = self.state[indices]
-        action = self.action[indices]
-        reward = self.reward[indices]
-        next_state = self.next_state[indices]
-        done = self.done[indices]
-
-        return (state, action, reward, next_state, done, weights, indices)
-
-    def update_priorities(self, indices, priorities):
-        """Update priorities for sampled transitions.
-
-        Parameters
-        ----------
-        indices : array-like
-            Buffer indices of transitions
-        priorities : array-like
-            New priorities (typically TD errors)
-        """
-        priorities = np.abs(priorities) + self.eps
-        self.max_priority = max(self.max_priority, priorities.max())
-
-        for idx, priority in zip(indices, priorities):
-            priority_alpha = priority**self.alpha
-            self.sum_tree[idx] = priority_alpha
-            self.min_tree[idx] = priority_alpha
-
-    def clear(self):
-        """Clear the replay buffer, resetting it to empty state."""
-        super().clear()
-        # Reset trees (they will be re-initialized on next store)
-        tree_capacity = 1
-        while tree_capacity < self.max_size:
-            tree_capacity *= 2
-        self.sum_tree = SumSegmentTree(tree_capacity)
-        self.min_tree = MinSegmentTree(tree_capacity)
-        self.max_priority = 1.0
+        # Keep arrays allocated but mark as empty
\ No newline at end of file
diff --git a/src/rl_hockey/common/evaluation/agent_evaluator.py b/src/rl_hockey/common/evaluation/agent_evaluator.py
index e361bd0..2f79ff4 100644
--- a/src/rl_hockey/common/evaluation/agent_evaluator.py
+++ b/src/rl_hockey/common/evaluation/agent_evaluator.py
@@ -1,164 +1,53 @@
 import multiprocessing as mp
-import os
-from pathlib import Path
-from typing import Any, Dict, Optional, Tuple, Union
-
-import hockey.hockey_env as h_env
 import numpy as np
-import torch
+from typing import Dict, Any, Tuple, Optional, Union
+import hockey.hockey_env as h_env
 
 from rl_hockey.common.training.agent_factory import create_agent, get_action_space_info
 from rl_hockey.common.training.curriculum_manager import AgentConfig, load_curriculum
-from rl_hockey.common.utils import (
-    discrete_to_continuous_action_with_fineness,
-    get_discrete_action_dim,
-    set_cuda_device,
-)
-
-
-def find_config_from_model_path(model_path: str) -> Optional[str]:
-    """
-    Try to find the config file automatically from the model path.
-    Looks for configs in the same directory structure.
-    """
-    model_path_obj = Path(model_path)
-
-    # Try to find config in the same run directory
-    # Model path: .../run_name/timestamp/models/model.pt
-    # Config path: .../run_name/timestamp/configs/run_name.json
-    if model_path_obj.parent.name == "models":
-        run_dir = model_path_obj.parent.parent
-        configs_dir = run_dir / "configs"
-        if configs_dir.exists():
-            # Try to find config file with same name as model (without extension)
-            model_name = model_path_obj.stem
-            config_file = configs_dir / f"{model_name}.json"
-            if config_file.exists():
-                return str(config_file)
-
-            # Try to find any config file in the directory
-            config_files = list(configs_dir.glob("*.json"))
-            if config_files:
-                return str(config_files[0])
-
-    return None
-
-
-def infer_fineness_from_action_dim(
-    action_dim: int, keep_mode: bool = True
-) -> Optional[int]:
-    """
-    Infer the action_fineness parameter from the action dimension.
-    Returns None if it doesn't match a known fineness pattern.
-    """
-    # Try common fineness values: 3, 5, 7, 9, 11, 13, 15, etc.
-    for fineness in [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]:
-        expected_dim = get_discrete_action_dim(fineness=fineness, keep_mode=keep_mode)
-        if expected_dim == action_dim:
-            return fineness
-    return None
-
-
-def load_agent_params_from_checkpoint(checkpoint_path: str) -> Dict[str, Any]:
-    """
-    Load agent parameters from checkpoint file.
-    Returns a dictionary with available parameters.
-    """
-    try:
-        checkpoint = torch.load(checkpoint_path, map_location="cpu")
-        params = {}
-
-        if "action_dim" in checkpoint:
-            params["action_dim"] = checkpoint["action_dim"]
-        if "state_dim" in checkpoint:
-            params["state_dim"] = checkpoint["state_dim"]
-        if "config" in checkpoint:
-            params["config"] = checkpoint["config"]
-        if "hidden_dim" in checkpoint:
-            params["hidden_dim"] = checkpoint["hidden_dim"]
-
-        return params
-    except Exception:
-        return {}
-
-
-def _pool_initializer():
-    """Initialize worker processes with a safe working directory."""
-    # Change to a safe directory that always exists
-    # This prevents FileNotFoundError when multiprocessing tries to get cwd
-    try:
-        os.chdir("/tmp")
-    except OSError:
-        # If /tmp doesn't work, try home directory
-        try:
-            os.chdir(os.path.expanduser("~"))
-        except OSError:
-            pass  # If all else fails, let it use current directory
-
+from rl_hockey.common.utils import discrete_to_continuous_action_with_fineness, set_cuda_device
 
 def run_single_game(args: Tuple) -> Dict[str, Any]:
     """
     Run a single game between an agent and a weak opponent.
-
+    
     Args:
         args: Tuple containing agent path, agent configuration dictionary, weak opponent flag, maximum steps, random seed, and device
     Returns:
         Dictionary containing game results
     """
     agent_path, agent_config_dict, weak_opponent, max_steps, seed, device = args
-
-    # Verify checkpoint file exists before trying to load
-    # Note: agent_path should already be absolute from the parent process
-    if not os.path.exists(agent_path):
-        current_cwd = "unknown"
-        try:
-            current_cwd = os.getcwd()
-        except (OSError, FileNotFoundError):
-            pass
-        raise FileNotFoundError(
-            f"Agent checkpoint not found: {agent_path}\n"
-            f"Current working directory: {current_cwd}\n"
-            f"File is absolute: {os.path.isabs(agent_path)}"
-        )
-
+    
     set_cuda_device(device)
     np.random.seed(seed)
 
     env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)
-    action_fineness = agent_config_dict.get("hyperparameters", {}).get(
-        "action_fineness", None
-    )
-    state_dim, action_dim, is_discrete = get_action_space_info(
-        env, agent_config_dict["type"], fineness=action_fineness
-    )
-
+    action_fineness = agent_config_dict.get('hyperparameters', {}).get('action_fineness', None)
+    state_dim, action_dim, is_discrete = get_action_space_info(env, agent_config_dict['type'], fineness=action_fineness)
+    
+    from rl_hockey.common.training.curriculum_manager import AgentConfig
+    
     agent_config = AgentConfig(
-        type=agent_config_dict["type"],
-        hyperparameters=agent_config_dict["hyperparameters"],
+        type=agent_config_dict['type'],
+        hyperparameters=agent_config_dict['hyperparameters']
     )
     agent = create_agent(agent_config, state_dim, action_dim, {})
     agent.load(agent_path)
-
-    if hasattr(agent, "q_network"):
+    
+    if hasattr(agent, 'q_network'):
         agent.q_network.eval()
-    if hasattr(agent, "q_network_target"):
+    if hasattr(agent, 'q_network_target'):
         agent.q_network_target.eval()
-    if hasattr(agent, "actor") and hasattr(agent.actor, "eval"):
+    if hasattr(agent, 'actor') and hasattr(agent.actor, 'eval'):
         agent.actor.eval()
-    if hasattr(agent, "critic1") and hasattr(agent.critic1, "eval"):
+    if hasattr(agent, 'critic1') and hasattr(agent.critic1, 'eval'):
         agent.critic1.eval()
-    # For TD3
-    if hasattr(agent, "actor_target") and hasattr(agent.actor_target, "eval"):
-        agent.actor_target.eval()
-    if hasattr(agent, "critic") and hasattr(agent.critic, "eval"):
-        agent.critic.eval()
-    if hasattr(agent, "critic_target") and hasattr(agent.critic_target, "eval"):
-        agent.critic_target.eval()
+    
     opponent = h_env.BasicOpponent(weak=weak_opponent)
     state, _ = env.reset()
     obs_agent2 = env.obs_agent_two()
     total_reward = 0
-
+    
     for step in range(max_steps):
         if is_discrete:
             discrete_action = agent.act(state.astype(np.float32), deterministic=True)
@@ -178,12 +67,15 @@ def run_single_game(args: Tuple) -> Dict[str, Any]:
         obs_agent2 = env.obs_agent_two()
         if done or trunc:
             break
-
-    winner = info.get("winner", 0)
+    
+    winner = info.get('winner', 0)
     env.close()
-
-    return {"winner": winner, "reward": total_reward, "steps": step + 1}
-
+    
+    return {
+        'winner': winner,
+        'reward': total_reward,
+        'steps': step + 1
+    }
 
 def evaluate_agent(
     agent_path: str,
@@ -193,222 +85,81 @@ def evaluate_agent(
     weak_opponent: bool = True,
     max_steps: int = 250,
     num_parallel: int = None,
-    device: Optional[Union[str, int]] = None,
-    use_cpu_for_eval: bool = False,
+    device: Optional[Union[str, int]] = None
 ) -> Dict[str, Any]:
     """
     Evaluate an agent against a weak opponent.
-
+    
     Args:
         agent_path: Path to agent checkpoint
-        config_path: Path to curriculum configuration JSON file (if None, will try to auto-detect from model path)
-        agent_config_dict: Dictionary containing agent configuration (if None, will try to load from config or checkpoint)
+        config_path: Path to curriculum configuration JSON file
+        agent_config_dict: Dictionary containing agent configuration
         num_games: Number of games to evaluate
         weak_opponent: Whether to use weak opponent
         max_steps: Maximum number of steps per game
         num_parallel: Number of parallel processes to use
         device: CUDA device to use (None = auto-detect, 'cpu' = CPU, 'cuda' = cuda:0, 'cuda:0' = first GPU, 'cuda:1' = second GPU, etc.). Can also be an integer (0, 1, etc.) for device ID.
-        use_cpu_for_eval: If True, force CPU usage for evaluation to avoid GPU memory issues with parallel processes
     Returns:
         Dictionary containing evaluation results
     """
-    # Auto-detect config_path if not provided
-    if config_path is None:
-        detected_config = find_config_from_model_path(agent_path)
-        if detected_config is not None:
-            config_path = detected_config
-
-    # Load config if available
     if config_path is not None:
         curriculum = load_curriculum(config_path)
         agent_config_dict = {
-            "type": curriculum.agent.type,
-            "hyperparameters": curriculum.agent.hyperparameters,
+            'type': curriculum.agent.type,
+            'hyperparameters': curriculum.agent.hyperparameters
         }
-
-    # If still no config, try to load from checkpoint and infer parameters
-    if agent_config_dict is None:
-        checkpoint_params = load_agent_params_from_checkpoint(agent_path)
-
-        # Try to infer agent type from checkpoint or default to DDDQN
-        # Check if checkpoint has q_network (DDDQN) or actor (SAC/PPO)
-        agent_type = "DDDQN"  # Default fallback
-        if "config" in checkpoint_params:
-            agent_type = checkpoint_params["config"].get("type", "DDDQN")
-
-        # Try to infer action_fineness from action_dim
-        action_fineness = None
-        if "action_dim" in checkpoint_params:
-            action_dim = checkpoint_params["action_dim"]
-            # Try both keep_mode=True and keep_mode=False
-            action_fineness = infer_fineness_from_action_dim(action_dim, keep_mode=True)
-            if action_fineness is None:
-                action_fineness = infer_fineness_from_action_dim(
-                    action_dim, keep_mode=False
-                )
-
-        # Build hyperparameters dict
-        hyperparameters = {}
-        if action_fineness is not None:
-            hyperparameters["action_fineness"] = action_fineness
-
-        # Add other hyperparameters from checkpoint config if available
-        if "config" in checkpoint_params:
-            checkpoint_config = checkpoint_params["config"]
-            # Copy relevant hyperparameters
-            for key in [
-                "hidden_dim",
-                "target_update_freq",
-                "eps",
-                "eps_min",
-                "eps_decay",
-                "use_huber_loss",
-            ]:
-                if key in checkpoint_config:
-                    hyperparameters[key] = checkpoint_config[key]
-
-        agent_config_dict = {"type": agent_type, "hyperparameters": hyperparameters}
-
-    # Final check: if action_fineness is still missing, try to infer from checkpoint
-    if agent_config_dict.get("hyperparameters", {}).get("action_fineness") is None:
-        checkpoint_params = load_agent_params_from_checkpoint(agent_path)
-        if "action_dim" in checkpoint_params:
-            action_dim = checkpoint_params["action_dim"]
-            action_fineness = infer_fineness_from_action_dim(action_dim, keep_mode=True)
-            if action_fineness is None:
-                action_fineness = infer_fineness_from_action_dim(
-                    action_dim, keep_mode=False
-                )
-            if action_fineness is not None:
-                if "hyperparameters" not in agent_config_dict:
-                    agent_config_dict["hyperparameters"] = {}
-                agent_config_dict["hyperparameters"]["action_fineness"] = (
-                    action_fineness
-                )
-
-    # Override device to CPU if requested
-    if use_cpu_for_eval:
-        device = "cpu"
-
+    elif agent_config_dict is None:
+        raise ValueError("Either config_path or agent_config_dict must be provided")
+    
+    
     if num_parallel is None:
-        # Limit parallel processes to avoid GPU memory exhaustion
-        # Each process loads the model, so we cap at a reasonable number
-        # Use CPU count for CPU evaluation, but limit to 3 for GPU evaluation
-        if (
-            device is not None
-            and device != "cpu"
-            and (
-                isinstance(device, str)
-                and "cuda" in str(device)
-                or isinstance(device, int)
-            )
-        ):
-            # GPU evaluation: limit to 3 parallel processes to avoid OOM
-            num_parallel = min(3, mp.cpu_count(), num_games)
-        else:
-            # CPU evaluation: can use more processes
-            num_parallel = min(mp.cpu_count(), num_games)
-
-    # Save current working directory and convert agent_path to absolute path BEFORE changing directories
-    # This ensures worker processes can find the checkpoint file even after we change cwd
-    original_cwd = None
-    try:
-        original_cwd = os.getcwd()
-    except (OSError, FileNotFoundError):
-        original_cwd = None
-
-    # Convert agent_path to absolute path BEFORE changing directories
-    # This ensures worker processes can find the checkpoint file even after we change cwd
-    if not os.path.isabs(agent_path):
-        if original_cwd is not None:
-            # We have a valid cwd, use os.path.join with cwd to make absolute
-            agent_path = os.path.normpath(os.path.join(original_cwd, agent_path))
-        else:
-            # No valid cwd, try Path.resolve() which doesn't require file to exist
-            try:
-                agent_path = str(Path(agent_path).resolve())
-            except (OSError, RuntimeError):
-                # If that fails, try to expand user and resolve
-                agent_path = os.path.normpath(os.path.expanduser(agent_path))
-
-    # Change to a safe directory that always exists BEFORE creating Pool
-    # This prevents FileNotFoundError when multiprocessing spawns workers
-    # in SLURM/Singularity environments where the working directory might not be accessible
-    safe_dir = "/tmp"
-    try:
-        home_dir = os.path.expanduser("~")
-        if os.path.exists(home_dir):
-            safe_dir = home_dir
-        else:
-            safe_dir = "/tmp"
-        os.chdir(safe_dir)
-    except (OSError, FileNotFoundError):
-        # If we can't change directory, try to continue
-        pass
-
+        num_parallel = min(mp.cpu_count(), num_games)
+    
+    
     try:
-        mp.set_start_method("spawn", force=True)
+        mp.set_start_method('spawn', force=True)
     except RuntimeError:
         pass
-
+    
+    
     seeds = np.random.randint(0, 2**31, size=num_games)
     args_list = [
         (agent_path, agent_config_dict, weak_opponent, max_steps, int(seed), device)
         for seed in seeds
     ]
-
+    
     results = []
-    try:
-        if num_parallel > 1:
-            # Use initializer to set a safe working directory for worker processes
-            # This prevents FileNotFoundError when multiprocessing spawns workers
-            with mp.Pool(processes=num_parallel, initializer=_pool_initializer) as pool:
-                results = pool.map(run_single_game, args_list)
-        else:
-            results = [run_single_game(args) for args in args_list]
-    finally:
-        # Restore original working directory if we changed it
-        if original_cwd is not None:
-            try:
-                os.chdir(original_cwd)
-            except (OSError, FileNotFoundError):
-                pass
-
-    wins = sum(1 for r in results if r["winner"] == 1)
-    losses = sum(1 for r in results if r["winner"] == -1)
-    draws = sum(1 for r in results if r["winner"] == 0)
-    rewards = [r["reward"] for r in results]
+    if num_parallel > 1:
+        with mp.Pool(processes=num_parallel) as pool:
+            results = pool.map(run_single_game, args_list)
+    else:
+        results = [run_single_game(args) for args in args_list]
+    
+    wins = sum(1 for r in results if r['winner'] == 1)
+    losses = sum(1 for r in results if r['winner'] == -1)
+    draws = sum(1 for r in results if r['winner'] == 0)
+    rewards = [r['reward'] for r in results]
     mean_reward = np.mean(rewards)
     std_reward = np.std(rewards)
     win_rate = wins / num_games if num_games > 0 else 0.0
-
+    
+    
     return {
-        "wins": wins,
-        "losses": losses,
-        "draws": draws,
-        "win_rate": win_rate,
-        "mean_reward": mean_reward,
-        "std_reward": std_reward,
-        "num_games": num_games,
-        "weak_opponent": weak_opponent,
-        "all_rewards": rewards,
-        "all_winners": [r["winner"] for r in results],
+        'wins': wins,
+        'losses': losses,
+        'draws': draws,
+        'win_rate': win_rate,
+        'mean_reward': mean_reward,
+        'std_reward': std_reward,
+        'num_games': num_games,
+        'weak_opponent': weak_opponent,
+        'all_rewards': rewards,
+        'all_winners': [r['winner'] for r in results]
     }
 
-
 if __name__ == "__main__":
     try:
-        mp.set_start_method("spawn", force=True)
+        mp.set_start_method('spawn', force=True)
     except RuntimeError:
         pass
-    # Now config_path can be None - it will auto-detect from model path
-    print(
-        evaluate_agent(
-            agent_path="results/hyperparameter_runs/2026-01-03_18-24-14/run_lr1e04_bs256_h512_512_512_31fb74b2_0002/2026-01-03_18-24-17/models/run_lr1e04_bs256_h512_512_512_31fb74b2_0002.pt",
-            config_path=None,
-            num_games=250,
-            weak_opponent=False,
-            max_steps=250,
-            num_parallel=None,
-        )
-    )
+    print(evaluate_agent(agent_path="results/hyperparameter_runs/2026-01-03_13-43-03/models/run_lr5e04_bs512_h256_512_256_7e411fc0_20260103_134303_vec8_ep045000.pt", config_path="configs/curriculum_simple.json", num_games=100, weak_opponent=True, max_steps=250, num_parallel=None))
\ No newline at end of file
diff --git a/src/rl_hockey/common/evaluation/benchmark_per.py b/src/rl_hockey/common/evaluation/benchmark_per.py
deleted file mode 100644
index b716463..0000000
--- a/src/rl_hockey/common/evaluation/benchmark_per.py
+++ /dev/null
@@ -1,117 +0,0 @@
-"""
-Benchmark script to compare training speed between DDDQN and DDQN_PER.
-"""
-
-import json
-import time
-from pathlib import Path
-
-from rl_hockey.common.training.train_single_run import train_single_run
-
-
-def benchmark_agent(config_path, agent_name, num_runs=1):
-    """Benchmark a single agent configuration."""
-    print(f"\n{'=' * 60}")
-    print(f"Benchmarking: {agent_name}")
-    print(f"{'=' * 60}")
-
-    times = []
-
-    for run_idx in range(num_runs):
-        print(f"\nRun {run_idx + 1}/{num_runs}")
-
-        start_time = time.time()
-
-        try:
-            train_single_run(
-                config_path=config_path,
-                base_output_dir="results/benchmark",
-                run_name=f"{agent_name}_run_{run_idx}",
-                verbose=False,
-                num_envs=1,
-                device="cpu",
-            )
-
-            elapsed = time.time() - start_time
-            times.append(elapsed)
-            print(f"  Completed in {elapsed:.2f} seconds")
-
-        except Exception as e:
-            print(f"  Error: {e}")
-            continue
-
-    if times:
-        avg_time = sum(times) / len(times)
-        print(f"\nAverage time: {avg_time:.2f} seconds")
-        print(f"Times: {times}")
-        return avg_time, times
-    else:
-        return None, None
-
-
-def main():
-    """Run benchmark comparison."""
-    print("PER vs Standard DDDQN Benchmark")
-    print("=" * 60)
-
-    # Config files
-    config_standard = "configs/curriculum_test.json"
-    config_per = "configs/curriculum_test_per.json"
-
-    # Verify configs exist
-    if not Path(config_standard).exists():
-        print(f"Error: {config_standard} not found")
-        return
-
-    if not Path(config_per).exists():
-        print(f"Error: {config_per} not found")
-        return
-
-    num_runs = 1  # Number of runs per agent
-
-    # Benchmark standard DDDQN
-    std_avg, std_times = benchmark_agent(config_standard, "DDDQN_Standard", num_runs)
-
-    # Benchmark DDQN_PER
-    per_avg, per_times = benchmark_agent(config_per, "DDQN_PER", num_runs)
-
-    # Compare results
-    print(f"\n{'=' * 60}")
-    print("BENCHMARK RESULTS")
-    print(f"{'=' * 60}")
-
-    if std_avg and per_avg:
-        print(f"\nStandard DDDQN:  {std_avg:.2f} seconds")
-        print(f"DDQN with PER:   {per_avg:.2f} seconds")
-
-        speedup = std_avg / per_avg
-        slowdown = per_avg / std_avg
-
-        if speedup > 1:
-            print(f"\nPER is {speedup:.2f}x FASTER")
-        else:
-            print(f"\nPER is {slowdown:.2f}x SLOWER")
-
-        overhead = ((per_avg - std_avg) / std_avg) * 100
-        print(f"PER overhead: {overhead:+.1f}%")
-
-        # Save results
-        results = {
-            "standard_ddqn": {"average_time": std_avg, "times": std_times},
-            "ddqn_per": {"average_time": per_avg, "times": per_times},
-            "overhead_percent": overhead,
-            "speedup_factor": speedup if speedup > 1 else slowdown,
-        }
-
-        results_path = "results/benchmark_results.json"
-        Path(results_path).parent.mkdir(parents=True, exist_ok=True)
-        with open(results_path, "w") as f:
-            json.dump(results, f, indent=2)
-
-        print(f"\nResults saved to: {results_path}")
-    else:
-        print("\nBenchmark incomplete - some runs failed")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/src/rl_hockey/common/evaluation/benchmark_per_simple.py b/src/rl_hockey/common/evaluation/benchmark_per_simple.py
deleted file mode 100644
index 4e4d909..0000000
--- a/src/rl_hockey/common/evaluation/benchmark_per_simple.py
+++ /dev/null
@@ -1,164 +0,0 @@
-"""
-Simple benchmark to compare training step speed between DDDQN and DDQN_PER.
-Measures only the training step time, not full episode execution.
-"""
-
-import time
-import numpy as np
-import torch
-import hockey.hockey_env as h_env
-
-from rl_hockey.DDDQN import DDDQN, DDQN_PER
-from rl_hockey.common.utils import get_discrete_action_dim
-
-
-def fill_buffer(agent, num_transitions=10000):
-    """Fill the agent's buffer with random transitions."""
-    env = h_env.HockeyEnv(mode=h_env.Mode.TRAIN_SHOOTING, keep_mode=True)
-    state, _ = env.reset()
-    state_dim = env.observation_space.shape[0]
-    action_dim = 7 if not env.keep_mode else 8
-    
-    from rl_hockey.common.utils import discrete_to_continuous_action_standard
-    
-    for _ in range(num_transitions):
-        discrete_action = np.random.randint(0, action_dim)
-        # Convert to continuous action for player 1
-        action_p1 = discrete_to_continuous_action_standard(discrete_action, keep_mode=env.keep_mode)
-        # Player 2 does nothing
-        action_p2 = np.zeros(4 if env.keep_mode else 3, dtype=np.float32)
-        # Concatenate actions
-        full_action = np.hstack([action_p1, action_p2])
-        
-        next_state, reward, done, truncated, _ = env.step(full_action)
-        
-        agent.store_transition((state, np.array([discrete_action]), reward, next_state, done))
-        
-        if done or truncated:
-            state, _ = env.reset()
-        else:
-            state = next_state
-    
-    env.close()
-
-
-def benchmark_training_steps(agent, num_steps=1000, batch_size=256):
-    """Benchmark training step execution time."""
-    times = []
-    
-    for _ in range(num_steps):
-        start = time.time()
-        agent.train(steps=1)
-        elapsed = time.time() - start
-        times.append(elapsed)
-    
-    return times
-
-
-def main():
-    """Run benchmark comparison."""
-    print("PER vs Standard DDDQN Training Step Benchmark")
-    print("="*60)
-    
-    # Create environment to get dimensions
-    env = h_env.HockeyEnv(mode=h_env.Mode.TRAIN_SHOOTING, keep_mode=True)
-    state_dim = env.observation_space.shape[0]
-    action_dim = 7 if not env.keep_mode else 8
-    env.close()
-    
-    # Configuration
-    num_transitions = 10000
-    num_training_steps = 500
-    batch_size = 256
-    
-    print(f"\nBuffer size: {num_transitions} transitions")
-    print(f"Training steps: {num_training_steps}")
-    print(f"Batch size: {batch_size}")
-    
-    # Create standard DDDQN
-    print(f"\n{'='*60}")
-    print("Setting up Standard DDDQN...")
-    agent_standard = DDDQN(
-        state_dim=state_dim,
-        action_dim=action_dim,
-        hidden_dim=[256, 256],
-        batch_size=batch_size
-    )
-    fill_buffer(agent_standard, num_transitions)
-    print("Buffer filled")
-    
-    # Benchmark standard DDDQN
-    print("\nBenchmarking Standard DDDQN training steps...")
-    std_times = benchmark_training_steps(agent_standard, num_training_steps, batch_size)
-    std_avg = np.mean(std_times)
-    std_std = np.std(std_times)
-    print(f"Average: {std_avg*1000:.2f} ms ± {std_std*1000:.2f} ms per step")
-    print(f"Total: {sum(std_times):.2f} seconds")
-    
-    # Create DDQN_PER
-    print(f"\n{'='*60}")
-    print("Setting up DDQN_PER...")
-    agent_per = DDQN_PER(
-        state_dim=state_dim,
-        action_dim=action_dim,
-        hidden_dim=[256, 256],
-        use_per=True,
-        batch_size=batch_size,
-        per_alpha=0.6,
-        per_beta=0.4
-    )
-    fill_buffer(agent_per, num_transitions)
-    print("Buffer filled")
-    
-    # Benchmark DDQN_PER
-    print("\nBenchmarking DDQN_PER training steps...")
-    per_times = benchmark_training_steps(agent_per, num_training_steps, batch_size)
-    per_avg = np.mean(per_times)
-    per_std = np.std(per_times)
-    print(f"Average: {per_avg*1000:.2f} ms ± {per_std*1000:.2f} ms per step")
-    print(f"Total: {sum(per_times):.2f} seconds")
-    
-    # Compare results
-    print(f"\n{'='*60}")
-    print("BENCHMARK RESULTS")
-    print(f"{'='*60}")
-    
-    slowdown = per_avg / std_avg
-    overhead = ((per_avg - std_avg) / std_avg) * 100
-    
-    print(f"\nStandard DDDQN:  {std_avg*1000:.2f} ms ± {std_std*1000:.2f} ms per step")
-    print(f"DDQN with PER:   {per_avg*1000:.2f} ms ± {per_std*1000:.2f} ms per step")
-    print(f"\nPER is {slowdown:.2f}x SLOWER")
-    print(f"PER overhead: {overhead:+.1f}%")
-    
-    # Save results
-    results = {
-        "standard_ddqn": {
-            "avg_time_ms": std_avg * 1000,
-            "std_time_ms": std_std * 1000,
-            "total_time_sec": sum(std_times)
-        },
-        "ddqn_per": {
-            "avg_time_ms": per_avg * 1000,
-            "std_time_ms": per_std * 1000,
-            "total_time_sec": sum(per_times)
-        },
-        "slowdown_factor": slowdown,
-        "overhead_percent": overhead,
-        "num_training_steps": num_training_steps,
-        "batch_size": batch_size,
-        "buffer_size": num_transitions
-    }
-    
-    import json
-    from pathlib import Path
-    results_path = "results/benchmark_per_results.json"
-    Path(results_path).parent.mkdir(parents=True, exist_ok=True)
-    with open(results_path, 'w') as f:
-        json.dump(results, f, indent=2)
-    
-    print(f"\nResults saved to: {results_path}")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/src/rl_hockey/common/prioritizedbuffer.py b/src/rl_hockey/common/prioritizedbuffer.py
deleted file mode 100644
index 8d7de1f..0000000
--- a/src/rl_hockey/common/prioritizedbuffer.py
+++ /dev/null
@@ -1,66 +0,0 @@
-import random
-import numpy as np
-from rl_hockey.common.buffer import ReplayBuffer
-from rl_hockey.common.sumtree import SumTree
-
-
-class PERMemory(ReplayBuffer):  # stored as ( s, a, r, s_, done ) in SumTree
-    def __init__(
-        self,
-        eps=0.01,
-        alpha=0.6,
-        beta=0.4,
-        beta_increment_per_sampling=0.0001,
-        **kwargs,
-    ):
-        super().__init__(**kwargs)
-        self.tree = SumTree(self.max_size)
-        self.max_size = self.max_size
-        self.eps = eps
-        self.alpha = alpha
-        self.beta = beta
-        self.beta_increment_per_sampling = beta_increment_per_sampling
-
-    def _get_priority(self, error):
-        return (np.abs(error) + self.eps) ** self.alpha
-
-    def store(self, transition):
-        p = self.tree.max_p
-        self.tree.add(p, self.current_idx)
-        super().store(transition)
-
-    def sample(self, batch_size):
-        data_indices = []
-        idxs = []
-        segment = self.tree.total_p() / batch_size
-        priorities = []
-
-        self.beta = np.min([1.0, self.beta + self.beta_increment_per_sampling])
-
-        for i in range(batch_size):
-            a = segment * i
-            b = segment * (i + 1)
-
-            s = random.uniform(a, b)
-            (idx, p, data) = self.tree.get(s)
-            priorities.append(p)
-            data_indices.append(data)
-            idxs.append(idx)
-
-        sampling_probabilities = priorities / self.tree.total_p()
-        is_weight = np.power(self.size * sampling_probabilities, -self.beta)
-        is_weight /= is_weight.max()
-
-        batch = (
-            self.state[data_indices],
-            self.action[data_indices],
-            self.reward[data_indices],
-            self.next_state[data_indices],
-            self.done[data_indices],
-        )
-
-        return batch, idxs, is_weight
-
-    def update(self, idx, error):
-        p = self._get_priority(error)
-        self.tree.update(idx, p)
diff --git a/src/rl_hockey/common/reward_backprop.py b/src/rl_hockey/common/reward_backprop.py
deleted file mode 100644
index ff65207..0000000
--- a/src/rl_hockey/common/reward_backprop.py
+++ /dev/null
@@ -1,82 +0,0 @@
-"""
-Reward backpropagation utility for TD-MPC2 buffer.
-
-This module provides functions to apply discounted reward bonuses backwards
-through episodes when the agent wins, matching the behavior in TDMPC2ReplayBuffer.
-"""
-
-import numpy as np
-import torch
-
-
-def apply_win_reward_backprop(
-    rewards,
-    winner,
-    win_reward_bonus=0.0,
-    win_reward_discount=0.99,
-    use_torch=False,
-):
-    """
-    Apply discounted reward bonus backwards through an episode for winning episodes.
-
-    This function implements the same reward backpropagation logic used in
-    TDMPC2ReplayBuffer._flush_episode(). When an episode ends with a win (winner == 1),
-    it adds a bonus reward to each step, with the bonus discounted backwards from
-    the end of the episode.
-
-    Args:
-        rewards: Array of rewards for the episode, shape (T,) or (T, 1)
-            Can be numpy array or torch tensor
-        winner: Winner information (1 for agent win, -1 for loss, 0 for draw)
-        win_reward_bonus: Bonus reward to add to each step in a winning episode
-        win_reward_discount: Discount factor for applying win reward bonus
-            backwards through the episode (1.0 = no discount, 0.99 = standard)
-        use_torch: If True, return torch tensor; else numpy array
-
-    Returns:
-        Modified rewards array with bonus applied (same type as input)
-        Original rewards array (for comparison)
-        Bonus rewards array (for analysis)
-    """
-    # Convert to numpy for processing if needed
-    is_torch_input = isinstance(rewards, torch.Tensor)
-    if is_torch_input:
-        rewards_np = rewards.detach().cpu().numpy()
-    else:
-        rewards_np = np.asarray(rewards, dtype=np.float32)
-
-    # Ensure 1D array
-    if rewards_np.ndim > 1:
-        rewards_np = rewards_np.flatten()
-
-    original_rewards = rewards_np.copy()
-    bonus_rewards = np.zeros_like(rewards_np)
-
-    # Apply reward shaping for winning episodes
-    # Skip the terminal reward (last step) which already has +10, start from n-1
-    if winner == 1 and win_reward_bonus > 0.0 and len(rewards_np) > 0:
-        T = len(rewards_np)
-        # Only apply bonus to steps 0 to T-2 (skip the last step T-1)
-        for t in range(T - 1):
-            # Calculate steps from the second-to-last step (n-1)
-            # Step T-2 is 0 steps from n-1, step T-3 is 1 step from n-1, etc.
-            steps_from_n_minus_1 = (T - 2) - t
-            discount_factor = win_reward_discount**steps_from_n_minus_1
-            bonus = win_reward_bonus * discount_factor
-            bonus_rewards[t] = bonus
-
-        rewards_np = rewards_np + bonus_rewards
-
-    # Convert back to torch if needed
-    if use_torch or is_torch_input:
-        rewards_out = torch.as_tensor(rewards_np, dtype=torch.float32)
-        original_rewards = torch.as_tensor(original_rewards, dtype=torch.float32)
-        bonus_rewards = torch.as_tensor(bonus_rewards, dtype=torch.float32)
-        if is_torch_input and rewards.device.type != "cpu":
-            rewards_out = rewards_out.to(rewards.device)
-            original_rewards = original_rewards.to(rewards.device)
-            bonus_rewards = bonus_rewards.to(rewards.device)
-    else:
-        rewards_out = rewards_np
-
-    return rewards_out, original_rewards, bonus_rewards
diff --git a/src/rl_hockey/common/segment_tree.py b/src/rl_hockey/common/segment_tree.py
deleted file mode 100644
index 25451db..0000000
--- a/src/rl_hockey/common/segment_tree.py
+++ /dev/null
@@ -1,146 +0,0 @@
-"""
-Segment tree implementation for Prioritized Experience Replay.
-Based on OpenAI baselines implementation.
-"""
-
-import operator
-
-
-class SegmentTree:
-    def __init__(self, capacity, operation, neutral_element):
-        """Build a Segment Tree data structure.
-
-        https://en.wikipedia.org/wiki/Segment_tree
-
-        Can be used as regular array, but with two
-        important differences:
-
-            a) setting item's value is slightly slower.
-               It is O(lg capacity) instead of O(1).
-            b) user has access to an efficient ( O(log segment size) )
-               `reduce` operation which reduces `operation` over
-               a contiguous subsequence of items in the array.
-
-        Parameters
-        ----------
-        capacity: int
-            Total size of the array - must be a power of two.
-        operation: lambda obj, obj -> obj
-            and operation for combining elements (eg. sum, max)
-            must form a mathematical group together with the set of
-            possible values for array elements (i.e. be associative)
-        neutral_element: obj
-            neutral element for the operation above. eg. float('-inf')
-            for max and 0 for sum.
-        """
-        assert capacity > 0 and capacity & (capacity - 1) == 0, (
-            "capacity must be positive and a power of 2."
-        )
-        self._capacity = capacity
-        self._value = [neutral_element for _ in range(2 * capacity)]
-        self._operation = operation
-
-    def _reduce_helper(self, start, end, node, node_start, node_end):
-        if start == node_start and end == node_end:
-            return self._value[node]
-        mid = (node_start + node_end) // 2
-        if end <= mid:
-            return self._reduce_helper(start, end, 2 * node, node_start, mid)
-        else:
-            if mid + 1 <= start:
-                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)
-            else:
-                return self._operation(
-                    self._reduce_helper(start, mid, 2 * node, node_start, mid),
-                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),
-                )
-
-    def reduce(self, start=0, end=None):
-        """Returns result of applying `self.operation`
-        to a contiguous subsequence of the array.
-
-            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))
-
-        Parameters
-        ----------
-        start: int
-            beginning of the subsequence
-        end: int
-            end of the subsequences
-
-        Returns
-        -------
-        reduced: obj
-            result of reducing self.operation over the specified range of array elements.
-        """
-        if end is None:
-            end = self._capacity
-        if end < 0:
-            end += self._capacity
-        end -= 1
-        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)
-
-    def __setitem__(self, idx, val):
-        # index of the leaf
-        idx += self._capacity
-        self._value[idx] = val
-        idx //= 2
-        while idx >= 1:
-            self._value[idx] = self._operation(
-                self._value[2 * idx], self._value[2 * idx + 1]
-            )
-            idx //= 2
-
-    def __getitem__(self, idx):
-        assert 0 <= idx < self._capacity
-        return self._value[self._capacity + idx]
-
-
-class SumSegmentTree(SegmentTree):
-    def __init__(self, capacity):
-        super(SumSegmentTree, self).__init__(
-            capacity=capacity, operation=operator.add, neutral_element=0.0
-        )
-
-    def sum(self, start=0, end=None):
-        """Returns arr[start] + ... + arr[end]"""
-        return super(SumSegmentTree, self).reduce(start, end)
-
-    def find_prefixsum_idx(self, prefixsum):
-        """Find the highest index `i` in the array such that
-            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum
-
-        if array values are probabilities, this function
-        allows to sample indexes according to the discrete
-        probability efficiently.
-
-        Parameters
-        ----------
-        prefixsum: float
-            upperbound on the sum of array prefix
-
-        Returns
-        -------
-        idx: int
-            highest index satisfying the prefixsum constraint
-        """
-        assert 0 <= prefixsum <= self.sum() + 1e-5
-        idx = 1
-        while idx < self._capacity:  # while non-leaf
-            if self._value[2 * idx] > prefixsum:
-                idx = 2 * idx
-            else:
-                prefixsum -= self._value[2 * idx]
-                idx = 2 * idx + 1
-        return idx - self._capacity
-
-
-class MinSegmentTree(SegmentTree):
-    def __init__(self, capacity):
-        super(MinSegmentTree, self).__init__(
-            capacity=capacity, operation=min, neutral_element=float("inf")
-        )
-
-    def min(self, start=0, end=None):
-        """Returns min(arr[start], ...,  arr[end])"""
-        return super(MinSegmentTree, self).reduce(start, end)
diff --git a/src/rl_hockey/common/sumtree.py b/src/rl_hockey/common/sumtree.py
deleted file mode 100644
index 8a53c8a..0000000
--- a/src/rl_hockey/common/sumtree.py
+++ /dev/null
@@ -1,60 +0,0 @@
-# Code based on https://github.com/rlcode/per.
-# Copied from https://github.com/kitteltom/rl-laser-hockey/blob/main/sum_tree.py
-# Might have to redo it ourselves to avoid plagarism issues.
-
-import numpy as np
-
-
-class SumTree:
-    # A binary tree data structure where the parent’s value is the sum of its children
-    def __init__(self, max_size):
-        self.max_size = max_size
-        self.max_p = 1.0
-
-        self.tree = np.zeros(2 * max_size - 1)
-
-    # Recursively propagate the update to the root node
-    def _propagate(self, idx, delta_p):
-        parent = (idx - 1) // 2
-
-        self.tree[parent] += delta_p
-
-        if parent != 0:
-            self._propagate(parent, delta_p)
-
-    # Find sample on leaf node
-    def _retrieve(self, idx, sample_p):
-        left = 2 * idx + 1
-        right = left + 1
-
-        if left >= self.tree.size:
-            return idx
-
-        if sample_p <= self.tree[left]:
-            return self._retrieve(left, sample_p)
-        else:
-            return self._retrieve(right, sample_p - self.tree[left])
-
-    # Return total priority
-    def total_p(self):
-        return self.tree[0]
-
-    # Store priority
-    def add(self, p, ptr):
-        idx = ptr + self.max_size - 1
-        self.update(idx, p)
-
-    # Update priority
-    def update(self, idx, p):
-        delta_p = p - self.tree[idx]
-
-        self.tree[idx] = p
-        self.max_p = max(p, self.max_p)
-        self._propagate(idx, delta_p)
-
-    # Get priority and data index
-    def get(self, sample_p):
-        idx = self._retrieve(0, sample_p)
-        data_idx = idx - self.max_size + 1
-
-        return idx, self.tree[idx], data_idx
diff --git a/src/rl_hockey/common/training/README.md b/src/rl_hockey/common/training/README.md
index e43c31c..139dbf2 100644
--- a/src/rl_hockey/common/training/README.md
+++ b/src/rl_hockey/common/training/README.md
@@ -150,186 +150,6 @@ This module implements a flexible curriculum learning system for RL hockey train
                      │   └─► Use sampled opponent for episode
 ```
 
-### Scenario 5: Vectorized Environment Training (num_envs > 1)
-
-```
-┌─────────────────────────────────────────────────────────────┐
-│              _train_run_vectorized()                        │
-│  (Vectorized Training with Parallel Environments)          │
-└────────────────────┬────────────────────────────────────────┘
-                     │
-                     ├─► Create VectorizedHockeyEnv
-                     │   └─► num_envs parallel processes
-                     │       ├─► Env0 (Process 1)
-                     │       ├─► Env1 (Process 2)
-                     │       ├─► Env2 (Process 3)
-                     │       └─► Env3 (Process 4)
-                     │
-                     │   Training Loop (Synchronized Steps):
-                     │   │
-                     │   ┌─────────────────────────────────────────┐
-                     │   │  Step 1: Collect Observations (Batched) │
-                     │   └─────────────────────────────────────────┘
-                     │           │
-                     │           ├─► Get states from all envs
-                     │           │   └─► states = [s0, s1, s2, s3]
-                     │           │
-                     │           ├─► GPU: agent.act_batch(states)
-                     │           │   └─► Process all 4 observations
-                     │           │       simultaneously on GPU
-                     │           │       └─► Returns [a0, a1, a2, a3]
-                     │           │
-                     │           ├─► Get opponent actions (per env)
-                     │           │   └─► [opp_a0, opp_a1, opp_a2, opp_a3]
-                     │           │
-                     │   ┌─────────────────────────────────────────┐
-                     │   │  Step 2: Step All Environments (PARALLEL)│
-                     │   └─────────────────────────────────────────┘
-                     │           │
-                     │           ├─► Send actions to all envs
-                     │           │   ├─► Env0.step([a0, opp_a0])
-                     │           │   ├─► Env1.step([a1, opp_a1])
-                     │           │   ├─► Env2.step([a2, opp_a2])
-                     │           │   └─► Env3.step([a3, opp_a3])
-                     │           │
-                     │           ├─► ⏸️  WAIT for ALL to finish
-                     │           │   │
-                     │           │   ├─► Env0: Done in 5ms ✅
-                     │           │   ├─► Env1: Done in 8ms ✅
-                     │           │   ├─► Env2: Done in 3ms ✅
-                     │           │   └─► Env3: Done in 10ms ✅
-                     │           │
-                     │           └─► Collect results (blocking)
-                     │               └─► [obs0, obs1, obs2, obs3]
-                     │                   [rew0, rew1, rew2, rew3]
-                     │                   [done0, done1, done2, done3]
-                     │
-                     │   ┌─────────────────────────────────────────┐
-                     │   │  Step 3: Store Transitions (Sequential) │
-                     │   └─────────────────────────────────────────┘
-                     │           │
-                     │           ├─► For each environment i:
-                     │           │   ├─► Apply reward shaping
-                     │           │   └─► agent.store_transition(
-                     │           │       (states[i], actions[i], 
-                     │           │        rewards[i], next_states[i], 
-                     │           │        dones[i])
-                     │           │   )
-                     │           │
-                     │           └─► Buffer now contains:
-                     │               [Env0-transition, Env1-transition,
-                     │                Env2-transition, Env3-transition]
-                     │               (Interleaved, not sequential)
-                     │
-                     │   ┌─────────────────────────────────────────┐
-                     │   │  Step 4: Train Agent (From Buffer)     │
-                     │   └─────────────────────────────────────────┘
-                     │           │
-                     │           ├─► if steps >= warmup_steps:
-                     │           │   └─► agent.train(updates_per_step)
-                     │           │       └─► Sample random batch
-                     │           │           from replay buffer
-                     │           │           └─► Update network
-                     │           │
-                     │           └─► Continue to next iteration
-                     │
-                     │   Episode Completion Handling:
-                     │   ├─► When env finishes (done=True):
-                     │   │   ├─► Auto-reset in worker process
-                     │   │   ├─► Track episode stats
-                     │   │   └─► Continue (other envs keep running)
-                     │   │
-                     │   └─► Environments run independently:
-                     │       ├─► Env0: Episode 1, Step 100
-                     │       ├─► Env1: Episode 1, Step 50
-                     │       ├─► Env2: Episode 2, Step 25 (finished Ep1)
-                     │       └─► Env3: Episode 1, Step 150
-                     │
-                     └─► Key Benefits:
-                         ├─► 1.4-2.4x faster training
-                         ├─► Better GPU utilization (40-60% vs 12-15%)
-                         ├─► Batched GPU operations (3-4x more efficient)
-                         └─► Mathematically equivalent to parallel single envs
-```
-
-#### Vectorized Environment Timeline Example
-
-```
-Time T: Training Iteration
-─────────────────────────────────────────────────────────────
-
-┌─────────────────────────────────────────────────────────────┐
-│ GPU: Process Batch of Observations                          │
-│ Input:  [obs0, obs1, obs2, obs3]  (shape: 4 × obs_dim)     │
-│ Output: [act0, act1, act2, act3]  (shape: 4 × act_dim)     │
-│ Time:   ~2ms (batched is 3-4x faster than 4 sequential)     │
-└─────────────────────────────────────────────────────────────┘
-                            │
-                            ▼
-┌─────────────────────────────────────────────────────────────┐
-│ CPU: Step All Environments in Parallel                       │
-│                                                              │
-│  Env0 Process ──► [action0] ──► Step ──► 5ms ──► Done ✅     │
-│  Env1 Process ──► [action1] ──► Step ──► 8ms ──► Done ✅    │
-│  Env2 Process ──► [action2] ──► Step ──► 3ms ──► Done ✅    │
-│  Env3 Process ──► [action3] ──► Step ──► 10ms ──► Done ✅   │
-│                                                              │
-│  ⏸️  Main process WAITS for slowest (Env3: 10ms)            │
-└─────────────────────────────────────────────────────────────┘
-                            │
-                            ▼
-┌─────────────────────────────────────────────────────────────┐
-│ Store Transitions in Replay Buffer                           │
-│                                                              │
-│  Buffer.append(Env0: s0, a0, r0, s0', done0)                │
-│  Buffer.append(Env1: s1, a1, r1, s1', done1)                │
-│  Buffer.append(Env2: s2, a2, r2, s2', done2)                │
-│  Buffer.append(Env3: s3, a3, r3, s3', done3)                │
-│                                                              │
-│  Buffer order: [..., Env0, Env1, Env2, Env3, ...]          │
-│  (Interleaved from different episodes)                      │
-└─────────────────────────────────────────────────────────────┘
-                            │
-                            ▼
-┌─────────────────────────────────────────────────────────────┐
-│ Train Agent (if warmup complete)                             │
-│                                                              │
-│  agent.train()                                               │
-│    ├─► Sample random batch from buffer                      │
-│    ├─► Compute loss                                          │
-│    └─► Update network weights                               │
-│                                                              │
-│  Note: Order in buffer doesn't matter (random sampling)     │
-└─────────────────────────────────────────────────────────────┘
-                            │
-                            ▼
-                    Continue to T+1
-```
-
-#### Key Synchronization Points
-
-1. **Environment Stepping**: All environments must finish before proceeding
-   - `vectorized_env.step()` is blocking
-   - Waits for slowest environment to complete
-   - Returns batched results: `(obs, rewards, dones, truncs, infos)`
-
-2. **Transition Storage**: Sequential storage after all steps complete
-   - Each environment's transition stored independently
-   - Transitions from different episodes get interleaved in buffer
-   - This is mathematically correct (off-policy algorithms sample randomly)
-
-3. **Training**: Happens after all transitions stored
-   - Agent trains from randomly sampled batch
-   - Buffer order doesn't matter for learning
-   - Same distribution of experiences as parallel single environments
-
-#### Mathematical Equivalence
-
-- **Same transitions**: Each (s, a, r, s', done) is identical to parallel single envs
-- **Same distribution**: Experience distribution is equivalent
-- **Different order**: Transitions interleaved in buffer (doesn't affect learning)
-- **Faster collection**: 1.4-2.4x speedup through batching and parallelism
-
 ## Data Flow
 
 ### Configuration Loading Flow
diff --git a/src/rl_hockey/common/training/agent_factory.py b/src/rl_hockey/common/training/agent_factory.py
index 7842790..cdb1fa4 100644
--- a/src/rl_hockey/common/training/agent_factory.py
+++ b/src/rl_hockey/common/training/agent_factory.py
@@ -1,39 +1,27 @@
-from typing import Optional, Tuple
-
 import hockey.hockey_env as h_env
+from typing import Tuple, Optional
 
-from rl_hockey.td3.td3 import TD3
+from rl_hockey.DDDQN import DDDQN
+from rl_hockey.sac.sac import SAC
 from rl_hockey.common.agent import Agent
 from rl_hockey.common.training.curriculum_manager import AgentConfig
 from rl_hockey.common.utils import get_discrete_action_dim
-from rl_hockey.DDDQN import DDDQN, DDQN_PER
-from rl_hockey.sac.sac import SAC
-from rl_hockey.TD_MPC2.tdmpc2 import TDMPC2
-from rl_hockey.TD_MPC2_repo.tdmpc2_repo_wrapper import TDMPC2RepoWrapper
 
 
-def get_action_space_info(
-    env: h_env.HockeyEnv, agent_type: str = "DDDQN", fineness: Optional[int] = None
-) -> Tuple[int, int, bool]:
+def get_action_space_info(env: h_env.HockeyEnv, agent_type: str = "DDDQN", fineness: Optional[int] = None) -> Tuple[int, int, bool]:
     state_dim = env.observation_space.shape[0]
-
-    if agent_type == "DDDQN" or agent_type == "DDQN_PER":
+    
+    if agent_type == "DDDQN":
         if fineness is not None:
-            action_dim = get_discrete_action_dim(
-                fineness=fineness, keep_mode=env.keep_mode
-            )
+            action_dim = get_discrete_action_dim(fineness=fineness, keep_mode=env.keep_mode)
         else:
             action_dim = 7 if not env.keep_mode else 8
         is_discrete = True
-    elif agent_type == "TDMPC2" or agent_type == "TDMPC2_REPO":
-        # TDMPC2 uses continuous actions, but also needs discretized action space for hockey
-        action_dim = 4 if env.keep_mode else 3
-        is_discrete = False
     else:
         # continuous action space with 3 or 4 dimensions depending on keep_mode
         action_dim = 3 if not env.keep_mode else 4
         is_discrete = False
-
+    
     return state_dim, action_dim, is_discrete
 
 
@@ -41,184 +29,25 @@ def create_agent(
     agent_config: AgentConfig,
     state_dim: int,
     action_dim: int,
-    common_hyperparams: dict,
-    device: str = None,
+    common_hyperparams: dict
 ) -> Agent:
-    agent_hyperparams = agent_config.hyperparameters.copy()
-    agent_hyperparams.update(
-        {
-            "learning_rate": common_hyperparams.get("learning_rate", 1e-4),
-            "batch_size": common_hyperparams.get("batch_size", 256),
-        }
-    )
 
+    agent_hyperparams = agent_config.hyperparameters.copy()
+    agent_hyperparams.update({
+        'learning_rate': common_hyperparams.get('learning_rate', 1e-4),
+        'batch_size': common_hyperparams.get('batch_size', 256),
+    })
+    
     if agent_config.type == "DDDQN":
-        hidden_dim = agent_hyperparams.pop("hidden_dim", [256, 256])
-        return DDDQN(
-            state_dim=state_dim,
-            action_dim=action_dim,
-            hidden_dim=hidden_dim,
-            **agent_hyperparams,
-        )
-    elif agent_config.type == "DDQN_PER":
-        hidden_dim = agent_hyperparams.pop("hidden_dim", [256, 256])
-        use_per = agent_hyperparams.pop("use_per", True)
-        return DDQN_PER(
-            state_dim=state_dim,
-            action_dim=action_dim,
-            hidden_dim=hidden_dim,
-            use_per=use_per,
-            **agent_hyperparams,
-        )
+        hidden_dim = agent_hyperparams.pop('hidden_dim', [256, 256])
+        return DDDQN(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, **agent_hyperparams)
     elif agent_config.type == "SAC":
         return SAC(state_dim=state_dim, action_dim=action_dim, **agent_hyperparams)
-    elif agent_config.type == "TDMPC2":
-        # TD-MPC2 specific parameters
-        latent_dim = agent_hyperparams.pop("latent_dim", 512)
-        hidden_dim = agent_hyperparams.pop("hidden_dim", None)
-        num_q = agent_hyperparams.pop("num_q", 5)
-        horizon = agent_hyperparams.pop("horizon", 5)
-        num_samples = agent_hyperparams.pop("num_samples", 512)
-        num_iterations = agent_hyperparams.pop("num_iterations", 6)
-        temperature = agent_hyperparams.pop("temperature", 0.5)
-        gamma = agent_hyperparams.pop("gamma", 0.99)
-        capacity = agent_hyperparams.pop("capacity", 1000000)
-        simnorm_temperature = agent_hyperparams.pop("simnorm_temperature", 1.0)
-        log_std_min = agent_hyperparams.pop("log_std_min", -10.0)
-        log_std_max = agent_hyperparams.pop("log_std_max", 2.0)
-        lambda_coef = agent_hyperparams.pop("lambda_coef", 0.95)
-        vmin = agent_hyperparams.pop("vmin", -10.0)
-        vmax = agent_hyperparams.pop("vmax", 10.0)
-        n_step = agent_hyperparams.pop("n_step", 1)
-        win_reward_bonus = agent_hyperparams.pop("win_reward_bonus", 10.0)
-        win_reward_discount = agent_hyperparams.pop("win_reward_discount", 0.92)
-
-        # Use provided device or default to CPU/CUDA
-        if device is None:
-            import torch
-
-            device = "cuda" if torch.cuda.is_available() else "cpu"
-
-        return TDMPC2(
-            obs_dim=state_dim,
-            action_dim=action_dim,
-            latent_dim=latent_dim,
-            hidden_dim=hidden_dim,
-            num_q=num_q,
-            lr=agent_hyperparams.get("learning_rate", 3e-4),
-            gamma=gamma,
-            horizon=horizon,
-            num_samples=num_samples,
-            num_iterations=num_iterations,
-            temperature=temperature,
-            capacity=capacity,
-            batch_size=agent_hyperparams.get("batch_size", 256),
-            device=device,
-            simnorm_temperature=simnorm_temperature,
-            log_std_min=log_std_min,
-            log_std_max=log_std_max,
-            lambda_coef=lambda_coef,
-            vmin=vmin,
-            vmax=vmax,
-            n_step=n_step,
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-        )
-    elif agent_config.type == "TDMPC2_REPO":
-        # TD-MPC2 reference repo wrapper specific parameters
-        latent_dim = agent_hyperparams.pop("latent_dim", 512)
-        hidden_dim = agent_hyperparams.pop("hidden_dim", None)
-        num_q = agent_hyperparams.pop("num_q", 5)
-        horizon = agent_hyperparams.pop("horizon", 5)
-        num_samples = agent_hyperparams.pop("num_samples", 512)
-        num_iterations = agent_hyperparams.pop("num_iterations", 6)
-        num_elites = agent_hyperparams.pop("num_elites", 64)
-        num_pi_trajs = agent_hyperparams.pop("num_pi_trajs", 24)
-        temperature = agent_hyperparams.pop("temperature", 0.5)
-        gamma = agent_hyperparams.pop("gamma", 0.99)
-        capacity = agent_hyperparams.pop("capacity", 1000000)
-        log_std_min = agent_hyperparams.pop("log_std_min", -10.0)
-        log_std_max = agent_hyperparams.pop("log_std_max", 2.0)
-        vmin = agent_hyperparams.pop("vmin", -10.0)
-        vmax = agent_hyperparams.pop("vmax", 10.0)
-        tau = agent_hyperparams.pop("tau", 0.01)
-        grad_clip_norm = agent_hyperparams.pop("grad_clip_norm", 20.0)
-        consistency_coef = agent_hyperparams.pop("consistency_coef", 20.0)
-        reward_coef = agent_hyperparams.pop("reward_coef", 0.1)
-        value_coef = agent_hyperparams.pop("value_coef", 0.1)
-        termination_coef = agent_hyperparams.pop("termination_coef", 1.0)
-        rho = agent_hyperparams.pop("rho", 0.5)
-        entropy_coef = agent_hyperparams.pop("entropy_coef", 1e-4)
-        min_std = agent_hyperparams.pop("min_std", 0.05)
-        max_std = agent_hyperparams.pop("max_std", 2.0)
-        discount_denom = agent_hyperparams.pop("discount_denom", 5.0)
-        discount_min = agent_hyperparams.pop("discount_min", 0.95)
-        discount_max = agent_hyperparams.pop("discount_max", 0.995)
-        episodic = agent_hyperparams.pop("episodic", False)
-        mpc = agent_hyperparams.pop("mpc", True)
-        compile_model = agent_hyperparams.pop("compile", True)
-        episode_length = agent_hyperparams.pop("episode_length", 500)
-        enc_lr_scale = agent_hyperparams.pop("enc_lr_scale", 0.3)
-        seed = agent_hyperparams.pop("seed", 1)
-        win_reward_bonus = agent_hyperparams.pop("win_reward_bonus", 10.0)
-        win_reward_discount = agent_hyperparams.pop("win_reward_discount", 0.92)
-        
-        # Extract batch_size and learning_rate before passing **agent_hyperparams
-        batch_size = agent_hyperparams.pop("batch_size", 256)
-        learning_rate = agent_hyperparams.pop("learning_rate", 3e-4)
-
-        # Use provided device or default to CPU/CUDA
-        if device is None:
-            import torch
-
-            device = "cuda" if torch.cuda.is_available() else "cpu"
-
-        return TDMPC2RepoWrapper(
-            obs_dim=state_dim,
-            action_dim=action_dim,
-            latent_dim=latent_dim,
-            hidden_dim=hidden_dim,
-            num_q=num_q,
-            lr=learning_rate,
-            enc_lr_scale=enc_lr_scale,
-            gamma=gamma,
-            horizon=horizon,
-            num_samples=num_samples,
-            num_iterations=num_iterations,
-            num_elites=num_elites,
-            num_pi_trajs=num_pi_trajs,
-            temperature=temperature,
-            capacity=capacity,
-            batch_size=batch_size,
-            device=device,
-            num_bins=101,
-            vmin=vmin,
-            vmax=vmax,
-            tau=tau,
-            grad_clip_norm=grad_clip_norm,
-            consistency_coef=consistency_coef,
-            reward_coef=reward_coef,
-            value_coef=value_coef,
-            termination_coef=termination_coef,
-            rho=rho,
-            entropy_coef=entropy_coef,
-            log_std_min=log_std_min,
-            log_std_max=log_std_max,
-            min_std=min_std,
-            max_std=max_std,
-            discount_denom=discount_denom,
-            discount_min=discount_min,
-            discount_max=discount_max,
-            episodic=episodic,
-            mpc=mpc,
-            compile=compile_model,
-            episode_length=episode_length,
-            seed=seed,
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-            **agent_hyperparams,
-        )
+    
     elif agent_config.type == "TD3":
-        return TD3(state_dim=state_dim, action_dim=action_dim, **agent_hyperparams)
+        raise NotImplementedError("TD3 is not yet implemented")
+    elif agent_config.type == "TDMPC2":
+        raise NotImplementedError("TDMPC2 is not yet implemented")
     else:
         raise ValueError(f"Unknown agent type: {agent_config.type}")
+
diff --git a/src/rl_hockey/common/training/config_validator.py b/src/rl_hockey/common/training/config_validator.py
index af263a5..57c238d 100644
--- a/src/rl_hockey/common/training/config_validator.py
+++ b/src/rl_hockey/common/training/config_validator.py
@@ -4,7 +4,7 @@ from typing import Dict, Any, List
 
 
 VALID_ENV_MODES = ["NORMAL", "TRAIN_SHOOTING", "TRAIN_DEFENSE"]
-VALID_AGENT_TYPES = ["DDDQN", "SAC", "TD3", "TDMPC2", "TDMPC2_REPO"]
+VALID_AGENT_TYPES = ["DDDQN", "SAC", "TD3"]
 VALID_OPPONENT_TYPES = ["none", "basic_weak", "basic_strong", "self_play", "weighted_mixture"]
 
 
@@ -263,93 +263,6 @@ def _validate_agent(agent: Dict[str, Any]) -> List[str]:
         if 'tau' in hyperparams and (not isinstance(hyperparams['tau'], (int, float)) or hyperparams['tau'] <= 0):
             errors.append("agent.hyperparameters.tau must be a positive number")
     
-    elif agent_type == "TDMPC2":
-        # TDMPC2 specific validations
-        if 'latent_dim' in hyperparams:
-            latent_dim = hyperparams['latent_dim']
-            if not isinstance(latent_dim, int) or latent_dim <= 0:
-                errors.append("agent.hyperparameters.latent_dim must be a positive integer")
-        
-        if 'hidden_dim' in hyperparams:
-            hidden_dim = hyperparams['hidden_dim']
-            if not isinstance(hidden_dim, dict):
-                errors.append("agent.hyperparameters.hidden_dim must be a dict with network-specific hidden dimensions")
-            else:
-                # Dict format (per-network hidden dimensions)
-                valid_network_types = ["encoder", "dynamics", "reward", "termination", "q_function", "policy"]
-                for network_type, network_hidden_dim in hidden_dim.items():
-                    if network_type not in valid_network_types:
-                        errors.append(f"agent.hyperparameters.hidden_dim has unknown network type: {network_type}. Valid types: {valid_network_types}")
-                    elif not isinstance(network_hidden_dim, list) or len(network_hidden_dim) == 0:
-                        errors.append(f"agent.hyperparameters.hidden_dim.{network_type} must be a non-empty list")
-                    elif not all(isinstance(x, int) and x > 0 for x in network_hidden_dim):
-                        errors.append(f"agent.hyperparameters.hidden_dim.{network_type} must contain positive integers")
-        
-        if 'num_q' in hyperparams:
-            num_q = hyperparams['num_q']
-            if not isinstance(num_q, int) or num_q <= 0:
-                errors.append("agent.hyperparameters.num_q must be a positive integer")
-        
-        if 'horizon' in hyperparams:
-            horizon = hyperparams['horizon']
-            if not isinstance(horizon, int) or horizon <= 0:
-                errors.append("agent.hyperparameters.horizon must be a positive integer")
-        
-        if 'num_samples' in hyperparams:
-            num_samples = hyperparams['num_samples']
-            if not isinstance(num_samples, int) or num_samples <= 0:
-                errors.append("agent.hyperparameters.num_samples must be a positive integer")
-        
-        if 'num_iterations' in hyperparams:
-            num_iterations = hyperparams['num_iterations']
-            if not isinstance(num_iterations, int) or num_iterations <= 0:
-                errors.append("agent.hyperparameters.num_iterations must be a positive integer")
-        
-        if 'temperature' in hyperparams:
-            temperature = hyperparams['temperature']
-            if not isinstance(temperature, (int, float)) or temperature <= 0:
-                errors.append("agent.hyperparameters.temperature must be a positive number")
-        
-        if 'gamma' in hyperparams:
-            gamma = hyperparams['gamma']
-            if not isinstance(gamma, (int, float)) or gamma <= 0 or gamma > 1:
-                errors.append("agent.hyperparameters.gamma must be a number between 0 and 1")
-
-        if 'n_step' in hyperparams:
-            n_step = hyperparams['n_step']
-            if not isinstance(n_step, int) or n_step <= 0:
-                errors.append("agent.hyperparameters.n_step must be a positive integer")
-
-        if 'simnorm_temperature' in hyperparams:
-            simnorm_temperature = hyperparams['simnorm_temperature']
-            if not isinstance(simnorm_temperature, (int, float)) or simnorm_temperature <= 0:
-                errors.append("agent.hyperparameters.simnorm_temperature must be a positive number")
-
-        if 'log_std_min' in hyperparams or 'log_std_max' in hyperparams:
-            log_std_min = hyperparams.get('log_std_min', None)
-            log_std_max = hyperparams.get('log_std_max', None)
-            if log_std_min is not None and not isinstance(log_std_min, (int, float)):
-                errors.append("agent.hyperparameters.log_std_min must be a number")
-            if log_std_max is not None and not isinstance(log_std_max, (int, float)):
-                errors.append("agent.hyperparameters.log_std_max must be a number")
-            if log_std_min is not None and log_std_max is not None and log_std_min >= log_std_max:
-                errors.append("agent.hyperparameters.log_std_min must be < log_std_max")
-
-        if 'lambda_coef' in hyperparams:
-            lambda_coef = hyperparams['lambda_coef']
-            if not isinstance(lambda_coef, (int, float)) or lambda_coef <= 0 or lambda_coef > 1:
-                errors.append("agent.hyperparameters.lambda_coef must be in (0, 1]")
-
-        if 'policy_alpha' in hyperparams:
-            policy_alpha = hyperparams['policy_alpha']
-            if not isinstance(policy_alpha, (int, float)) or policy_alpha < 0:
-                errors.append("agent.hyperparameters.policy_alpha must be non-negative")
-
-        if 'policy_beta' in hyperparams:
-            policy_beta = hyperparams['policy_beta']
-            if not isinstance(policy_beta, (int, float)) or policy_beta < 0:
-                errors.append("agent.hyperparameters.policy_beta must be non-negative")
-    
     return errors
 
 
diff --git a/src/rl_hockey/common/training/curriculum_manager.py b/src/rl_hockey/common/training/curriculum_manager.py
index e0362a4..9604497 100644
--- a/src/rl_hockey/common/training/curriculum_manager.py
+++ b/src/rl_hockey/common/training/curriculum_manager.py
@@ -58,24 +58,6 @@ class RewardShapingConfig:
     DIRECTION_FINAL: float = 2.0
 
 
-@dataclass
-class RewardBonusConfig:
-    """Configuration for reward bonus parameters that can change over time within a phase.
-    
-    Similar to RewardShapingConfig, this allows transitioning from START to FINAL values
-    over N + K episodes:
-    - Episodes 0 to N-1: Use START values
-    - Episodes N to N+K-1: Linearly interpolate from START to FINAL
-    - Episodes N+K onwards: Use FINAL values
-    """
-    N: int = 0  # Number of episodes to use START values
-    K: int = 2000  # Number of episodes to transition from START to FINAL
-    WIN_BONUS_START: float = 10.0  # Initial win reward bonus
-    WIN_BONUS_FINAL: float = 1.0  # Final win reward bonus
-    WIN_DISCOUNT_START: float = 0.92  # Initial win reward discount
-    WIN_DISCOUNT_FINAL: float = 0.92  # Final win reward discount (usually kept same)
-
-
 @dataclass
 class PhaseConfig:
     name: str
@@ -83,7 +65,6 @@ class PhaseConfig:
     environment: EnvironmentConfig
     opponent: OpponentConfig
     reward_shaping: Optional[RewardShapingConfig] = None
-    reward_bonus: Optional[RewardBonusConfig] = None
 
 
 @dataclass
@@ -149,19 +130,12 @@ def _parse_config(config_dict: Dict[str, Any]) -> CurriculumConfig:
         else:
             reward_shaping = RewardShapingConfig(**reward_shaping_dict)
         
-        reward_bonus_dict = phase_dict.get('reward_bonus')
-        if reward_bonus_dict is None:
-            reward_bonus = None
-        else:
-            reward_bonus = RewardBonusConfig(**reward_bonus_dict)
-        
         phase = PhaseConfig(
             name=phase_dict['name'],
             episodes=phase_dict['episodes'],
             environment=env_config,
             opponent=opponent_config,
-            reward_shaping=reward_shaping,
-            reward_bonus=reward_bonus
+            reward_shaping=reward_shaping
         )
         phases.append(phase)
     
diff --git a/src/rl_hockey/common/training/hyperparameter_tuning.py b/src/rl_hockey/common/training/hyperparameter_tuning.py
index f2c5305..5080f04 100644
--- a/src/rl_hockey/common/training/hyperparameter_tuning.py
+++ b/src/rl_hockey/common/training/hyperparameter_tuning.py
@@ -752,10 +752,6 @@ def main(
 
 
 if __name__ == "__main__":
-    # Enable TF32 for better performance on Ampere+ GPUs
-    if torch.cuda.is_available():
-        torch.set_float32_matmul_precision('high')
-    
     try:
         mp.set_start_method('spawn', force=True)
     except RuntimeError:
diff --git a/src/rl_hockey/common/training/run_manager.py b/src/rl_hockey/common/training/run_manager.py
index ff7b5df..c6a4d03 100644
--- a/src/rl_hockey/common/training/run_manager.py
+++ b/src/rl_hockey/common/training/run_manager.py
@@ -1,408 +1,239 @@
-import csv
-import hashlib
+import os
 import json
-from datetime import datetime
+import csv
 from pathlib import Path
-from typing import Any, Dict, List
+from datetime import datetime
+from typing import Dict, Any, List
+import hashlib
 
 
 class RunManager:
     """Manages file organization for hyperparameter runs."""
-
+    
     def __init__(self, base_output_dir: str = "results/hyperparameter_runs"):
         """Initialize the run manager."""
         # Add current date and time to the output directory
         current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
         self.base_output_dir = Path(base_output_dir) / current_datetime
         self.base_output_dir.mkdir(parents=True, exist_ok=True)
-
+        
         # Create subdirectories
         self.configs_dir = self.base_output_dir / "configs"
         self.plots_dir = self.base_output_dir / "plots"
         self.csvs_dir = self.base_output_dir / "csvs"
         self.models_dir = self.base_output_dir / "models"
-
-        for dir_path in [
-            self.configs_dir,
-            self.plots_dir,
-            self.csvs_dir,
-            self.models_dir,
-        ]:
+        
+        for dir_path in [self.configs_dir, self.plots_dir, self.csvs_dir, self.models_dir]:
             dir_path.mkdir(parents=True, exist_ok=True)
-
+    
     def generate_run_name(self, config: Dict[str, Any], index: int = None) -> str:
         """Generate a unique run name based on configuration."""
-        # Extract agent type and hyperparameters
-        agent_type = config.get("agent", {}).get("type", "unknown")
-        agent_hyperparams = config.get("agent", {}).get("hyperparameters", {})
-        common_hyperparams = config.get("hyperparameters", {})
-
+        # Extract agent hyperparameters (nested structure)
+        agent_hyperparams = config.get('agent', {}).get('hyperparameters', {})
+        common_hyperparams = config.get('hyperparameters', {})
+        
         # Create a hash from key hyperparameters for uniqueness
         key_params = {
-            "hidden_dim": agent_hyperparams.get("hidden_dim", [128, 128, 128]),
-            "learning_rate": common_hyperparams.get("learning_rate", 0.0001),
-            "batch_size": common_hyperparams.get("batch_size", 256),
-            "target_update_freq": agent_hyperparams.get("target_update_freq", 50),
-            "eps_decay": agent_hyperparams.get("eps_decay", 0.999),
+            'hidden_dim': agent_hyperparams.get('hidden_dim', [128, 128, 128]),
+            'learning_rate': common_hyperparams.get('learning_rate', 0.0001),
+            'batch_size': common_hyperparams.get('batch_size', 256),
+            'target_update_freq': agent_hyperparams.get('target_update_freq', 50),
+            'eps_decay': agent_hyperparams.get('eps_decay', 0.999),
         }
-
+        
         # Create a short hash
         config_str = json.dumps(key_params, sort_keys=True)
         config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
-
+        
         # Create descriptive name
-        lr_str = f"lr{key_params['learning_rate']:.0e}".replace("-", "")
+        lr_str = f"lr{key_params['learning_rate']:.0e}".replace('-', '')
         batch_str = f"bs{key_params['batch_size']}"
         hidden_str = f"h{'_'.join(map(str, key_params['hidden_dim']))}"
-
+        
         # Use index if provided, otherwise use timestamp
         if index is not None:
-            run_name = f"{agent_type}_run_{lr_str}_{batch_str}_{hidden_str}_{config_hash}_{index:04d}"
+            run_name = f"run_{lr_str}_{batch_str}_{hidden_str}_{config_hash}_{index:04d}"
         else:
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-            run_name = f"{agent_type}_run_{lr_str}_{batch_str}_{hidden_str}_{config_hash}_{timestamp}"
-
+            run_name = f"run_{lr_str}_{batch_str}_{hidden_str}_{config_hash}_{timestamp}"
+        
         return run_name
-
+    
     def get_run_directories(self, run_name: str) -> Dict[str, Path]:
         """Get all directory paths for a specific run."""
         return {
-            "config": self.configs_dir / f"{run_name}.json",
-            "plot_rewards": self.plots_dir / f"{run_name}_rewards.png",
-            "plot_losses": self.plots_dir / f"{run_name}_losses.png",
-            "plot_evaluation": self.plots_dir / f"{run_name}_evaluation.png",
-            "csv_rewards": self.csvs_dir / f"{run_name}_rewards.csv",
-            "csv_losses": self.csvs_dir / f"{run_name}_losses.csv",
-            "csv_evaluation": self.csvs_dir / f"{run_name}_evaluation.csv",
-            "csv_resources": self.csvs_dir / f"{run_name}_resources.csv",
-            "csv_episode_logs": self.csvs_dir / f"{run_name}_episode_logs.csv",
-            "model": self.models_dir / f"{run_name}.pt",
+            'config': self.configs_dir / f"{run_name}.json",
+            'plot_rewards': self.plots_dir / f"{run_name}_rewards.png",
+            'plot_losses': self.plots_dir / f"{run_name}_losses.png",
+            'plot_evaluation': self.plots_dir / f"{run_name}_evaluation.png",
+            'csv_rewards': self.csvs_dir / f"{run_name}_rewards.csv",
+            'csv_losses': self.csvs_dir / f"{run_name}_losses.csv",
+            'csv_evaluation': self.csvs_dir / f"{run_name}_evaluation.csv",
+            'model': self.models_dir / f"{run_name}.pt",
         }
-
+    
     def save_config(self, run_name: str, config: Dict[str, Any]):
         """Save configuration to JSON file."""
         paths = self.get_run_directories(run_name)
-        with open(paths["config"], "w") as f:
+        with open(paths['config'], 'w') as f:
             json.dump(config, f, indent=2, default=str)
-
-    def save_rewards_csv(
-        self, run_name: str, rewards: List[float], phases: List[str] = None
-    ):
+    
+    def save_rewards_csv(self, run_name: str, rewards: List[float], phases: List[str] = None):
         """Save rewards to CSV file."""
         paths = self.get_run_directories(run_name)
-        with open(paths["csv_rewards"], "w", newline="") as f:
+        with open(paths['csv_rewards'], 'w', newline='') as f:
             writer = csv.writer(f)
             if phases:
-                writer.writerow(["episode", "reward", "phase"])
+                writer.writerow(['episode', 'reward', 'phase'])
                 for i, reward in enumerate(rewards):
                     phase = phases[i] if i < len(phases) else phases[-1]
                     writer.writerow([i, reward, phase])
             else:
-                writer.writerow(["episode", "reward"])
+                writer.writerow(['episode', 'reward'])
                 for i, reward in enumerate(rewards):
                     writer.writerow([i, reward])
-
+    
     def save_losses_csv(self, run_name: str, losses: List[float]):
         """Save losses to CSV file."""
         paths = self.get_run_directories(run_name)
-        with open(paths["csv_losses"], "w", newline="") as f:
+        with open(paths['csv_losses'], 'w', newline='') as f:
             writer = csv.writer(f)
-            writer.writerow(["step", "loss"])
+            writer.writerow(['step', 'loss'])
             for i, loss in enumerate(losses):
                 writer.writerow([i, loss])
-
-    def save_plots(
-        self,
-        run_name: str,
-        rewards: List[float],
-        losses: List[float],
-        reward_window: int = 10,
-        loss_window: int = 100,
-    ):
+    
+    def save_plots(self, run_name: str, rewards: List[float], losses: List[float], 
+                   reward_window: int = 10, loss_window: int = 100):
         """Save reward and loss plots."""
         import matplotlib
-
-        matplotlib.use("Agg")  # Use non-interactive backend
+        matplotlib.use('Agg')  # Use non-interactive backend
         import matplotlib.pyplot as plt
-
+        
         paths = self.get_run_directories(run_name)
-
+        
         # Plot rewards
         if rewards:
             moving_avg_rewards = self._moving_average(rewards, reward_window)
             plt.figure(figsize=(10, 6))
-            plt.plot(rewards, alpha=0.3, label="Raw")
-            plt.plot(moving_avg_rewards, label=f"Moving Avg (window={reward_window})")
-            plt.xlabel("Episodes")
-            plt.ylabel("Total Reward")
-            plt.title(f"Reward per Episode - {run_name}")
+            plt.plot(rewards, alpha=0.3, label='Raw')
+            plt.plot(moving_avg_rewards, label=f'Moving Avg (window={reward_window})')
+            plt.xlabel('Episodes')
+            plt.ylabel('Total Reward')
+            plt.title(f'Reward per Episode - {run_name}')
             plt.legend()
             plt.grid(True)
             plt.tight_layout()
-            plt.savefig(paths["plot_rewards"])
+            plt.savefig(paths['plot_rewards'])
             plt.close()
-
+        
         # Plot losses
         if losses:
             moving_avg_losses = self._moving_average(losses, loss_window)
             plt.figure(figsize=(10, 6))
-            plt.plot(losses, alpha=0.3, label="Raw")
-            plt.plot(moving_avg_losses, label=f"Moving Avg (window={loss_window})")
-            plt.xlabel("Training Steps")
-            plt.ylabel("Q-Loss")
-            plt.title(f"Q-Loss over Time - {run_name}")
+            plt.plot(losses, alpha=0.3, label='Raw')
+            plt.plot(moving_avg_losses, label=f'Moving Avg (window={loss_window})')
+            plt.xlabel('Training Steps')
+            plt.ylabel('Q-Loss')
+            plt.title(f'Q-Loss over Time - {run_name}')
             plt.legend()
             plt.grid(True)
             plt.tight_layout()
-            plt.savefig(paths["plot_losses"])
+            plt.savefig(paths['plot_losses'])
             plt.close()
         else:
-            print(
-                f"Warning: No losses collected for {run_name}. Loss plot will not be generated."
-            )
-            print(
-                "  This usually means training hasn't started yet (warmup_steps not reached) or agent.train() returned no losses."
-            )
-
+            print(f"Warning: No losses collected for {run_name}. Loss plot will not be generated.")
+            print(f"  This usually means training hasn't started yet (warmup_steps not reached) or agent.train() returned no losses.")
+    
     def get_model_path(self, run_name: str) -> Path:
         """Get model save path for a run."""
-        return self.get_run_directories(run_name)["model"]
-
-    def save_checkpoint(
-        self,
-        run_name: str,
-        episode: int,
-        agent: Any,
-        phase_index: int = None,
-        phase_episode: int = None,
-        episode_logs: List[Dict[str, Any]] = None,
-    ):
+        return self.get_run_directories(run_name)['model']
+    
+    def save_checkpoint(self, run_name: str, episode: int, agent: Any, phase_index: int = None, phase_episode: int = None):
         """Save a checkpoint for the current training state."""
         checkpoint_name = f"{run_name}_ep{episode:06d}"
         checkpoint_path = self.models_dir / f"{checkpoint_name}.pt"
         agent.save(str(checkpoint_path))
-
+        
         # Also save checkpoint metadata for resumption
         metadata_path = self.models_dir / f"{checkpoint_name}_metadata.json"
         metadata = {
-            "episode": episode,
-            "phase_index": phase_index,
-            "phase_episode": phase_episode,
+            'episode': episode,
+            'phase_index': phase_index,
+            'phase_episode': phase_episode,
         }
-        with open(metadata_path, "w") as f:
+        with open(metadata_path, 'w') as f:
             json.dump(metadata, f, indent=2)
-
-        # Save episode logs CSV with checkpoint
-        if episode_logs:
-            checkpoint_csv_path = self.csvs_dir / f"{checkpoint_name}_episode_logs.csv"
-            self.save_episode_logs_csv(
-                checkpoint_name, episode_logs, checkpoint_csv_path
-            )
-
+    
     def get_checkpoint_path(self, run_name: str, episode: int) -> Path:
         """Get checkpoint path for a specific episode."""
         checkpoint_name = f"{run_name}_ep{episode:06d}"
         return self.models_dir / f"{checkpoint_name}.pt"
-
-    def save_evaluation_csv(
-        self, run_name: str, evaluation_results: List[Dict[str, Any]]
-    ):
+    
+    def save_evaluation_csv(self, run_name: str, evaluation_results: List[Dict[str, Any]]):
         """Save evaluation results to CSV file."""
         paths = self.get_run_directories(run_name)
-        with open(paths["csv_evaluation"], "w", newline="") as f:
+        with open(paths['csv_evaluation'], 'w', newline='') as f:
             writer = csv.writer(f)
-            writer.writerow(
-                [
-                    "step",
-                    "episode",
-                    "win_rate",
-                    "mean_reward",
-                    "std_reward",
-                    "wins",
-                    "losses",
-                    "draws",
-                ]
-            )
+            writer.writerow(['step', 'episode', 'win_rate', 'mean_reward', 'std_reward', 'wins', 'losses', 'draws'])
             for result in evaluation_results:
-                writer.writerow(
-                    [
-                        result["step"],
-                        result["episode"],
-                        result["win_rate"],
-                        result["mean_reward"],
-                        result["std_reward"],
-                        result["wins"],
-                        result["losses"],
-                        result["draws"],
-                    ]
-                )
-
-    def save_evaluation_plot(
-        self, run_name: str, evaluation_results: List[Dict[str, Any]]
-    ):
+                writer.writerow([
+                    result['step'],
+                    result['episode'],
+                    result['win_rate'],
+                    result['mean_reward'],
+                    result['std_reward'],
+                    result['wins'],
+                    result['losses'],
+                    result['draws']
+                ])
+    
+    def save_evaluation_plot(self, run_name: str, evaluation_results: List[Dict[str, Any]]):
         """Save evaluation metrics plot."""
         import matplotlib
-
-        matplotlib.use("Agg")  # Use non-interactive backend
+        matplotlib.use('Agg')  # Use non-interactive backend
         import matplotlib.pyplot as plt
-
+        
         paths = self.get_run_directories(run_name)
-
+        
         if not evaluation_results:
             return
-
-        steps = [r["step"] for r in evaluation_results]
-        win_rates = [r["win_rate"] for r in evaluation_results]
-        mean_rewards = [r["mean_reward"] for r in evaluation_results]
-        std_rewards = [r["std_reward"] for r in evaluation_results]
-
+        
+        steps = [r['step'] for r in evaluation_results]
+        win_rates = [r['win_rate'] for r in evaluation_results]
+        mean_rewards = [r['mean_reward'] for r in evaluation_results]
+        std_rewards = [r['std_reward'] for r in evaluation_results]
+        
         fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
-
+        
         # Plot win rate
-        ax1.plot(steps, win_rates, "o-", label="Win Rate", linewidth=2, markersize=6)
-        ax1.axhline(y=0.5, color="r", linestyle="--", alpha=0.5, label="50% (Random)")
-        ax1.set_xlabel("Training Steps")
-        ax1.set_ylabel("Win Rate")
-        ax1.set_title(f"Win Rate vs Base Opponent - {run_name}")
+        ax1.plot(steps, win_rates, 'o-', label='Win Rate', linewidth=2, markersize=6)
+        ax1.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% (Random)')
+        ax1.set_xlabel('Training Steps')
+        ax1.set_ylabel('Win Rate')
+        ax1.set_title(f'Win Rate vs Base Opponent - {run_name}')
         ax1.set_ylim([0, 1])
         ax1.legend()
         ax1.grid(True, alpha=0.3)
-
+        
         # Plot mean reward with error bars
-        ax2.errorbar(
-            steps,
-            mean_rewards,
-            yerr=std_rewards,
-            fmt="o-",
-            label="Mean Reward ± Std",
-            linewidth=2,
-            markersize=6,
-            capsize=4,
-        )
-        ax2.set_xlabel("Training Steps")
-        ax2.set_ylabel("Mean Reward")
-        ax2.set_title(f"Mean Reward vs Base Opponent - {run_name}")
+        ax2.errorbar(steps, mean_rewards, yerr=std_rewards, fmt='o-', 
+                    label='Mean Reward ± Std', linewidth=2, markersize=6, capsize=4)
+        ax2.set_xlabel('Training Steps')
+        ax2.set_ylabel('Mean Reward')
+        ax2.set_title(f'Mean Reward vs Base Opponent - {run_name}')
         ax2.legend()
         ax2.grid(True, alpha=0.3)
-
+        
         plt.tight_layout()
-        plt.savefig(paths["plot_evaluation"])
+        plt.savefig(paths['plot_evaluation'])
         plt.close()
-
-    def save_resources_csv(self, run_name: str, resource_logs: List[Dict[str, Any]]):
-        """Save resource usage logs to CSV file."""
-        if not resource_logs:
-            return
-
-        paths = self.get_run_directories(run_name)
-
-        # Get all unique keys from all resource logs
-        all_keys = set()
-        for log in resource_logs:
-            all_keys.update(log.keys())
-
-        # Sort keys for consistent column order
-        key_order = [
-            "step",
-            "episode",
-            "timestamp",
-            "cpu_percent",
-            "cpu_cores",
-            "load_avg_1min",
-            "load_avg_5min",
-            "load_avg_15min",
-            "memory_used",
-            "memory_total",
-            "memory_percent",
-            "memory_available",
-            "gpu_available",
-            "gpu_device",
-            "gpu_utilization",
-            "gpu_memory_allocated",
-            "gpu_memory_reserved",
-            "gpu_memory_total",
-            "gpu_memory_percent",
-            "gpu_temperature",
-        ]
-
-        # Order keys: known keys first, then any others
-        ordered_keys = [k for k in key_order if k in all_keys]
-        remaining_keys = sorted([k for k in all_keys if k not in key_order])
-        csv_keys = ordered_keys + remaining_keys
-
-        with open(paths["csv_resources"], "w", newline="") as f:
-            writer = csv.writer(f)
-            writer.writerow(csv_keys)
-
-            for log in resource_logs:
-                row = [log.get(key, "") for key in csv_keys]
-                writer.writerow(row)
-
-    def save_episode_logs_csv(
-        self,
-        run_name: str,
-        episode_logs: List[Dict[str, Any]],
-        custom_path: Path = None,
-    ):
-        """Save episode logs with all loss types to CSV file."""
-        if not episode_logs:
-            return
-
-        # Use custom path if provided (for checkpoints), otherwise use default
-        if custom_path is not None:
-            csv_path = custom_path
-        else:
-            paths = self.get_run_directories(run_name)
-            csv_path = paths["csv_episode_logs"]
-
-        # Collect all possible loss keys from all episodes
-        all_loss_keys = set()
-        for log in episode_logs:
-            if "losses" in log and isinstance(log["losses"], dict):
-                all_loss_keys.update(log["losses"].keys())
-
-        # Sort loss keys for consistent column order
-        sorted_loss_keys = sorted(all_loss_keys)
-
-        # Define column order: episode, reward, shaped_reward, backprop_reward, total_gradient_steps first, then all loss types
-        csv_keys = [
-            "episode",
-            "reward",
-            "shaped_reward",
-            "backprop_reward",
-            "total_gradient_steps",
-        ] + sorted_loss_keys
-
-        with open(csv_path, "w", newline="") as f:
-            writer = csv.writer(f)
-            writer.writerow(csv_keys)
-
-            for log in episode_logs:
-                row = [
-                    log.get("episode", ""),
-                    log.get("reward", ""),
-                    log.get("shaped_reward", ""),
-                    log.get("backprop_reward", ""),
-                    log.get("total_gradient_steps", ""),
-                ]
-                # Add loss values in sorted order
-                losses_dict = log.get("losses", {})
-                for loss_key in sorted_loss_keys:
-                    loss_value = losses_dict.get(loss_key, "")
-                    # If it's a list, compute average; otherwise use value directly
-                    if isinstance(loss_value, list):
-                        loss_value = (
-                            sum(loss_value) / len(loss_value) if loss_value else ""
-                        )
-                    row.append(loss_value)
-                writer.writerow(row)
-
+    
     @staticmethod
     def _moving_average(data: List[float], window_size: int) -> List[float]:
         """Calculate moving average of data."""
         moving_averages = []
         for i in range(len(data)):
             window_start = max(0, i - window_size + 1)
-            window = data[window_start : i + 1]
+            window = data[window_start:i + 1]
             moving_averages.append(sum(window) / len(window))
         return moving_averages
diff --git a/src/rl_hockey/common/training/train_continue_run.py b/src/rl_hockey/common/training/train_continue_run.py
index 3d0abee..9182ab2 100644
--- a/src/rl_hockey/common/training/train_continue_run.py
+++ b/src/rl_hockey/common/training/train_continue_run.py
@@ -1,321 +1,252 @@
 """
 Continue training from an existing run directory.
-Loads the latest checkpoint and continues training with the original configuration.
+Loads the latest checkpoint and continues training with a simple configuration.
 """
-
-import csv
 import json
-import logging
 import os
-import shutil
-import sys
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Union
-
-# Configure logging (ensure it's configured before importing train_run)
-# Unbuffer stdout for immediate output in batch jobs
-if hasattr(sys.stdout, "reconfigure"):
-    sys.stdout.reconfigure(line_buffering=True)
-
-logging.basicConfig(
-    level=logging.INFO,
-    format="[%(asctime)s] [%(levelname)s] %(message)s",
-    datefmt="%Y-%m-%d %H:%M:%S",
-    handlers=[logging.StreamHandler(sys.stdout)],
-    force=True,  # Force reconfiguration if already configured
-)
-
-# Imports after logging configuration to ensure logging is set up before modules use it
-# ruff: noqa: E402
-from rl_hockey.common.training.curriculum_manager import load_curriculum
-from rl_hockey.common.training.run_manager import RunManager
-from rl_hockey.common.training.train_run import _curriculum_to_dict, train_run
+from typing import Optional, Union
+from rl_hockey.common.training.train_run import train_run
 from rl_hockey.common.utils import set_cuda_device
-
-
-def load_episode_logs_from_csv(csv_path: Path) -> List[Dict[str, Any]]:
-    """Load episode logs from a CSV file."""
-    episode_logs = []
-
-    if not csv_path.exists():
-        return episode_logs
-
-    with open(csv_path, "r", newline="") as f:
-        reader = csv.DictReader(f)
-        for row in reader:
-            episode_log = {
-                "episode": int(row["episode"]),
-                "reward": float(row["reward"]) if row["reward"] else 0.0,
-                "shaped_reward": float(row["shaped_reward"])
-                if row["shaped_reward"]
-                else 0.0,
-                "backprop_reward": float(row["backprop_reward"])
-                if row.get("backprop_reward")
-                else 0.0,
-                "losses": {},
-            }
-
-            # Extract all loss columns
-            for key, value in row.items():
-                if (
-                    key
-                    not in [
-                        "episode",
-                        "reward",
-                        "shaped_reward",
-                        "backprop_reward",
-                        "total_gradient_steps",
-                    ]
-                    and value
-                ):
-                    try:
-                        episode_log["losses"][key] = float(value)
-                    except ValueError:
-                        pass
-
-            episode_logs.append(episode_log)
-
-    return episode_logs
-
-
-def find_all_episode_log_files(csvs_dir: Path, run_name: str) -> List[Path]:
-    """Find all episode log CSV files for a run (including checkpoints)."""
-    log_files = []
-
-    # Main episode logs file
-    main_file = csvs_dir / f"{run_name}_episode_logs.csv"
-    if main_file.exists():
-        log_files.append(main_file)
-
-    # Checkpoint episode logs files (pattern: {run_name}_ep{episode}_episode_logs.csv)
-    pattern = f"{run_name}_ep*_episode_logs.csv"
-    checkpoint_files = sorted(csvs_dir.glob(pattern))
-    log_files.extend(checkpoint_files)
-
-    return sorted(log_files, key=lambda x: x.name)
-
-
-def combine_episode_logs(log_files: List[Path]) -> List[Dict[str, Any]]:
-    """Combine episode logs from multiple CSV files, removing duplicates."""
-    all_logs = {}
-
-    for log_file in log_files:
-        logs = load_episode_logs_from_csv(log_file)
-        for log in logs:
-            episode_num = log["episode"]
-            # Keep the latest entry if there are duplicates
-            all_logs[episode_num] = log
-
-    # Sort by episode number
-    sorted_logs = [all_logs[ep] for ep in sorted(all_logs.keys())]
-    return sorted_logs
+import tempfile
 
 
 def find_latest_checkpoint(models_dir: Path) -> Optional[Path]:
-    """Find the latest checkpoint in the models directory, excluding temp evaluation checkpoints."""
+    """Find the latest checkpoint in the models directory."""
     if not models_dir.exists():
         return None
-
+    
     checkpoints = list(models_dir.glob("*.pt"))
     if not checkpoints:
         return None
-
+    
     # Filter out temp evaluation checkpoints
     checkpoints = [c for c in checkpoints if "temp_eval" not in c.name]
-
-    if not checkpoints:
-        return None
-
+    
     # Sort by modification time, most recent first
     checkpoints.sort(key=lambda p: p.stat().st_mtime, reverse=True)
     return checkpoints[0]
 
 
+def get_run_name_from_dir(run_dir: Path) -> Optional[str]:
+    """Extract the run name from the directory structure."""
+    configs_dir = run_dir / "configs"
+    if not configs_dir.exists():
+        return None
+    
+    # Find config files (should be only one per run directory)
+    config_files = list(configs_dir.glob("*.json"))
+    if not config_files:
+        return None
+    
+    # Get run name from config file (remove .json extension)
+    config_file = config_files[0]
+    run_name = config_file.stem
+    return run_name
+
+
+def create_continuation_config(
+    episodes: int = 12000,
+    mixture_weights: dict = None,
+    opponent_type: str = "basic_weak",
+    learning_rate: float = 0.0001,
+    batch_size: int = 256,
+    max_episode_steps: int = 500,
+    updates_per_step: int = 1,
+    warmup_steps: int = 20000,
+    reward_scale: float = 0.5,
+    checkpoint_save_freq: int = 100,
+    agent_type: str = "DDDQN",
+    agent_hyperparameters: dict = None
+) -> dict:
+    """Create a simple continuation config with one mixed training phase."""
+    if mixture_weights is None:
+        mixture_weights = {
+            "TRAIN_SHOOTING": 0.1,
+            "TRAIN_DEFENSE": 0.1,
+            "NORMAL": 0.8
+        }
+    
+    if agent_hyperparameters is None:
+        agent_hyperparameters = {}
+    
+    mixture_list = [
+        {"mode": mode, "weight": weight}
+        for mode, weight in mixture_weights.items()
+    ]
+    
+    config = {
+        "curriculum": {
+            "phases": [
+                {
+                    "name": "continuation_mixed_training",
+                    "episodes": episodes,
+                    "environment": {
+                        "mixture": mixture_list,
+                        "keep_mode": True
+                    },
+                    "opponent": {
+                        "type": opponent_type,
+                        "weight": 1.0
+                    },
+                    "reward_shaping": None
+                }
+            ]
+        },
+        "hyperparameters": {
+            "learning_rate": learning_rate,
+            "batch_size": batch_size
+        },
+        "training": {
+            "max_episode_steps": max_episode_steps,
+            "updates_per_step": updates_per_step,
+            "warmup_steps": warmup_steps,
+            "reward_scale": reward_scale,
+            "checkpoint_save_freq": checkpoint_save_freq
+        },
+        "agent": {
+            "type": agent_type,
+            "hyperparameters": agent_hyperparameters
+        }
+    }
+    
+    return config
+
+
 def train_continue_run(
-    base_path: str,
-    base_output_dir: str = None,
+    existing_run_dir: str,
+    continuation_config_path: Optional[str] = None,
+    base_output_dir: str = "results/runs",
     run_name: str = None,
     verbose: bool = True,
-    eval_freq_steps: int = 100_000,
-    eval_num_games: int = 100,
+    eval_freq_steps: int = 100000,
+    eval_num_games: int = 200,
     eval_weak_opponent: bool = True,
     device: Optional[Union[str, int]] = None,
+    continuation_episodes: int = 12000,
     checkpoint_episode: Optional[int] = None,
-    num_envs: int = 4,
+    num_envs: int = 1
 ):
     """
-    Resume training from the last saved checkpoint in a run directory.
-    Uses the original config from the run directory and continues training.
-
+    Continue training from an existing run directory.
+    
     Args:
-        base_path: Path to the existing run directory (e.g., "results/tdmpc2_runs/2026-01-21_19-12-44")
-        base_output_dir: Directory for saving results (if None, uses parent of base_path)
-        run_name: Name for this run (if None, extracted from config file)
-        verbose: Print progress
-        eval_freq_steps: Evaluation frequency
-        eval_num_games: Number of evaluation games
-        eval_weak_opponent: Use weak opponent for eval
-        device: CUDA device
+        existing_run_dir: Path to the existing run directory (e.g., "results/hyperparameter_runs/2026-01-03_09-43-53")
+        continuation_config_path: Optional path to a JSON config file for continuation.
+                                  If None, uses default simple continuation config.
+        base_output_dir: Base directory for saving continuation results
+        run_name: Name for the continuation run (if None, generated automatically)
+        verbose: Whether to print progress information
+        eval_freq_steps: Frequency of evaluation in steps
+        eval_num_games: Number of games to run for evaluation
+        eval_weak_opponent: Whether to use weak (True) or strong (False) BasicOpponent for evaluation
+        device: CUDA device to use
+        continuation_episodes: Number of episodes for continuation (used if continuation_config_path is None)
         checkpoint_episode: Specific checkpoint episode to load (if None, loads latest)
-        num_envs: Number of parallel environments (1 = no vectorization, 4-8 recommended)
+        num_envs: Number of parallel environments (1 = single env, 4-8 recommended for speedup)
     """
     set_cuda_device(device)
-
-    base_path = Path(base_path)
-    if not base_path.exists():
-        raise ValueError(f"Base path does not exist: {base_path}")
-
-    # Find config file
-    configs_dir = base_path / "configs"
-    if not configs_dir.exists():
-        raise ValueError(f"Configs directory not found: {configs_dir}")
-
-    config_files = list(configs_dir.glob("*.json"))
-    if not config_files:
-        raise ValueError(f"No config file found in {configs_dir}")
-
-    # Use the first config file (should be only one per run)
-    config_file = config_files[0]
+    
+    existing_run_dir = Path(existing_run_dir)
+    if not existing_run_dir.exists():
+        raise ValueError(f"Existing run directory does not exist: {existing_run_dir}")
+    
+    # Get run name from existing directory
+    run_name_old = get_run_name_from_dir(existing_run_dir)
+    if run_name_old is None:
+        raise ValueError(f"Could not find config file in {existing_run_dir}")
+    
     if verbose:
-        logging.info(f"Found config file: {config_file}")
-
-    # Extract run_name from config filename if not provided
-    if run_name is None:
-        run_name = config_file.stem
-        if verbose:
-            logging.info(f"Extracted run name: {run_name}")
-
-    # Find checkpoint
-    models_dir = base_path / "models"
+        print(f"Found existing run: {run_name_old}")
+    
+    # Load existing config to get agent parameters
+    configs_dir = existing_run_dir / "configs"
+    config_file = configs_dir / f"{run_name_old}.json"
+    if not config_file.exists():
+        raise ValueError(f"Config file not found: {config_file}")
+    
+    with open(config_file, 'r') as f:
+        existing_config = json.load(f)
+    
+    # Find latest checkpoint
+    models_dir = existing_run_dir / "models"
     if checkpoint_episode is not None:
-        checkpoint_path = models_dir / f"{run_name}_ep{checkpoint_episode:06d}.pt"
+        checkpoint_path = models_dir / f"{run_name_old}_ep{checkpoint_episode:06d}.pt"
         if not checkpoint_path.exists():
             raise ValueError(f"Specified checkpoint not found: {checkpoint_path}")
     else:
         checkpoint_path = find_latest_checkpoint(models_dir)
         if checkpoint_path is None:
             raise ValueError(f"No checkpoint found in {models_dir}")
-
-    if verbose:
-        logging.info(f"Found checkpoint: {checkpoint_path}")
-
-    # Load metadata if available
-    start_episode = 0
-    metadata_path = checkpoint_path.parent / f"{checkpoint_path.stem}_metadata.json"
-    if metadata_path.exists():
-        with open(metadata_path, "r") as f:
-            metadata = json.load(f)
-        start_episode = metadata.get("episode", 0)
-        if verbose:
-            logging.info(
-                f"Checkpoint metadata: episode={start_episode}, "
-                f"phase_index={metadata.get('phase_index')}, "
-                f"phase_episode={metadata.get('phase_episode')}"
-            )
-            logging.info(f"Resuming training from episode {start_episode}")
-
-    # Determine base_output_dir
-    if base_output_dir is None:
-        # Use parent directory of base_path (e.g., if base_path is results/tdmpc2_runs/2026-01-21_19-12-44,
-        # base_output_dir should be results/tdmpc2_runs)
-        base_output_dir = str(base_path.parent)
-
-    # Load curriculum to verify it's valid
-    curriculum = load_curriculum(str(config_file))
-    config_dict = _curriculum_to_dict(curriculum)
-
-    # Create RunManager to set up directory structure
-    run_manager = RunManager(base_output_dir=base_output_dir)
-
-    # Save config file (this will create a new run directory, but that's okay for resuming)
+    
     if verbose:
-        logging.info(f"Saving config file for resumed run: {run_name}")
-    run_manager.save_config(run_name, config_dict)
-
-    # Load old episode logs from CSV files
-    old_csvs_dir = base_path / "csvs"
-    old_episode_logs = []
-    if old_csvs_dir.exists():
-        log_files = find_all_episode_log_files(old_csvs_dir, run_name)
-        if log_files:
-            old_episode_logs = combine_episode_logs(log_files)
-            # Filter to only include episodes up to start_episode
-            old_episode_logs = [
-                log for log in old_episode_logs if log["episode"] <= start_episode
-            ]
-            if verbose:
-                logging.info(
-                    f"Loaded {len(old_episode_logs)} old episode logs (up to episode {start_episode})"
-                )
-
-            # Copy old CSV files to new run directory
-            new_csvs_dir = run_manager.csvs_dir
-            for log_file in log_files:
-                try:
-                    shutil.copy2(log_file, new_csvs_dir / log_file.name)
-                    if verbose:
-                        logging.info(f"Copied {log_file.name} to new run directory")
-                except Exception as e:
-                    if verbose:
-                        logging.warning(f"Could not copy {log_file.name}: {e}")
-
-    # Resume training with the checkpoint
-    if verbose:
-        logging.info(f"Resuming training from checkpoint: {checkpoint_path}")
-        if old_episode_logs:
-            logging.info(f"Loaded {len(old_episode_logs)} old episode logs into memory")
-        logging.info(
-            "Note: Buffer will be empty and will fill up as training continues"
+        print(f"Loading checkpoint: {checkpoint_path}")
+    
+    # Create continuation config
+    if continuation_config_path is not None:
+        if not os.path.exists(continuation_config_path):
+            raise ValueError(f"Continuation config file not found: {continuation_config_path}")
+        
+        from rl_hockey.common.training.config_validator import validate_config
+        errors = validate_config(continuation_config_path)
+        if errors:
+            raise ValueError(f"Configuration errors:\n" + "\n".join(f"  - {e}" for e in errors))
+        
+        temp_config_path = continuation_config_path
+        use_temp = False
+    else:
+        # Use default continuation config with parameters from existing config
+        agent_hyperparams = existing_config.get('agent', {}).get('hyperparameters', {})
+        common_hyperparams = existing_config.get('hyperparameters', {})
+        training_params = existing_config.get('training', {})
+        
+        continuation_config_dict = create_continuation_config(
+            episodes=continuation_episodes,
+            learning_rate=common_hyperparams.get('learning_rate', 0.0001),
+            batch_size=common_hyperparams.get('batch_size', 256),
+            max_episode_steps=training_params.get('max_episode_steps', 500),
+            updates_per_step=training_params.get('updates_per_step', 1),
+            warmup_steps=training_params.get('warmup_steps', 20000),
+            reward_scale=training_params.get('reward_scale', 0.5),
+            checkpoint_save_freq=training_params.get('checkpoint_save_freq', 100),
+            agent_type=existing_config.get('agent', {}).get('type', 'DDDQN'),
+            agent_hyperparameters=agent_hyperparams
         )
-
-    return train_run(
-        str(config_file),
-        base_output_dir,
-        run_name,
-        verbose,
-        eval_freq_steps=eval_freq_steps,
-        eval_num_games=eval_num_games,
-        eval_weak_opponent=eval_weak_opponent,
-        device=device,
-        checkpoint_path=str(checkpoint_path),
-        num_envs=num_envs,
-        run_manager=run_manager,
-        start_episode=start_episode,
-        initial_episode_logs=old_episode_logs if old_episode_logs else None,
-    )
+        
+        # Create temporary config file
+        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
+            json.dump(continuation_config_dict, f, indent=2)
+            temp_config_path = f.name
+            use_temp = True
+    
+    try:
+        # Call train_run with the checkpoint path
+        return train_run(
+            config_path=temp_config_path,
+            base_output_dir=base_output_dir,
+            run_name=run_name,
+            verbose=verbose,
+            eval_freq_steps=eval_freq_steps,
+            eval_num_games=eval_num_games,
+            eval_weak_opponent=eval_weak_opponent,
+            device=device,
+            checkpoint_path=str(checkpoint_path),
+            num_envs=num_envs
+        )
+    finally:
+        if use_temp and os.path.exists(temp_config_path):
+            os.remove(temp_config_path)
 
 
 if __name__ == "__main__":
-    import torch
-
-    # Enable TF32 for better performance on Ampere+ GPUs
-    if torch.cuda.is_available():
-        torch.set_float32_matmul_precision("high")
-
-    # Auto-detect device
-    if torch.cuda.is_available():
-        device = "cuda:0"
-        print(f"CUDA available: Using GPU {device} ({torch.cuda.get_device_name(0)})")
-    else:
-        device = "cpu"
-        print("CUDA not available: Using CPU")
-
-    # Get num_envs from environment variable if set, otherwise use default
-    num_envs = int(os.environ.get("NUM_ENVS", "1"))
-
-    # Get base_path from environment variable or use default
-    base_path = os.environ.get("RESUME_PATH", "results/tdmpc2_runs/2026-01-21_16-15-43")
-
-    print(f"Resuming training from: {base_path}")
+    existing_run_dir = "results/hyperparameter_runs/2026-01-03_13-13-37"
+    
     train_continue_run(
-        base_path=base_path,
-        base_output_dir="results/tdmpc2_runs",
-        device=device,
-        num_envs=num_envs,
+        existing_run_dir,
+        continuation_config_path=None,  # Uses default continuation config
+        base_output_dir="results/runs",
+        device="cuda:1",
+        continuation_episodes=12000,
         verbose=True,
+        num_envs=60  # Use 60 parallel environments for speedup
     )
-
-    # Usage examples:
-    # python -u src/rl_hockey/common/training/train_continue_run.py
-    # RESUME_PATH=results/tdmpc2_runs/2026-01-21_19-12-44 python -u src/rl_hockey/common/training/train_continue_run.py
-    # nohup python -u src/rl_hockey/common/training/train_continue_run.py > results/tdmpc2_runs/train_continue_run.log 2>&1 &
diff --git a/src/rl_hockey/common/training/train_run.py b/src/rl_hockey/common/training/train_run.py
index b9a385b..a6a1e99 100644
--- a/src/rl_hockey/common/training/train_run.py
+++ b/src/rl_hockey/common/training/train_run.py
@@ -1,219 +1,29 @@
-import csv
-import logging
 import os
-import random
-import sys
+import numpy as np
+from tqdm import tqdm
+import torch
+from typing import Optional, Union
 from functools import partial
-from typing import Dict, List, Optional, Union
-
 import hockey.hockey_env as h_env
-import numpy as np
 
 from rl_hockey.common import utils
-from rl_hockey.common.evaluation.agent_evaluator import evaluate_agent
-from rl_hockey.common.reward_backprop import apply_win_reward_backprop
-from rl_hockey.common.training.agent_factory import create_agent, get_action_space_info
-from rl_hockey.common.training.config_validator import validate_config
+from rl_hockey.common.training.run_manager import RunManager
 from rl_hockey.common.training.curriculum_manager import (
-    CurriculumConfig,
-    get_phase_for_episode,
-    get_total_episodes,
-    load_curriculum,
+    load_curriculum, get_phase_for_episode, get_total_episodes, CurriculumConfig
 )
+from rl_hockey.common.training.agent_factory import create_agent, get_action_space_info
 from rl_hockey.common.training.opponent_manager import (
-    get_opponent_action,
-    sample_opponent,
+    sample_opponent, get_opponent_action
 )
-from rl_hockey.common.training.run_manager import RunManager
-from rl_hockey.common.utils import (
-    discrete_to_continuous_action_with_fineness,
-    get_resource_usage,
-    set_cuda_device,
-)
-from rl_hockey.common.vectorized_env import (
-    ThreadedVectorizedHockeyEnvOptimized,
-    VectorizedHockeyEnvOptimized,
-)
-
-# Configure logging
-logging.basicConfig(
-    level=logging.INFO,
-    format="[%(asctime)s] [%(levelname)s] %(message)s",
-    datefmt="%Y-%m-%d %H:%M:%S",
-    handlers=[logging.StreamHandler(sys.stdout)],
-)
-logger = logging.getLogger(__name__)
-
-
-def _save_random_episodes_to_csv(
-    agent, run_manager, run_name, num_episodes=10, save_episode=None
-):
-    """Save random episodes from the buffer to a CSV file.
-
-    Args:
-        agent: The agent with a buffer attribute
-        run_manager: RunManager instance for getting output directory
-        run_name: Name of the current run
-        num_episodes: Number of random episodes to sample (default: 10)
-        save_episode: Episode number when this save occurred (for tracking)
-    """
-    try:
-        buffer = agent.buffer
-
-        # Check if buffer has episodes
-        if not hasattr(buffer, "num_eps") or buffer.num_eps == 0:
-            logger.warning("Buffer is empty, cannot save episodes to CSV")
-            return
-
-        # Check if buffer has enough episodes
-        available_episodes = buffer.num_eps
-        if available_episodes < num_episodes:
-            logger.warning(
-                f"Buffer only has {available_episodes} episodes, "
-                f"but requested {num_episodes}. Saving {available_episodes} episodes instead."
-            )
-            num_episodes = available_episodes
-
-        # Get access to episodes - check buffer type
-        if hasattr(buffer, "_episodes"):
-            # TDMPC2ReplayBuffer
-            all_episodes = buffer._episodes
-        elif hasattr(buffer, "buffer") and hasattr(buffer.buffer, "_storage"):
-            # Regular ReplayBuffer - we need to handle differently
-            logger.warning(
-                "Regular ReplayBuffer detected, episode sampling may not work as expected"
-            )
-            return
-        else:
-            logger.warning(f"Unknown buffer type: {type(buffer)}, cannot save episodes")
-            return
-
-        # Sample random episodes
-        selected_indices = random.sample(range(len(all_episodes)), num_episodes)
-        selected_episodes = [all_episodes[i] for i in selected_indices]
-
-        # Prepare CSV data
-        csv_data = []
-
-        for ep_idx, episode in enumerate(selected_episodes):
-            # Convert tensors to numpy if needed
-            obs = episode["obs"]
-            action = episode["action"]
-            reward = episode["reward"]
-            terminated = episode["terminated"]
-            winner = episode.get("winner", None)
-
-            # Handle torch tensors
-            if hasattr(obs, "cpu"):
-                obs = obs.cpu().numpy()
-            if hasattr(action, "cpu"):
-                action = action.cpu().numpy()
-            if hasattr(reward, "cpu"):
-                reward = reward.cpu().numpy()
-            if hasattr(terminated, "cpu"):
-                terminated = terminated.cpu().numpy()
-
-            # Ensure arrays are numpy
-            obs = np.asarray(obs)
-            action = np.asarray(action)
-            reward = np.asarray(reward).flatten()
-            terminated = np.asarray(terminated).flatten()
-
-            # Convert winner to int if it's a tensor
-            if winner is not None:
-                if hasattr(winner, "cpu"):
-                    winner = winner.cpu().item()
-                elif hasattr(winner, "item"):
-                    winner = winner.item()
-                winner = int(winner) if winner is not None else None
-
-            # Get original reward if available
-            reward_original = episode.get("reward_original", reward)
-            if hasattr(reward_original, "cpu"):
-                reward_original = reward_original.cpu().numpy()
-            reward_original = np.asarray(reward_original).flatten()
-
-            # obs shape: (T+1, obs_dim), action/reward/terminated: (T, ...)
-            T = len(action)
-
-            for step in range(T):
-                row = {
-                    "save_episode": save_episode if save_episode is not None else 0,
-                    "episode_id": ep_idx,
-                    "step": step,
-                }
-
-                # Add observation features (current state)
-                obs_flat = obs[step].flatten()
-                for i, val in enumerate(obs_flat):
-                    row[f"obs_{i}"] = val
-
-                # Add action features
-                action_flat = action[step].flatten()
-                for i, val in enumerate(action_flat):
-                    row[f"action_{i}"] = val
-
-                # Add both original and backprop rewards
-                row["reward_original"] = (
-                    reward_original[step]
-                    if reward_original.ndim == 1
-                    else reward_original[step, 0]
-                )
-                row["reward_backprop"] = (
-                    reward[step] if reward.ndim == 1 else reward[step, 0]
-                )
-
-                # Add terminated/done flag
-                row["terminated"] = (
-                    terminated[step] if terminated.ndim == 1 else terminated[step, 0]
-                )
-
-                # Add winner (same for all steps in the episode)
-                row["winner"] = winner if winner is not None else ""
-
-                csv_data.append(row)
-
-        # Write to CSV - save to a new file each time (not append)
-        # Include save_episode in filename to make each file unique
-        save_episode_str = f"ep{save_episode}" if save_episode is not None else "ep0"
-        csv_path = (
-            run_manager.csvs_dir
-            / f"{run_name}_buffer_episodes_sample_{save_episode_str}.csv"
-        )
-
-        if csv_data:
-            fieldnames = [
-                "save_episode",
-                "episode_id",
-                "step",
-                "reward_original",
-                "reward_backprop",
-                "terminated",
-                "winner",
-            ]
-            # Add obs and action fieldnames
-            first_row = csv_data[0]
-            for key in first_row.keys():
-                if key not in fieldnames:
-                    fieldnames.append(key)
-
-            # Always write new file (not append)
-            with open(csv_path, "w", newline="") as csvfile:
-                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
-                writer.writeheader()
-                writer.writerows(csv_data)
-
-            logger.info(
-                f"Saved {num_episodes} random episodes to {csv_path} (save_episode={save_episode})"
-            )
-
-    except Exception as e:
-        logger.warning(f"Failed to save episodes to CSV: {e}")
+from rl_hockey.common.training.config_validator import validate_config
+from rl_hockey.common.evaluation.agent_evaluator import evaluate_agent
+from rl_hockey.common.utils import discrete_to_continuous_action_with_fineness, set_cuda_device
+from rl_hockey.common.vectorized_env import VectorizedHockeyEnvOptimized, ThreadedVectorizedHockeyEnvOptimized
 
 
 def _make_hockey_env(mode, keep_mode):
     """Module-level factory function for creating HockeyEnv instances.
-
+    
     This function must be at module level to be picklable for multiprocessing.
     """
     return h_env.HockeyEnv(mode=mode, keep_mode=keep_mode)
@@ -229,14 +39,11 @@ def train_run(
     eval_weak_opponent: bool = True,
     device: Optional[Union[str, int]] = None,
     checkpoint_path: Optional[str] = None,
-    num_envs: int = 1,
-    run_manager: Optional[RunManager] = None,
-    start_episode: int = 0,
-    initial_episode_logs: Optional[List[Dict]] = None,
+    num_envs: int = 1
 ):
     """
     Train an agent with curriculum learning.
-
+    
     Args:
         config_path: Path to curriculum configuration JSON file
         base_output_dir: Base directory for saving results
@@ -248,184 +55,94 @@ def train_run(
         device: CUDA device to use (None = auto-detect, 'cpu' = CPU, 'cuda' = cuda:0, 'cuda:0' = first GPU, 'cuda:1' = second GPU, etc.). Can also be an integer (0, 1, etc.) for device ID.
         checkpoint_path: Optional path to a checkpoint file to load and continue training from
         num_envs: Number of parallel environments (1 = single env, 4-8 recommended for speedup)
-        run_manager: Optional RunManager instance to reuse (if None, creates a new one)
-        start_episode: Episode number to start training from (for resuming from checkpoint, default 0)
-        initial_episode_logs: Optional list of episode logs to preload (for resuming from checkpoint)
     """
     # Route to vectorized training if num_envs > 1
     # Check if we're in a multiprocessing Pool worker to decide which implementation to use
     if num_envs > 1:
         from multiprocessing import current_process
-
         current_proc = current_process()
-        is_daemon = getattr(current_proc, "daemon", False)
-        proc_name = getattr(current_proc, "name", "")
-        is_pool_worker = "PoolWorker" in proc_name or is_daemon
-
+        is_daemon = getattr(current_proc, 'daemon', False)
+        proc_name = getattr(current_proc, 'name', '')
+        is_pool_worker = 'PoolWorker' in proc_name or is_daemon
+        
         if num_envs > 1:
             if is_pool_worker:
                 if verbose:
-                    print(
-                        f"Using threaded vectorized environments with {num_envs} parallel instances (Pool worker mode)"
-                    )
+                    print(f"Using threaded vectorized environments with {num_envs} parallel instances (Pool worker mode)")
             else:
                 if verbose:
-                    print(
-                        f"Using multiprocess vectorized environments with {num_envs} parallel instances"
-                    )
+                    print(f"Using multiprocess vectorized environments with {num_envs} parallel instances")
             return _train_run_vectorized(
-                config_path,
-                base_output_dir,
-                run_name,
-                verbose,
-                eval_freq_steps,
-                eval_num_games,
-                eval_weak_opponent,
-                device,
-                checkpoint_path,
-                num_envs,
-                use_threading=is_pool_worker,
-                run_manager=run_manager,
-                start_episode=start_episode,
-                initial_episode_logs=initial_episode_logs,
+                config_path, base_output_dir, run_name, verbose,
+                eval_freq_steps, eval_num_games, eval_weak_opponent,
+                device, checkpoint_path, num_envs, use_threading=is_pool_worker
             )
     # Set CUDA device if specified
     set_cuda_device(device)
-
+    
     errors = validate_config(config_path)
     if errors:
-        raise ValueError(
-            "Configuration errors:\n" + "\n".join(f"  - {e}" for e in errors)
-        )
-
+        raise ValueError(f"Configuration errors:\n" + "\n".join(f"  - {e}" for e in errors))
+    
     curriculum = load_curriculum(config_path)
-
-    if run_manager is None:
-        run_manager = RunManager(base_output_dir=base_output_dir)
+    
+    run_manager = RunManager(base_output_dir=base_output_dir)
     if run_name is None:
         config_dict = _curriculum_to_dict(curriculum)
         run_name = run_manager.generate_run_name(config_dict)
-
+    
     training_params = curriculum.training
-    max_episode_steps = training_params.get("max_episode_steps", 500)
-    updates_per_step = training_params.get("updates_per_step", 1)
-    warmup_steps = training_params.get("warmup_steps", 400)
-    reward_scale = training_params.get("reward_scale", 0.1)
-    checkpoint_save_freq = training_params.get("checkpoint_save_freq", 100)
-    train_freq = training_params.get("train_freq", 1)
-
+    max_episode_steps = training_params.get('max_episode_steps', 500)
+    updates_per_step = training_params.get('updates_per_step', 1)
+    warmup_steps = training_params.get('warmup_steps', 400)
+    reward_scale = training_params.get('reward_scale', 0.1)
+    checkpoint_save_freq = training_params.get('checkpoint_save_freq', 100)
+    train_freq = training_params.get('train_freq', 1)
+    
     total_episodes = get_total_episodes(curriculum)
-
+    
     current_env = None
     current_phase_idx = -1
     current_opponent = None
     current_phase_start = 0
-
+    
     # none is the default action space of 8 for DDDQN
-    action_fineness = curriculum.agent.hyperparameters.get("action_fineness", None)
-
+    action_fineness = curriculum.agent.hyperparameters.get('action_fineness', None)
+    
     first_phase = curriculum.phases[0]
     initial_mode_str = first_phase.environment.get_mode_for_episode(0)
     env_mode = getattr(h_env.Mode, initial_mode_str)
-    current_env = h_env.HockeyEnv(
-        mode=env_mode, keep_mode=first_phase.environment.keep_mode
-    )
-    state_dim, agent_action_dim, is_agent_discrete = get_action_space_info(
-        current_env, curriculum.agent.type, fineness=action_fineness
-    )
-
+    current_env = h_env.HockeyEnv(mode=env_mode, keep_mode=first_phase.environment.keep_mode)
+    state_dim, agent_action_dim, is_agent_discrete = get_action_space_info(current_env, curriculum.agent.type, fineness=action_fineness)
+    
     agent = create_agent(
         curriculum.agent,
         state_dim,
         agent_action_dim,
-        curriculum.hyperparameters,
-        device=device,
+        curriculum.hyperparameters
     )
-
-    # Log the agent network architecture
-    if verbose:
-        print("\n" + agent.log_architecture() + "\n")
-
-    # Log backprop parameters if available
-    if (
-        verbose
-        and hasattr(agent, "buffer")
-        and hasattr(agent.buffer, "win_reward_bonus")
-        and hasattr(agent.buffer, "win_reward_discount")
-    ):
-        print("REWARD BACKPROPAGATION PARAMETERS:")
-        print(f"  win_reward_bonus: {agent.buffer.win_reward_bonus}")
-        print(f"  win_reward_discount: {agent.buffer.win_reward_discount}")
-        print("")
-
+    
     if checkpoint_path is not None:
         if verbose:
             print(f"Loading agent from checkpoint: {checkpoint_path}")
         agent.load(checkpoint_path)
         if verbose:
             print("Checkpoint loaded successfully")
-
-    # Helper function to initialize reward bonus (defined early for use before main loop)
-    def _init_reward_bonus_from_config(episode, curriculum_config, agent_ref):
-        """Initialize reward bonus parameters based on episode number."""
-        phase_idx, phase_local_ep, phase_cfg = get_phase_for_episode(curriculum_config, episode)
-        if phase_cfg.reward_bonus is not None:
-            rb = phase_cfg.reward_bonus
-            N, K = rb.N, rb.K
-            if phase_local_ep < N:
-                win_bonus = rb.WIN_BONUS_START
-                win_discount = rb.WIN_DISCOUNT_START
-            elif phase_local_ep < N + K:
-                alpha = (phase_local_ep - N) / K
-                win_bonus = rb.WIN_BONUS_START * (1 - alpha) + rb.WIN_BONUS_FINAL * alpha
-                win_discount = rb.WIN_DISCOUNT_START * (1 - alpha) + rb.WIN_DISCOUNT_FINAL * alpha
-            else:
-                win_bonus = rb.WIN_BONUS_FINAL
-                win_discount = rb.WIN_DISCOUNT_FINAL
-            
-            if hasattr(agent_ref, "buffer"):
-                if hasattr(agent_ref.buffer, "win_reward_bonus"):
-                    agent_ref.buffer.win_reward_bonus = win_bonus
-                if hasattr(agent_ref.buffer, "win_reward_discount"):
-                    agent_ref.buffer.win_reward_discount = win_discount
-            return {"win_bonus": win_bonus, "win_discount": win_discount}
-        return None
-
-    # Initialize reward bonus parameters based on start_episode (important for resuming)
-    if start_episode > 0:
-        init_bonus = _init_reward_bonus_from_config(start_episode, curriculum, agent)
-        if init_bonus is not None and verbose:
-            print(f"Initialized reward bonus for episode {start_episode}:")
-            print(f"  win_reward_bonus: {init_bonus['win_bonus']:.4f}")
-            print(f"  win_reward_discount: {init_bonus['win_discount']:.4f}")
-
+    
     losses = []
-    all_losses_dict = {}  # Dictionary to store all loss types: {loss_type: [values]}
     rewards = []
     phases = []
-    episode_logs = []  # List to store episode logs with all loss types
     steps = 0
     gradient_steps = 0
     evaluation_results = []
     last_eval_step = 0
-    resource_logs = []
-    resource_log_freq = training_params.get(
-        "resource_log_freq", 10
-    )  # Log every N steps
-    last_resource_log_step = 0
-
-    # Track resource usage per episode for averaging
-    episode_resource_window = training_params.get("resource_avg_window_episodes", 10)
-    episode_resource_samples = []  # List of dicts, one per episode
-    current_episode_cpu_samples = []  # CPU samples during current episode
-    current_episode_gpu_samples = []  # GPU samples during current episode
-
+    
     def get_reward_weights(episode_idx, phase_config):
         reward_shaping = phase_config.reward_shaping
-
+        
         if reward_shaping is None:
             return None
-
+        
         N = reward_shaping.N
         K = reward_shaping.K
         CLOSENESS_START = reward_shaping.CLOSENESS_START
@@ -433,639 +150,306 @@ def train_run(
         CLOSENESS_FINAL = reward_shaping.CLOSENESS_FINAL
         TOUCH_FINAL = reward_shaping.TOUCH_FINAL
         DIRECTION_FINAL = reward_shaping.DIRECTION_FINAL
-
-        if episode_idx < N:
-            return {
-                "closeness": CLOSENESS_START,
-                "touch": TOUCH_START,
-                "direction": 0.0,
-            }
-        elif episode_idx < N + K:
-            alpha = (episode_idx - N) / K
-            return {
-                "closeness": CLOSENESS_START * (1 - alpha) + CLOSENESS_FINAL * alpha,
-                "touch": TOUCH_START * (1 - alpha) + TOUCH_FINAL * alpha,
-                "direction": DIRECTION_FINAL * alpha,
-            }
-        else:
-            return {
-                "closeness": CLOSENESS_FINAL,
-                "touch": TOUCH_FINAL,
-                "direction": DIRECTION_FINAL,
-            }
-
-    def get_reward_bonus_values(episode_idx, phase_config):
-        """Get the reward bonus parameters for a given episode within a phase.
         
-        Returns a dict with 'win_bonus' and 'win_discount' values, or None if no
-        reward_bonus config exists for this phase.
-        """
-        reward_bonus = phase_config.reward_bonus
-
-        if reward_bonus is None:
-            return None
-
-        N = reward_bonus.N
-        K = reward_bonus.K
-        WIN_BONUS_START = reward_bonus.WIN_BONUS_START
-        WIN_BONUS_FINAL = reward_bonus.WIN_BONUS_FINAL
-        WIN_DISCOUNT_START = reward_bonus.WIN_DISCOUNT_START
-        WIN_DISCOUNT_FINAL = reward_bonus.WIN_DISCOUNT_FINAL
-
         if episode_idx < N:
             return {
-                "win_bonus": WIN_BONUS_START,
-                "win_discount": WIN_DISCOUNT_START,
+                'closeness': CLOSENESS_START,
+                'touch': TOUCH_START,
+                'direction': 0.0,
             }
         elif episode_idx < N + K:
             alpha = (episode_idx - N) / K
             return {
-                "win_bonus": WIN_BONUS_START * (1 - alpha) + WIN_BONUS_FINAL * alpha,
-                "win_discount": WIN_DISCOUNT_START * (1 - alpha) + WIN_DISCOUNT_FINAL * alpha,
+                'closeness': CLOSENESS_START * (1 - alpha) + CLOSENESS_FINAL * alpha,
+                'touch': TOUCH_START * (1 - alpha) + TOUCH_FINAL * alpha,
+                'direction': DIRECTION_FINAL * alpha,
             }
         else:
             return {
-                "win_bonus": WIN_BONUS_FINAL,
-                "win_discount": WIN_DISCOUNT_FINAL,
+                'closeness': CLOSENESS_FINAL,
+                'touch': TOUCH_FINAL,
+                'direction': DIRECTION_FINAL,
             }
-
-    def update_buffer_reward_bonus(agent, bonus_values):
-        """Update the agent's buffer reward bonus parameters if they exist."""
-        if bonus_values is None:
-            return
-        if not hasattr(agent, "buffer"):
-            return
-        if hasattr(agent.buffer, "win_reward_bonus"):
-            agent.buffer.win_reward_bonus = bonus_values["win_bonus"]
-        if hasattr(agent.buffer, "win_reward_discount"):
-            agent.buffer.win_reward_discount = bonus_values["win_discount"]
-
-    for global_episode in range(start_episode, total_episodes):
-        phase_idx, phase_local_episode, phase_config = get_phase_for_episode(
-            curriculum, global_episode
-        )
-
-        if phase_idx != current_phase_idx:
-            if verbose:
-                print(
-                    f"\nTransitioning to phase {phase_idx + 1}/{len(curriculum.phases)}: {phase_config.name}"
-                )
-
-            current_opponent = sample_opponent(
-                phase_config.opponent,
-                agent=agent,
-                checkpoint_dir=str(run_manager.models_dir),
-                agent_config=curriculum.agent,
-                state_dim=state_dim,
-                action_dim=agent_action_dim,
-                is_discrete=is_agent_discrete,
-            )
-
-            if current_phase_idx >= 0:
+    
+    pbar = tqdm(range(total_episodes), desc=run_name, disable=not verbose)
+    
+    for global_episode in pbar:
+            phase_idx, phase_local_episode, phase_config = get_phase_for_episode(curriculum, global_episode)
+            
+            if phase_idx != current_phase_idx:
                 if verbose:
-                    print(
-                        f"  Clearing replay buffer (size: {agent.buffer.size}) to avoid distribution mismatch"
-                    )
-                agent.buffer.clear()
-
-            current_phase_idx = phase_idx
-            current_phase_start = steps
-
-        # Update reward bonus parameters for this episode
-        bonus_values = get_reward_bonus_values(phase_local_episode, phase_config)
-        update_buffer_reward_bonus(agent, bonus_values)
-
-        sampled_mode_str = phase_config.environment.get_mode_for_episode(
-            phase_local_episode
-        )
-        env_mode = getattr(h_env.Mode, sampled_mode_str)
-
-        if (
-            current_env is None
-            or current_env.mode != env_mode
-            or current_env.keep_mode != phase_config.environment.keep_mode
-        ):
-            if current_env is not None:
-                current_env.close()
-            current_env = h_env.HockeyEnv(
-                mode=env_mode, keep_mode=phase_config.environment.keep_mode
-            )
-
-        state, _ = current_env.reset()
-        # Ensure state is float32 to avoid repeated conversions
-        if state.dtype != np.float32:
-            state = state.astype(np.float32, copy=False)
-
-        reward_weights = get_reward_weights(phase_local_episode, phase_config)
-
-        agent.on_episode_start(global_episode)
-        total_reward = 0
-        total_shaped_reward = 0
-        # Track scaled rewards for backprop calculation
-        episode_scaled_rewards = []
-        episode_winner = None
-        # Reset episode resource tracking
-        current_episode_cpu_samples = []
-        current_episode_gpu_samples = []
-        # Track losses for this episode
-        episode_losses = {}
-
-        if (
-            phase_config.opponent.type == "self_play"
-            and phase_config.opponent.checkpoint is None
-        ):
-            current_opponent = sample_opponent(
-                phase_config.opponent,
-                agent=agent,
-                checkpoint_dir=str(run_manager.models_dir),
-                agent_config=curriculum.agent,
-                state_dim=state_dim,
-                action_dim=agent_action_dim,
-                is_discrete=is_agent_discrete,
-            )
-
-        for t in range(max_episode_steps):
-            if is_agent_discrete:
-                discrete_action = agent.act(state)
-                if action_fineness is not None:
-                    action_p1 = discrete_to_continuous_action_with_fineness(
-                        discrete_action,
-                        fineness=action_fineness,
-                        keep_mode=phase_config.environment.keep_mode,
-                    )
-                else:
-                    action_p1 = current_env.discrete_to_continous_action(
-                        discrete_action
-                    )
-            else:
-                action_p1 = agent.act(state, t0=(t == 0))
-
-            obs_agent2 = current_env.obs_agent_two()
-            deterministic_opponent = (
-                phase_config.opponent.deterministic
-                if phase_config.opponent.type == "self_play"
-                else True
-            )
-            action_p2 = get_opponent_action(
-                current_opponent, obs_agent2, deterministic=deterministic_opponent
-            )
-
-            if isinstance(action_p2, (int, np.integer)):
-                if action_fineness is not None:
-                    action_p2 = discrete_to_continuous_action_with_fineness(
-                        action_p2,
-                        fineness=action_fineness,
-                        keep_mode=phase_config.environment.keep_mode,
-                    )
-                else:
-                    action_p2 = current_env.discrete_to_continous_action(action_p2)
-
-            action = np.hstack([action_p1, action_p2])
-
-            next_state, reward, done, trunc, info = current_env.step(action)
-            # Ensure next_state is float32 to avoid repeated conversions
-            if next_state.dtype != np.float32:
-                next_state = next_state.astype(np.float32, copy=False)
-
-            if reward_weights is not None:
-                shaped_reward = reward
-                closeness_raw = info.get("reward_closeness_to_puck", 0.0)
-                touch_raw = info.get("reward_touch_puck", 0.0)
-                direction_raw = info.get("reward_puck_direction", 0.0)
-
-                closeness_contribution = closeness_raw * reward_weights["closeness"]
-                touch_contribution = touch_raw * reward_weights["touch"]
-                direction_contribution = direction_raw * reward_weights["direction"]
-
-                shaped_reward += closeness_contribution
-                shaped_reward += touch_contribution
-                shaped_reward += direction_contribution
-
-                # Debug logging when shaped_reward is unusually high (potential bug indicator)
-                # Also log around episode 90, step 17 range
-                should_log = (
-                    abs(shaped_reward) > 25.0  # Unusually high reward
+                    print(f"\nTransitioning to phase {phase_idx + 1}/{len(curriculum.phases)}: {phase_config.name}")
+                
+                current_opponent = sample_opponent(
+                    phase_config.opponent,
+                    agent=agent,
+                    checkpoint_dir=str(run_manager.models_dir),
+                    agent_config=curriculum.agent,
+                    state_dim=state_dim,
+                    action_dim=agent_action_dim,
+                    is_discrete=is_agent_discrete
                 )
-
-                if should_log:
-                    logger.warning(
-                        f"[DEBUG REWARD] global_ep={global_episode}, step={t}: "
-                        f"base_reward={reward:.6f}, "
-                        f"closeness_raw={closeness_raw:.6f}, "
-                        f"touch_raw={touch_raw:.6f}, "
-                        f"direction_raw={direction_raw:.6f}, "
-                        f"weights_closeness={reward_weights['closeness']:.6f}, "
-                        f"weights_touch={reward_weights['touch']:.6f}, "
-                        f"weights_direction={reward_weights['direction']:.6f}, "
-                        f"closeness_contrib={closeness_contribution:.6f}, "
-                        f"touch_contrib={touch_contribution:.6f}, "
-                        f"direction_contrib={direction_contribution:.6f}, "
-                        f"shaped_reward={shaped_reward:.6f}, "
-                        f"reward_scale={reward_scale:.6f}"
-                    )
-            else:
-                shaped_reward = reward
-
-            scaled_reward = shaped_reward * reward_scale
-
-            # Debug logging for scaled reward (after scaling)
-            should_log_scaled = (
-                abs(scaled_reward) > 25.0  # Unusually high reward
-                or (
-                    global_episode >= 89
-                    and global_episode <= 91
-                    and t >= 15
-                    and t <= 19
+                
+                if current_phase_idx >= 0:
+                    if verbose:
+                        print(f"  Clearing replay buffer (size: {agent.buffer.size}) to avoid distribution mismatch")
+                    agent.buffer.clear()
+                
+                current_phase_idx = phase_idx
+                current_phase_start = steps
+            
+            sampled_mode_str = phase_config.environment.get_mode_for_episode(phase_local_episode)
+            env_mode = getattr(h_env.Mode, sampled_mode_str)
+            
+            if current_env is None or \
+               current_env.mode != env_mode or \
+               current_env.keep_mode != phase_config.environment.keep_mode:
+                if current_env is not None:
+                    current_env.close()
+                current_env = h_env.HockeyEnv(
+                    mode=env_mode,
+                    keep_mode=phase_config.environment.keep_mode
                 )
-            )
-
-            if should_log_scaled:
-                logger.warning(
-                    f"[DEBUG SCALED] global_ep={global_episode}, step={t}: "
-                    f"scaled_reward={scaled_reward:.6f}, "
-                    f"done={done}, "
-                    f"winner={info.get('winner', None)}"
+            
+            state, _ = current_env.reset()
+            # Ensure state is float32 to avoid repeated conversions
+            if state.dtype != np.float32:
+                state = state.astype(np.float32, copy=False)
+            
+            reward_weights = get_reward_weights(phase_local_episode, phase_config)
+            
+            agent.on_episode_start(global_episode)
+            total_reward = 0
+            total_shaped_reward = 0
+            
+            if phase_config.opponent.type == "self_play" and phase_config.opponent.checkpoint is None:
+                current_opponent = sample_opponent(
+                    phase_config.opponent,
+                    agent=agent,
+                    checkpoint_dir=str(run_manager.models_dir),
+                    agent_config=curriculum.agent,
+                    state_dim=state_dim,
+                    action_dim=agent_action_dim,
+                    is_discrete=is_agent_discrete
                 )
-
-            # Track scaled rewards for backprop calculation
-            episode_scaled_rewards.append(scaled_reward)
-
-            # Get winner information if episode is done (only pass when done=True, not trunc)
-            # Winner is 1 for agent win, -1 for agent loss
-            winner = None
-            if done:
-                winner = info.get("winner", 0)
-                episode_winner = winner
-
-            # Check if buffer is episode-based (TDMPC2) - skip inline mirroring
-            # Episode-based buffers accumulate transitions into a single episode,
-            # so storing mirrored transitions would double/triple rewards incorrectly
-            is_episode_based_buffer = hasattr(agent, "buffer") and hasattr(
-                agent.buffer, "_episodes"
-            )
-
-            if is_agent_discrete:
-                # Pre-allocate array once and reuse (optimization)
-                discrete_action_array = np.array([discrete_action], dtype=np.float32)
-                agent.store_transition(
-                    (state, discrete_action_array, scaled_reward, next_state, done),
-                    winner=winner,
+            
+            for t in range(max_episode_steps):
+                if is_agent_discrete:
+                    discrete_action = agent.act(state)
+                    if action_fineness is not None:
+                        action_p1 = discrete_to_continuous_action_with_fineness(
+                            discrete_action, 
+                            fineness=action_fineness, 
+                            keep_mode=phase_config.environment.keep_mode
+                        )
+                    else:
+                        action_p1 = current_env.discrete_to_continous_action(discrete_action)
+                else:
+                    action_p1 = agent.act(state)
+                
+                obs_agent2 = current_env.obs_agent_two()
+                deterministic_opponent = phase_config.opponent.deterministic if phase_config.opponent.type == "self_play" else True
+                action_p2 = get_opponent_action(
+                    current_opponent,
+                    obs_agent2,
+                    deterministic=deterministic_opponent
                 )
-
-                # Only store mirrored transitions for transition-based buffers
-                # For episode-based buffers, this would corrupt the episode structure
-                if not is_episode_based_buffer and (
-                    current_opponent is None or phase_config.opponent.type == "none"
-                ):
+                
+                if isinstance(action_p2, (int, np.integer)):
                     if action_fineness is not None:
-                        mirrored_action = utils.mirror_discrete_action(
-                            discrete_action,
+                        action_p2 = discrete_to_continuous_action_with_fineness(
+                            action_p2,
                             fineness=action_fineness,
-                            keep_mode=phase_config.environment.keep_mode,
+                            keep_mode=phase_config.environment.keep_mode
                         )
                     else:
-                        mirrored_action = utils.mirror_discrete_action(discrete_action)
-                    mirrored_action_array = np.array(
-                        [mirrored_action], dtype=np.float32
-                    )
-                    agent.store_transition(
-                        (
+                        action_p2 = current_env.discrete_to_continous_action(action_p2)
+                
+                action = np.hstack([action_p1, action_p2])
+                
+                next_state, reward, done, trunc, info = current_env.step(action)
+                # Ensure next_state is float32 to avoid repeated conversions
+                if next_state.dtype != np.float32:
+                    next_state = next_state.astype(np.float32, copy=False)
+                
+                if reward_weights is not None:
+                    shaped_reward = reward
+                    shaped_reward += info.get('reward_closeness_to_puck', 0.0) * reward_weights['closeness']
+                    shaped_reward += info.get('reward_touch_puck', 0.0) * reward_weights['touch']
+                    shaped_reward += info.get('reward_puck_direction', 0.0) * reward_weights['direction']
+                else:
+                    shaped_reward = reward
+                
+                scaled_reward = shaped_reward * reward_scale
+                
+                if is_agent_discrete:
+                    # Pre-allocate array once and reuse (optimization)
+                    discrete_action_array = np.array([discrete_action], dtype=np.float32)
+                    agent.store_transition((state, discrete_action_array, scaled_reward, next_state, done))
+                    
+                    if current_opponent is None or phase_config.opponent.type == "none":
+                        if action_fineness is not None:
+                            mirrored_action = utils.mirror_discrete_action(
+                                discrete_action,
+                                fineness=action_fineness,
+                                keep_mode=phase_config.environment.keep_mode
+                            )
+                        else:
+                            mirrored_action = utils.mirror_discrete_action(discrete_action)
+                        mirrored_action_array = np.array([mirrored_action], dtype=np.float32)
+                        agent.store_transition((
                             utils.mirror_state(state),
                             mirrored_action_array,
                             scaled_reward,
                             utils.mirror_state(next_state),
-                            done,
-                        ),
-                        winner=winner,
-                    )
-            else:
-                # Ensure action is float32, avoid copy if already correct type
-                if action_p1.dtype != np.float32:
-                    action_array = action_p1.astype(np.float32, copy=False)
+                            done
+                        ))
                 else:
-                    action_array = action_p1
-                agent.store_transition(
-                    (state, action_array, scaled_reward, next_state, done),
-                    winner=winner,
-                )
-
-                # Only store mirrored transitions for transition-based buffers
-                # For episode-based buffers, this would corrupt the episode structure
-                if not is_episode_based_buffer and (
-                    current_opponent is None or phase_config.opponent.type == "none"
-                ):
-                    mirrored_action_array = utils.mirror_action(action_array)
-                    agent.store_transition(
-                        (
+                    # Ensure action is float32, avoid copy if already correct type
+                    if action_p1.dtype != np.float32:
+                        action_array = action_p1.astype(np.float32, copy=False)
+                    else:
+                        action_array = action_p1
+                    agent.store_transition((state, action_array, scaled_reward, next_state, done))
+                    
+                    if current_opponent is None or phase_config.opponent.type == "none":
+                        mirrored_action_array = utils.mirror_action(action_array)
+                        agent.store_transition((
                             utils.mirror_state(state),
                             mirrored_action_array,
                             scaled_reward,
                             utils.mirror_state(next_state),
-                            done,
-                        ),
-                        winner=winner,
-                    )
-
-            state = next_state
-            steps += 1
-            total_reward += reward
-            total_shaped_reward += shaped_reward
-
-            # Log resource usage at intervals
-            if (
-                resource_log_freq > 0
-                and steps - last_resource_log_step >= resource_log_freq
-            ):
-                try:
-                    resource_usage = get_resource_usage()
-                    resource_usage["step"] = steps
-                    resource_usage["episode"] = global_episode
-                    resource_logs.append(resource_usage)
-                    last_resource_log_step = steps
-
-                    # Collect samples for episode averages
-                    cpu_percent = resource_usage.get("cpu_percent", 0)
-                    gpu_utilization = resource_usage.get("gpu_utilization", 0)
-                    if cpu_percent is not None and cpu_percent > 0:
-                        current_episode_cpu_samples.append(cpu_percent)
-                    if gpu_utilization is not None and gpu_utilization > 0:
-                        current_episode_gpu_samples.append(gpu_utilization)
-                except Exception as e:
-                    if verbose:
-                        logger.warning(f"Failed to log resource usage: {e}")
-
-            if steps >= current_phase_start + warmup_steps and steps % train_freq == 0:
-                stats = agent.train(updates_per_step)
-                gradient_steps += updates_per_step
-                if isinstance(stats, dict):
-                    # Track all loss types and gradient norms from stats
-                    for loss_key, loss_value in stats.items():
-                        if loss_value is not None and (
-                            "loss" in loss_key.lower()
-                            or "grad_norm" in loss_key.lower()
-                        ):
-                            if loss_key not in all_losses_dict:
-                                all_losses_dict[loss_key] = []
-                            if loss_key not in episode_losses:
-                                episode_losses[loss_key] = []
-
-                            if isinstance(loss_value, list):
-                                all_losses_dict[loss_key].extend(loss_value)
-                                episode_losses[loss_key].extend(loss_value)
+                            done
+                        ))
+                
+                state = next_state
+                steps += 1
+                total_reward += reward
+                total_shaped_reward += shaped_reward
+                
+                if steps >= current_phase_start + warmup_steps and steps % train_freq == 0:
+                    stats = agent.train(updates_per_step)
+                    gradient_steps += updates_per_step
+                    if isinstance(stats, dict):
+                        loss_list = None
+                        if 'loss' in stats and stats['loss'] is not None:
+                            loss_list = stats['loss']
+                        elif 'critic_loss' in stats and stats['critic_loss'] is not None:
+                            loss_list = stats['critic_loss']
+                        elif 'actor_loss' in stats and stats['actor_loss'] is not None:
+                            loss_list = stats['actor_loss']
+                        
+                        if loss_list is not None:
+                            if isinstance(loss_list, list):
+                                losses.extend(loss_list)
                             else:
-                                all_losses_dict[loss_key].append(loss_value)
-                                episode_losses[loss_key].append(loss_value)
-
-                    # Keep backward compatibility: extract single loss for old loss tracking
-                    loss_list = None
-                    if "loss" in stats and stats["loss"] is not None:
-                        loss_list = stats["loss"]
-                    elif "critic_loss" in stats and stats["critic_loss"] is not None:
-                        loss_list = stats["critic_loss"]
-                    elif "actor_loss" in stats and stats["actor_loss"] is not None:
-                        loss_list = stats["actor_loss"]
-
-                    if loss_list is not None:
-                        if isinstance(loss_list, list):
-                            losses.extend(loss_list)
-                        else:
-                            losses.append(loss_list)
-
-            if (
-                eval_freq_steps is not None
-                and steps - last_eval_step >= eval_freq_steps
-            ):
-                if verbose:
-                    print(f"\nEvaluating agent at step {steps}...")
-
-                temp_checkpoint_path = (
-                    run_manager.models_dir / f"{run_name}_temp_eval.pt"
-                )
-                agent.save(str(temp_checkpoint_path))
-
-                agent_config_dict = {
-                    "type": curriculum.agent.type,
-                    "hyperparameters": curriculum.agent.hyperparameters,
-                }
-
-                try:
-                    # Convert to absolute path to ensure it works even after cwd changes
-                    abs_checkpoint_path = os.path.abspath(str(temp_checkpoint_path))
-                    eval_results = evaluate_agent(
-                        agent_path=abs_checkpoint_path,
-                        agent_config_dict=agent_config_dict,
-                        num_games=eval_num_games,
-                        weak_opponent=eval_weak_opponent,
-                        max_steps=max_episode_steps,
-                        num_parallel=None,
-                        device=device,
-                    )
-
-                    evaluation_results.append(
-                        {
-                            "step": steps,
-                            "episode": global_episode,
-                            "win_rate": eval_results["win_rate"],
-                            "mean_reward": eval_results["mean_reward"],
-                            "std_reward": eval_results["std_reward"],
-                            "wins": eval_results["wins"],
-                            "losses": eval_results["losses"],
-                            "draws": eval_results["draws"],
-                        }
-                    )
-
-                    if evaluation_results:
-                        run_manager.save_evaluation_plot(run_name, evaluation_results)
-                        run_manager.save_evaluation_csv(run_name, evaluation_results)
-
+                                losses.append(loss_list)
+                
+                if eval_freq_steps is not None and steps - last_eval_step >= eval_freq_steps:
                     if verbose:
-                        print(
-                            f"Evaluation results: Win rate: {eval_results['win_rate']:.2%}, "
-                            f"Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}"
+                        print(f"\nEvaluating agent at step {steps}...")
+                    
+                    temp_checkpoint_path = run_manager.models_dir / f"{run_name}_temp_eval.pt"
+                    agent.save(str(temp_checkpoint_path))
+                    
+                    agent_config_dict = {
+                        'type': curriculum.agent.type,
+                        'hyperparameters': curriculum.agent.hyperparameters
+                    }
+                    
+                    try:
+                        eval_results = evaluate_agent(
+                            agent_path=str(temp_checkpoint_path),
+                            agent_config_dict=agent_config_dict,
+                            num_games=eval_num_games,
+                            weak_opponent=eval_weak_opponent,
+                            max_steps=max_episode_steps,
+                            num_parallel=None,
+                            device=device
                         )
-                finally:
-                    if temp_checkpoint_path.exists():
-                        os.remove(temp_checkpoint_path)
-
-                last_eval_step = steps
-
-            if done or trunc:
-                break
-
-        agent.on_episode_end(global_episode)
-
-        # Save 10 random episodes from buffer to CSV every 10 episodes
-        if steps >= warmup_steps and (global_episode + 1) % 10 == 0:
-            if hasattr(agent, "buffer") and hasattr(agent.buffer, "num_eps"):
-                if agent.buffer.num_eps >= 10:
-                    _save_random_episodes_to_csv(
-                        agent,
-                        run_manager,
-                        run_name,
-                        num_episodes=10,
-                        save_episode=global_episode + 1,
-                    )
-
-        rewards.append(total_reward)
-        phases.append(phase_config.name)
-
-        # Calculate episode averages for resource usage
-        episode_cpu_avg = (
-            sum(current_episode_cpu_samples) / len(current_episode_cpu_samples)
-            if current_episode_cpu_samples
-            else 0.0
-        )
-        episode_gpu_avg = (
-            sum(current_episode_gpu_samples) / len(current_episode_gpu_samples)
-            if current_episode_gpu_samples
-            else 0.0
-        )
-        episode_resource_samples.append(
-            {
-                "episode": global_episode,
-                "cpu_percent": episode_cpu_avg,
-                "gpu_utilization": episode_gpu_avg,
-            }
-        )
-
-        # Calculate and print averages over last n episodes (only once every episode_resource_window episodes)
-        if (
-            len(episode_resource_samples) >= episode_resource_window
-            and (global_episode + 1) % episode_resource_window == 0
-        ):
-            recent_samples = episode_resource_samples[-episode_resource_window:]
-            avg_cpu = sum(s["cpu_percent"] for s in recent_samples) / len(
-                recent_samples
-            )
-            avg_gpu = sum(s["gpu_utilization"] for s in recent_samples) / len(
-                recent_samples
-            )
-
-            if verbose:
-                logger.info(
-                    f"Resource Usage (last {episode_resource_window} episodes): "
-                    f"CPU: {avg_cpu:.1f}%, GPU: {avg_gpu:.1f}%"
+                        
+                        evaluation_results.append({
+                            'step': steps,
+                            'episode': global_episode,
+                            'win_rate': eval_results['win_rate'],
+                            'mean_reward': eval_results['mean_reward'],
+                            'std_reward': eval_results['std_reward'],
+                            'wins': eval_results['wins'],
+                            'losses': eval_results['losses'],
+                            'draws': eval_results['draws']
+                        })
+                        
+                        if evaluation_results:
+                            run_manager.save_evaluation_plot(run_name, evaluation_results)
+                            run_manager.save_evaluation_csv(run_name, evaluation_results)
+                        
+                        if verbose:
+                            print(f"Evaluation results: Win rate: {eval_results['win_rate']:.2%}, "
+                                  f"Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}")
+                    finally:
+                        if temp_checkpoint_path.exists():
+                            os.remove(temp_checkpoint_path)
+                    
+                    last_eval_step = steps
+                
+                if done or trunc:
+                    break
+            
+            agent.on_episode_end(global_episode)
+            rewards.append(total_reward)
+            phases.append(phase_config.name)
+            
+            if (global_episode + 1) % checkpoint_save_freq == 0:
+                run_manager.save_checkpoint(
+                    run_name, global_episode + 1, agent,
+                    phase_index=phase_idx,
+                    phase_episode=phase_local_episode
                 )
-
-        if (global_episode + 1) % checkpoint_save_freq == 0:
-            run_manager.save_checkpoint(
-                run_name,
-                global_episode + 1,
-                agent,
-                phase_index=phase_idx,
-                phase_episode=phase_local_episode,
-                episode_logs=episode_logs,
-            )
-
-        # Log all losses at end of episode
-        # Calculate average losses for this episode
-        avg_losses = {}
-        if episode_losses:
-            for loss_key, loss_values in episode_losses.items():
-                if loss_values:
-                    avg_losses[loss_key] = sum(loss_values) / len(loss_values)
-
-        # Calculate backprop reward sum if buffer supports it
-        total_backprop_reward = (
-            total_shaped_reward  # Default to shaped reward if no backprop
-        )
-        if (
-            hasattr(agent, "buffer")
-            and hasattr(agent.buffer, "win_reward_bonus")
-            and hasattr(agent.buffer, "win_reward_discount")
-            and episode_winner is not None
-            and len(episode_scaled_rewards) > 0
-        ):
-            # Apply backprop to calculate the backprop reward sum
-            scaled_rewards_array = np.array(episode_scaled_rewards, dtype=np.float32)
-            backprop_rewards, _, _ = apply_win_reward_backprop(
-                scaled_rewards_array,
-                winner=episode_winner,
-                win_reward_bonus=agent.buffer.win_reward_bonus,
-                win_reward_discount=agent.buffer.win_reward_discount,
-                use_torch=False,
-            )
-            total_backprop_reward = float(np.sum(backprop_rewards))
-
-        # Store episode log (always, even if no losses)
-        episode_logs.append(
-            {
-                "episode": global_episode + 1,
-                "reward": total_reward,  # Original reward
-                "shaped_reward": total_shaped_reward,  # Shaped reward (before backprop)
-                "backprop_reward": total_backprop_reward,  # Reward after backprop
-                "total_gradient_steps": gradient_steps,
-                "losses": avg_losses,
-            }
-        )
-
-        # Log episode information (always log, even if no losses)
-        # Get environment mode and opponent type for logging
-        sampled_mode_str = phase_config.environment.get_mode_for_episode(
-            phase_local_episode
-        )
-        opponent_type = phase_config.opponent.type
-
-        loss_info_parts = [
-            f"Episode {global_episode + 1}: reward={total_reward:.2f}, shaped_reward={total_shaped_reward:.2f}, backprop_reward={total_backprop_reward:.2f}",
-            f"env={sampled_mode_str}",
-            f"opponent={opponent_type}",
-        ]
-        if avg_losses:
-            for loss_key in sorted(avg_losses.keys()):
-                loss_info_parts.append(f"{loss_key}={avg_losses[loss_key]:.6f}")
-        else:
-            loss_info_parts.append("(no training in this episode)")
-        logger.info(" | ".join(loss_info_parts))
-
+            
+            if verbose:
+                pbar.set_postfix({
+                    'reward': total_reward,
+                    'shaped': f'{total_shaped_reward:.1f}',
+                    'phase': phase_config.name[:10],
+                    'eps': agent.config.get('eps', 'N/A') if hasattr(agent, 'config') else 'N/A',
+                })
+    
     run_manager.save_config(run_name, _curriculum_to_dict(curriculum))
     run_manager.save_rewards_csv(run_name, rewards, phases=phases)
     run_manager.save_losses_csv(run_name, losses)
-    if episode_logs:
-        run_manager.save_episode_logs_csv(run_name, episode_logs)
-    if resource_logs:
-        run_manager.save_resources_csv(run_name, resource_logs)
-
+    
     if verbose:
-        print("\nTraining Summary:")
+        print(f"\nTraining Summary:")
         print(f"  Total episodes: {len(rewards)}")
         print(f"  Total steps: {steps}")
         print(f"  Total gradient steps: {gradient_steps}")
         print(f"  Losses collected: {len(losses)}")
         if losses:
-            print(f"  Mean loss: {sum(losses) / len(losses):.4f}")
+            print(f"  Mean loss: {sum(losses)/len(losses):.4f}")
         else:
-            print("  Warning: No losses collected!")
-            print(
-                f"    This might mean training hasn't started (warmup_steps={warmup_steps})"
-            )
-            print(
-                f"    or the replay buffer is too small (current size: {agent.buffer.size})"
-            )
-
+            print(f"  Warning: No losses collected!")
+            print(f"    This might mean training hasn't started (warmup_steps={warmup_steps})")
+            print(f"    or the replay buffer is too small (current size: {agent.buffer.size})")
+    
     run_manager.save_plots(run_name, rewards, losses)
-
+    
     if evaluation_results:
         run_manager.save_evaluation_plot(run_name, evaluation_results)
         run_manager.save_evaluation_csv(run_name, evaluation_results)
-
+    
     model_path = run_manager.get_model_path(run_name)
     agent.save(str(model_path))
-
+    
     current_env.close()
-
+    
     return {
-        "run_name": run_name,
-        "final_reward": rewards[-1] if rewards else 0,
-        "mean_reward": np.mean(rewards[-100:])
-        if len(rewards) >= 100
-        else np.mean(rewards)
-        if rewards
-        else 0,
-        "total_episodes": len(rewards),
-        "total_steps": steps,
-        "total_gradient_steps": gradient_steps,
-        "evaluation_results": evaluation_results,
+        'run_name': run_name,
+        'final_reward': rewards[-1] if rewards else 0,
+        'mean_reward': np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards) if rewards else 0,
+        'total_episodes': len(rewards),
+        'total_steps': steps,
+        'total_gradient_steps': gradient_steps,
+        'evaluation_results': evaluation_results,
     }
 
 
@@ -1080,180 +464,89 @@ def _train_run_vectorized(
     device: Optional[Union[str, int]] = None,
     checkpoint_path: Optional[str] = None,
     num_envs: int = 4,
-    use_threading: bool = False,
-    run_manager: Optional[RunManager] = None,
-    start_episode: int = 0,
-    initial_episode_logs: Optional[List[Dict]] = None,
+    use_threading: bool = False
 ):
     """
     Train with vectorized environments (multiple environments in parallel).
     This provides 1.4-2.4x speedup by batching GPU operations.
-
+    
     Args:
         use_threading: If True, use threaded vectorized env (for Pool workers).
                       If False, use multiprocess vectorized env (better performance).
-        run_manager: Optional RunManager instance to reuse (if None, creates a new one)
-        start_episode: Episode number to start training from (for resuming from checkpoint, default 0)
     """
     set_cuda_device(device)
-
+    
     errors = validate_config(config_path)
     if errors:
-        raise ValueError(
-            "Configuration errors:\n" + "\n".join(f"  - {e}" for e in errors)
-        )
-
+        raise ValueError(f"Configuration errors:\n" + "\n".join(f"  - {e}" for e in errors))
+    
     curriculum = load_curriculum(config_path)
-
-    if run_manager is None:
-        run_manager = RunManager(base_output_dir=base_output_dir)
+    
+    run_manager = RunManager(base_output_dir=base_output_dir)
     if run_name is None:
         config_dict = _curriculum_to_dict(curriculum)
         run_name = run_manager.generate_run_name(config_dict) + f"_vec{num_envs}"
-
+    
     training_params = curriculum.training
-    max_episode_steps = training_params.get("max_episode_steps", 500)
-    updates_per_step = training_params.get("updates_per_step", 1)
-    warmup_steps = training_params.get("warmup_steps", 400)
-    reward_scale = training_params.get("reward_scale", 0.1)
-    checkpoint_save_freq = training_params.get("checkpoint_save_freq", 100)
-    train_freq = training_params.get("train_freq", 1)
-
+    max_episode_steps = training_params.get('max_episode_steps', 500)
+    updates_per_step = training_params.get('updates_per_step', 1)
+    warmup_steps = training_params.get('warmup_steps', 400)
+    reward_scale = training_params.get('reward_scale', 0.1)
+    checkpoint_save_freq = training_params.get('checkpoint_save_freq', 100)
+    train_freq = training_params.get('train_freq', 1)
+    
     total_episodes = get_total_episodes(curriculum)
-
+    
     current_vec_env = None
     current_phase_idx = -1
     current_opponents = [None] * num_envs
     current_phase_start = 0
-
-    action_fineness = curriculum.agent.hyperparameters.get("action_fineness", None)
-
+    
+    action_fineness = curriculum.agent.hyperparameters.get('action_fineness', None)
+    
     # Create temporary environment to get dimensions
     first_phase = curriculum.phases[0]
     initial_mode_str = first_phase.environment.get_mode_for_episode(0)
     env_mode = getattr(h_env.Mode, initial_mode_str)
-    temp_env = h_env.HockeyEnv(
-        mode=env_mode, keep_mode=first_phase.environment.keep_mode
-    )
-    state_dim, agent_action_dim, is_agent_discrete = get_action_space_info(
-        temp_env, curriculum.agent.type, fineness=action_fineness
-    )
+    temp_env = h_env.HockeyEnv(mode=env_mode, keep_mode=first_phase.environment.keep_mode)
+    state_dim, agent_action_dim, is_agent_discrete = get_action_space_info(temp_env, curriculum.agent.type, fineness=action_fineness)
     temp_env.close()
-
+    
     # Create agent
     agent = create_agent(
         curriculum.agent,
         state_dim,
         agent_action_dim,
-        curriculum.hyperparameters,
-        device=device,
+        curriculum.hyperparameters
     )
-
-    # Log the agent network architecture
-    if verbose:
-        print("\n" + agent.log_architecture() + "\n")
-
-    # Log backprop parameters if available
-    if (
-        verbose
-        and hasattr(agent, "buffer")
-        and hasattr(agent.buffer, "win_reward_bonus")
-        and hasattr(agent.buffer, "win_reward_discount")
-    ):
-        print("REWARD BACKPROPAGATION PARAMETERS:")
-        print(f"  win_reward_bonus: {agent.buffer.win_reward_bonus}")
-        print(f"  win_reward_discount: {agent.buffer.win_reward_discount}")
-        print("")
-
-    # Enable fast mode for TDMPC2 if specified in training config
-    if hasattr(agent, "set_fast_mode") and training_params.get("use_fast_mode", False):
-        agent.set_fast_mode(True)
-        if verbose:
-            print(
-                "Fast mode enabled for TDMPC2 (using policy network instead of full MPC)"
-            )
-
+    
     if checkpoint_path is not None:
         if verbose:
             print(f"Loading agent from checkpoint: {checkpoint_path}")
         agent.load(checkpoint_path)
         if verbose:
             print("Checkpoint loaded successfully")
-
-    # Helper function to initialize reward bonus (defined early for use before main loop)
-    def _init_reward_bonus_from_config(episode, curriculum_config, agent_ref):
-        """Initialize reward bonus parameters based on episode number."""
-        phase_idx, phase_local_ep, phase_cfg = get_phase_for_episode(curriculum_config, episode)
-        if phase_cfg.reward_bonus is not None:
-            rb = phase_cfg.reward_bonus
-            N, K = rb.N, rb.K
-            if phase_local_ep < N:
-                win_bonus = rb.WIN_BONUS_START
-                win_discount = rb.WIN_DISCOUNT_START
-            elif phase_local_ep < N + K:
-                alpha = (phase_local_ep - N) / K
-                win_bonus = rb.WIN_BONUS_START * (1 - alpha) + rb.WIN_BONUS_FINAL * alpha
-                win_discount = rb.WIN_DISCOUNT_START * (1 - alpha) + rb.WIN_DISCOUNT_FINAL * alpha
-            else:
-                win_bonus = rb.WIN_BONUS_FINAL
-                win_discount = rb.WIN_DISCOUNT_FINAL
-            
-            if hasattr(agent_ref, "buffer"):
-                if hasattr(agent_ref.buffer, "win_reward_bonus"):
-                    agent_ref.buffer.win_reward_bonus = win_bonus
-                if hasattr(agent_ref.buffer, "win_reward_discount"):
-                    agent_ref.buffer.win_reward_discount = win_discount
-            return {"win_bonus": win_bonus, "win_discount": win_discount}
-        return None
-
-    # Initialize reward bonus parameters based on start_episode (important for resuming)
-    if start_episode > 0:
-        init_bonus = _init_reward_bonus_from_config(start_episode, curriculum, agent)
-        if init_bonus is not None and verbose:
-            print(f"Initialized reward bonus for episode {start_episode}:")
-            print(f"  win_reward_bonus: {init_bonus['win_bonus']:.4f}")
-            print(f"  win_reward_discount: {init_bonus['win_discount']:.4f}")
-
+    
     losses = []
-    all_losses_dict = {}  # Dictionary to store all loss types: {loss_type: [values]}
     rewards = []
     phases = []
-    # Initialize with old episode logs if resuming
-    episode_logs = initial_episode_logs.copy() if initial_episode_logs else []
     steps = 0
     gradient_steps = 0
     evaluation_results = []
     last_eval_step = 0
-    resource_logs = []
-    resource_log_freq = training_params.get(
-        "resource_log_freq", 200
-    )  # Log every N steps
-    last_resource_log_step = 0
-
-    # Track resource usage per episode for averaging
-    episode_resource_window = training_params.get("resource_avg_window_episodes", 10)
-    episode_resource_samples = []  # List of dicts, one per episode
-    current_episode_cpu_samples = []  # CPU samples during current episode
-    current_episode_gpu_samples = []  # GPU samples during current episode
-
+    
     # Track episode state for each parallel environment
     episode_rewards = [0.0] * num_envs
     episode_shaped_rewards = [0.0] * num_envs
-    episode_scaled_rewards = [
-        [] for _ in range(num_envs)
-    ]  # Track scaled rewards per step for backprop
-    episode_winners = [None] * num_envs  # Track winner for each environment
     episode_steps = [0] * num_envs
-    completed_episodes = start_episode
-    # Track losses for current episode (across all training steps)
-    current_episode_losses = {}
-
+    completed_episodes = 0
+    
     def get_reward_weights(episode_idx, phase_config):
         reward_shaping = phase_config.reward_shaping
-
+        
         if reward_shaping is None:
             return None
-
+        
         N = reward_shaping.N
         K = reward_shaping.K
         CLOSENESS_START = reward_shaping.CLOSENESS_START
@@ -1261,97 +554,45 @@ def _train_run_vectorized(
         CLOSENESS_FINAL = reward_shaping.CLOSENESS_FINAL
         TOUCH_FINAL = reward_shaping.TOUCH_FINAL
         DIRECTION_FINAL = reward_shaping.DIRECTION_FINAL
-
-        if episode_idx < N:
-            return {
-                "closeness": CLOSENESS_START,
-                "touch": TOUCH_START,
-                "direction": 0.0,
-            }
-        elif episode_idx < N + K:
-            alpha = (episode_idx - N) / K
-            return {
-                "closeness": CLOSENESS_START * (1 - alpha) + CLOSENESS_FINAL * alpha,
-                "touch": TOUCH_START * (1 - alpha) + TOUCH_FINAL * alpha,
-                "direction": DIRECTION_FINAL * alpha,
-            }
-        else:
-            return {
-                "closeness": CLOSENESS_FINAL,
-                "touch": TOUCH_FINAL,
-                "direction": DIRECTION_FINAL,
-            }
-
-    def get_reward_bonus_values(episode_idx, phase_config):
-        """Get the reward bonus parameters for a given episode within a phase.
         
-        Returns a dict with 'win_bonus' and 'win_discount' values, or None if no
-        reward_bonus config exists for this phase.
-        """
-        reward_bonus = phase_config.reward_bonus
-
-        if reward_bonus is None:
-            return None
-
-        N = reward_bonus.N
-        K = reward_bonus.K
-        WIN_BONUS_START = reward_bonus.WIN_BONUS_START
-        WIN_BONUS_FINAL = reward_bonus.WIN_BONUS_FINAL
-        WIN_DISCOUNT_START = reward_bonus.WIN_DISCOUNT_START
-        WIN_DISCOUNT_FINAL = reward_bonus.WIN_DISCOUNT_FINAL
-
         if episode_idx < N:
             return {
-                "win_bonus": WIN_BONUS_START,
-                "win_discount": WIN_DISCOUNT_START,
+                'closeness': CLOSENESS_START,
+                'touch': TOUCH_START,
+                'direction': 0.0,
             }
         elif episode_idx < N + K:
             alpha = (episode_idx - N) / K
             return {
-                "win_bonus": WIN_BONUS_START * (1 - alpha) + WIN_BONUS_FINAL * alpha,
-                "win_discount": WIN_DISCOUNT_START * (1 - alpha) + WIN_DISCOUNT_FINAL * alpha,
+                'closeness': CLOSENESS_START * (1 - alpha) + CLOSENESS_FINAL * alpha,
+                'touch': TOUCH_START * (1 - alpha) + TOUCH_FINAL * alpha,
+                'direction': DIRECTION_FINAL * alpha,
             }
         else:
             return {
-                "win_bonus": WIN_BONUS_FINAL,
-                "win_discount": WIN_DISCOUNT_FINAL,
+                'closeness': CLOSENESS_FINAL,
+                'touch': TOUCH_FINAL,
+                'direction': DIRECTION_FINAL,
             }
-
-    def update_buffer_reward_bonus(agent, bonus_values):
-        """Update the agent's buffer reward bonus parameters if they exist."""
-        if bonus_values is None:
-            return
-        if not hasattr(agent, "buffer"):
-            return
-        if hasattr(agent.buffer, "win_reward_bonus"):
-            agent.buffer.win_reward_bonus = bonus_values["win_bonus"]
-        if hasattr(agent.buffer, "win_reward_discount"):
-            agent.buffer.win_reward_discount = bonus_values["win_discount"]
-
+    
+    pbar = tqdm(total=total_episodes, desc=run_name, disable=not verbose)
+    
     # Initialize first phase
     phase_idx, phase_local_episode, phase_config = get_phase_for_episode(curriculum, 0)
     current_phase_idx = phase_idx
-
+    
     # Create vectorized environment
     sampled_mode_str = phase_config.environment.get_mode_for_episode(0)
     env_mode = getattr(h_env.Mode, sampled_mode_str)
-
+    
     # Choose appropriate vectorized env class based on whether we're in a Pool worker
-    VecEnvClass = (
-        ThreadedVectorizedHockeyEnvOptimized
-        if use_threading
-        else VectorizedHockeyEnvOptimized
-    )
-
+    VecEnvClass = ThreadedVectorizedHockeyEnvOptimized if use_threading else VectorizedHockeyEnvOptimized
+    
     current_vec_env = VecEnvClass(
         num_envs=num_envs,
-        env_fn=partial(
-            _make_hockey_env,
-            mode=env_mode,
-            keep_mode=phase_config.environment.keep_mode,
-        ),
+        env_fn=partial(_make_hockey_env, mode=env_mode, keep_mode=phase_config.environment.keep_mode)
     )
-
+    
     # Initialize opponents for each environment
     for i in range(num_envs):
         current_opponents[i] = sample_opponent(
@@ -1361,394 +602,150 @@ def _train_run_vectorized(
             agent_config=curriculum.agent,
             state_dim=state_dim,
             action_dim=agent_action_dim,
-            is_discrete=is_agent_discrete,
+            is_discrete=is_agent_discrete
         )
-
+    
     # Reset all environments
     states = current_vec_env.reset()
     if states.dtype != np.float32:
         states = states.astype(np.float32, copy=False)
-
-    t0s = np.ones(num_envs, dtype=bool)
-
+    
     agent.on_episode_start(0)
     reward_weights = get_reward_weights(0, phase_config)
-
+    
     while completed_episodes < total_episodes:
         # Get actions for all environments (batched!)
         if is_agent_discrete:
-            discrete_actions = agent.act_batch(states, t0s=t0s)
+            discrete_actions = agent.act_batch(states)
             if action_fineness is not None:
-                actions_p1 = np.array(
-                    [
-                        discrete_to_continuous_action_with_fineness(
-                            da,
-                            fineness=action_fineness,
-                            keep_mode=phase_config.environment.keep_mode,
-                        )
-                        for da in discrete_actions
-                    ]
-                )
+                actions_p1 = np.array([
+                    discrete_to_continuous_action_with_fineness(
+                        da, fineness=action_fineness, keep_mode=phase_config.environment.keep_mode
+                    ) for da in discrete_actions
+                ])
             else:
                 # Convert standard discrete actions to continuous
-                actions_p1 = np.array(
-                    [
-                        utils.discrete_to_continuous_action_standard(
-                            da, keep_mode=phase_config.environment.keep_mode
-                        )
-                        for da in discrete_actions
-                    ]
-                )
+                actions_p1 = np.array([
+                    utils.discrete_to_continuous_action_standard(
+                        da, keep_mode=phase_config.environment.keep_mode
+                    ) for da in discrete_actions
+                ])
         else:
-            actions_p1 = agent.act_batch(states, t0s=t0s)
-
+            actions_p1 = agent.act_batch(states)
+        
         # Get opponent actions for each environment
         obs_agent2 = current_vec_env.obs_agent_two()
-        actions_p2 = np.array(
-            [
-                get_opponent_action(
-                    current_opponents[i],
-                    obs_agent2[i],
-                    deterministic=phase_config.opponent.deterministic
-                    if phase_config.opponent.type == "self_play"
-                    else True,
-                )
-                for i in range(num_envs)
-            ]
-        )
-
+        actions_p2 = np.array([
+            get_opponent_action(
+                current_opponents[i],
+                obs_agent2[i],
+                deterministic=phase_config.opponent.deterministic if phase_config.opponent.type == "self_play" else True
+            ) for i in range(num_envs)
+        ])
+        
         # Convert discrete actions if needed
         for i in range(num_envs):
             if isinstance(actions_p2[i], (int, np.integer)):
                 if action_fineness is not None:
                     actions_p2[i] = discrete_to_continuous_action_with_fineness(
-                        actions_p2[i],
-                        fineness=action_fineness,
-                        keep_mode=phase_config.environment.keep_mode,
+                        actions_p2[i], fineness=action_fineness, keep_mode=phase_config.environment.keep_mode
                     )
                 else:
                     actions_p2[i] = utils.discrete_to_continuous_action_standard(
                         actions_p2[i], keep_mode=phase_config.environment.keep_mode
                     )
-
+        
         # Combine actions
         full_actions = np.hstack([actions_p1, actions_p2])
-
+        
         # Step all environments
-        next_states, env_rewards, dones, truncs, infos = current_vec_env.step(
-            full_actions
-        )
-        t0s = dones | truncs
+        next_states, env_rewards, dones, truncs, infos = current_vec_env.step(full_actions)
         if next_states.dtype != np.float32:
             next_states = next_states.astype(np.float32, copy=False)
-
+        
         # Count steps: each iteration steps ALL environments, so add num_envs
         steps += num_envs
-
+        
         # Process each environment
         for i in range(num_envs):
             reward = env_rewards[i]
             done = dones[i]
             trunc = truncs[i]
             info = infos[i]
-
+            
             # Apply reward shaping
             if reward_weights is not None:
                 shaped_reward = reward
-                closeness_raw = info.get("reward_closeness_to_puck", 0.0)
-                touch_raw = info.get("reward_touch_puck", 0.0)
-                direction_raw = info.get("reward_puck_direction", 0.0)
-
-                closeness_contribution = closeness_raw * reward_weights["closeness"]
-                touch_contribution = touch_raw * reward_weights["touch"]
-                direction_contribution = direction_raw * reward_weights["direction"]
-
-                shaped_reward += closeness_contribution
-                shaped_reward += touch_contribution
-                shaped_reward += direction_contribution
-
-                # Debug logging when shaped_reward is unusually high (potential bug indicator)
-                # Also log around episode 90, step 17 range
-                should_log = (
-                    abs(shaped_reward) > 25.0  # Unusually high reward
-                    or (
-                        completed_episodes >= 89
-                        and completed_episodes <= 91
-                        and episode_steps[i] >= 15
-                        and episode_steps[i] <= 19
-                    )
-                )
-
-                if should_log:
-                    logger.warning(
-                        f"[DEBUG REWARD] env={i}, completed_ep={completed_episodes}, step={episode_steps[i]}: "
-                        f"base_reward={reward:.6f}, "
-                        f"closeness_raw={closeness_raw:.6f}, "
-                        f"touch_raw={touch_raw:.6f}, "
-                        f"direction_raw={direction_raw:.6f}, "
-                        f"weights_closeness={reward_weights['closeness']:.6f}, "
-                        f"weights_touch={reward_weights['touch']:.6f}, "
-                        f"weights_direction={reward_weights['direction']:.6f}, "
-                        f"closeness_contrib={closeness_contribution:.6f}, "
-                        f"touch_contrib={touch_contribution:.6f}, "
-                        f"direction_contrib={direction_contribution:.6f}, "
-                        f"shaped_reward={shaped_reward:.6f}, "
-                        f"reward_scale={reward_scale:.6f}"
-                    )
+                shaped_reward += info.get('reward_closeness_to_puck', 0.0) * reward_weights['closeness']
+                shaped_reward += info.get('reward_touch_puck', 0.0) * reward_weights['touch']
+                shaped_reward += info.get('reward_puck_direction', 0.0) * reward_weights['direction']
             else:
                 shaped_reward = reward
-
+            
             scaled_reward = shaped_reward * reward_scale
-
-            # Debug logging for scaled reward (after scaling)
-            should_log_scaled = (
-                abs(scaled_reward) > 25.0  # Unusually high reward
-                or (
-                    completed_episodes >= 89
-                    and completed_episodes <= 91
-                    and episode_steps[i] >= 15
-                    and episode_steps[i] <= 19
-                )
-            )
-
-            if should_log_scaled:
-                logger.warning(
-                    f"[DEBUG SCALED] env={i}, completed_ep={completed_episodes}, step={episode_steps[i]}: "
-                    f"scaled_reward={scaled_reward:.6f}, "
-                    f"done={done}, "
-                    f"winner={info.get('winner', None)}"
-                )
-
-            # Track scaled rewards for backprop calculation
-            episode_scaled_rewards[i].append(scaled_reward)
-
-            # Get winner information if episode is done (only pass when done=True, not trunc)
-            # Winner is 1 for agent win, -1 for agent loss
-            winner = None
-            if done:
-                winner = info.get("winner", 0)
-                episode_winners[i] = winner
-
+            
             # Store transition
             if is_agent_discrete:
-                discrete_action_array = np.array(
-                    [discrete_actions[i]], dtype=np.float32
-                )
-                agent.store_transition(
-                    (
-                        states[i],
-                        discrete_action_array,
-                        scaled_reward,
-                        next_states[i],
-                        done,
-                    ),
-                    winner=winner,
-                )
+                discrete_action_array = np.array([discrete_actions[i]], dtype=np.float32)
+                agent.store_transition((states[i], discrete_action_array, scaled_reward, next_states[i], done))
             else:
-                action_array = (
-                    actions_p1[i].astype(np.float32, copy=False)
-                    if actions_p1[i].dtype != np.float32
-                    else actions_p1[i]
-                )
-                agent.store_transition(
-                    (states[i], action_array, scaled_reward, next_states[i], done),
-                    winner=winner,
-                )
-
+                action_array = actions_p1[i].astype(np.float32, copy=False) if actions_p1[i].dtype != np.float32 else actions_p1[i]
+                agent.store_transition((states[i], action_array, scaled_reward, next_states[i], done))
+            
             # Track episode stats
             episode_rewards[i] += reward
             episode_shaped_rewards[i] += shaped_reward
             episode_steps[i] += 1
-
+            
             # Handle episode completion
             if done or trunc or episode_steps[i] >= max_episode_steps:
                 rewards.append(episode_rewards[i])
                 phases.append(phase_config.name)
                 completed_episodes += 1
-
-                # Calculate episode averages for resource usage (using recent samples)
-                episode_cpu_avg = (
-                    sum(current_episode_cpu_samples[-20:])
-                    / len(current_episode_cpu_samples[-20:])
-                    if current_episode_cpu_samples
-                    else 0.0
-                )
-                episode_gpu_avg = (
-                    sum(current_episode_gpu_samples[-20:])
-                    / len(current_episode_gpu_samples[-20:])
-                    if current_episode_gpu_samples
-                    else 0.0
-                )
-                episode_resource_samples.append(
-                    {
-                        "episode": completed_episodes,
-                        "cpu_percent": episode_cpu_avg,
-                        "gpu_utilization": episode_gpu_avg,
-                    }
-                )
-
-                # Calculate and print averages over last n episodes (only once every episode_resource_window episodes)
-                if (
-                    len(episode_resource_samples) >= episode_resource_window
-                    and completed_episodes % episode_resource_window == 0
-                ):
-                    recent_samples = episode_resource_samples[-episode_resource_window:]
-                    avg_cpu = sum(s["cpu_percent"] for s in recent_samples) / len(
-                        recent_samples
-                    )
-                    avg_gpu = sum(s["gpu_utilization"] for s in recent_samples) / len(
-                        recent_samples
-                    )
-
-                    if verbose:
-                        logger.info(
-                            f"Resource Usage (last {episode_resource_window} episodes): "
-                            f"CPU: {avg_cpu:.1f}%, GPU: {avg_gpu:.1f}%"
-                        )
-
-                # Trim resource sample lists to prevent unbounded growth
-                # Keep only last 100 samples (enough for ~5 episodes at 20 samples/episode)
-                if len(current_episode_cpu_samples) > 100:
-                    current_episode_cpu_samples = current_episode_cpu_samples[-100:]
-                if len(current_episode_gpu_samples) > 100:
-                    current_episode_gpu_samples = current_episode_gpu_samples[-100:]
-
-                # Save episode rewards before resetting (for logging)
-                final_episode_reward = episode_rewards[i]
-                final_episode_shaped_reward = episode_shaped_rewards[i]
-
-                # Calculate backprop reward sum if buffer supports it
-                final_episode_backprop_reward = final_episode_shaped_reward  # Default to shaped reward if no backprop
-                if (
-                    hasattr(agent, "buffer")
-                    and hasattr(agent.buffer, "win_reward_bonus")
-                    and hasattr(agent.buffer, "win_reward_discount")
-                    and episode_winners[i] is not None
-                    and len(episode_scaled_rewards[i]) > 0
-                ):
-                    # Apply backprop to calculate the backprop reward sum
-                    scaled_rewards_array = np.array(
-                        episode_scaled_rewards[i], dtype=np.float32
-                    )
-                    backprop_rewards, _, _ = apply_win_reward_backprop(
-                        scaled_rewards_array,
-                        winner=episode_winners[i],
-                        win_reward_bonus=agent.buffer.win_reward_bonus,
-                        win_reward_discount=agent.buffer.win_reward_discount,
-                        use_torch=False,
-                    )
-                    final_episode_backprop_reward = float(np.sum(backprop_rewards))
-
+                pbar.update(1)
+                
                 # Reset episode tracking for this environment
                 episode_rewards[i] = 0.0
                 episode_shaped_rewards[i] = 0.0
-                episode_scaled_rewards[i] = []
-                episode_winners[i] = None
                 episode_steps[i] = 0
-
+                
                 # IMPORTANT: We need to manually reset this environment!
                 # The next_states[i] is already the reset state from the vectorized env
                 # But we need to initialize the next episode
                 agent.on_episode_start(completed_episodes)
-
-                # Save 10 random episodes from buffer to CSV every 10 episodes
-                if steps >= warmup_steps and completed_episodes % 10 == 0:
-                    if hasattr(agent, "buffer") and hasattr(agent.buffer, "num_eps"):
-                        if agent.buffer.num_eps >= 10:
-                            _save_random_episodes_to_csv(
-                                agent,
-                                run_manager,
-                                run_name,
-                                num_episodes=10,
-                                save_episode=completed_episodes,
-                            )
-
-                # Log all losses at end of episode
-                # Calculate average losses for this episode
-                avg_losses = {}
-                if current_episode_losses:
-                    for loss_key, loss_values in current_episode_losses.items():
-                        if loss_values:
-                            avg_losses[loss_key] = sum(loss_values) / len(loss_values)
-
-                # Store episode log (always, even if no losses)
-                episode_logs.append(
-                    {
-                        "episode": completed_episodes,
-                        "reward": final_episode_reward,  # Original reward
-                        "shaped_reward": final_episode_shaped_reward,  # Shaped reward (before backprop)
-                        "backprop_reward": final_episode_backprop_reward,  # Reward after backprop
-                        "total_gradient_steps": gradient_steps,
-                        "losses": avg_losses,
-                    }
-                )
-
-                # Log episode information (always log, even if no losses)
-                # Get environment mode and opponent type for logging
-                # Compute phase_local_episode for current episode
-                _, current_phase_local_episode, current_phase_config = (
-                    get_phase_for_episode(curriculum, completed_episodes)
-                )
-                sampled_mode_str = (
-                    current_phase_config.environment.get_mode_for_episode(
-                        current_phase_local_episode
-                    )
-                )
-                opponent_type = current_phase_config.opponent.type
-
-                loss_info_parts = [
-                    f"Episode {completed_episodes}: reward={final_episode_reward:.2f}, shaped_reward={final_episode_shaped_reward:.2f}, backprop_reward={final_episode_backprop_reward:.2f}",
-                    f"env={sampled_mode_str}",
-                    f"opponent={opponent_type}",
-                ]
-                if avg_losses:
-                    for loss_key in sorted(avg_losses.keys()):
-                        loss_info_parts.append(f"{loss_key}={avg_losses[loss_key]:.6f}")
-                else:
-                    loss_info_parts.append("(no training in this episode)")
-                logger.info(" | ".join(loss_info_parts))
-
-                # Reset episode losses after logging (for next episode)
-                current_episode_losses = {}
-
+                
+                # Update progress bar
+                if verbose:
+                    pbar.set_postfix({
+                        'reward': rewards[-1] if rewards else 0,
+                        'phase': phase_config.name[:10],
+                        'eps': agent.config.get('eps', 'N/A') if hasattr(agent, 'config') else 'N/A',
+                    })
+                
                 # Check if we need to change phase
                 if completed_episodes < total_episodes:
-                    new_phase_idx, new_phase_local_episode, new_phase_config = (
-                        get_phase_for_episode(curriculum, completed_episodes)
-                    )
-
+                    new_phase_idx, new_phase_local_episode, new_phase_config = get_phase_for_episode(curriculum, completed_episodes)
+                    
                     if new_phase_idx != current_phase_idx:
                         if verbose:
-                            print(
-                                f"\nTransitioning to phase {new_phase_idx + 1}/{len(curriculum.phases)}: {new_phase_config.name}"
-                            )
-
+                            print(f"\nTransitioning to phase {new_phase_idx + 1}/{len(curriculum.phases)}: {new_phase_config.name}")
+                        
                         # Clear buffer for phase transition
                         if verbose:
-                            print(
-                                f"  Clearing replay buffer (size: {agent.buffer.size})"
-                            )
+                            print(f"  Clearing replay buffer (size: {agent.buffer.size})")
                         agent.buffer.clear()
-
+                        
                         # Recreate environments with new phase settings
                         current_vec_env.close()
-
-                        sampled_mode_str = (
-                            new_phase_config.environment.get_mode_for_episode(
-                                new_phase_local_episode
-                            )
-                        )
+                        
+                        sampled_mode_str = new_phase_config.environment.get_mode_for_episode(new_phase_local_episode)
                         env_mode = getattr(h_env.Mode, sampled_mode_str)
-
+                        
                         current_vec_env = VecEnvClass(
                             num_envs=num_envs,
-                            env_fn=partial(
-                                _make_hockey_env,
-                                mode=env_mode,
-                                keep_mode=new_phase_config.environment.keep_mode,
-                            ),
+                            env_fn=partial(_make_hockey_env, mode=env_mode, keep_mode=new_phase_config.environment.keep_mode)
                         )
-
+                        
                         # Update opponents
                         for j in range(num_envs):
                             current_opponents[j] = sample_opponent(
@@ -1758,244 +755,171 @@ def _train_run_vectorized(
                                 agent_config=curriculum.agent,
                                 state_dim=state_dim,
                                 action_dim=agent_action_dim,
-                                is_discrete=is_agent_discrete,
+                                is_discrete=is_agent_discrete
                             )
-
+                        
                         current_phase_idx = new_phase_idx
                         current_phase_start = steps
                         phase_config = new_phase_config
-
-                    # Update reward weights for the next episode (must be done for every episode,
-                    # not just phase transitions, since reward shaping depends on phase_local_episode)
-                    reward_weights = get_reward_weights(
-                        new_phase_local_episode, new_phase_config
-                    )
-                    
-                    # Update reward bonus parameters for the next episode
-                    bonus_values = get_reward_bonus_values(
-                        new_phase_local_episode, new_phase_config
-                    )
-                    update_buffer_reward_bonus(agent, bonus_values)
-
+                        reward_weights = get_reward_weights(new_phase_local_episode, phase_config)
+                
                 # Note: individual environment resets are handled automatically by vectorized env
-
+        
         # Update states
         states = next_states
-
-        # Log resource usage at intervals
-        if (
-            resource_log_freq > 0
-            and steps - last_resource_log_step >= resource_log_freq
-        ):
-            try:
-                resource_usage = get_resource_usage()
-                resource_usage["step"] = steps
-                resource_usage["episode"] = completed_episodes
-                resource_logs.append(resource_usage)
-                last_resource_log_step = steps
-
-                # Collect samples for episode averages
-                cpu_percent = resource_usage.get("cpu_percent", 0)
-                gpu_utilization = resource_usage.get("gpu_utilization", 0)
-                if cpu_percent is not None and cpu_percent > 0:
-                    current_episode_cpu_samples.append(cpu_percent)
-                if gpu_utilization is not None and gpu_utilization > 0:
-                    current_episode_gpu_samples.append(gpu_utilization)
-            except Exception as e:
-                if verbose:
-                    logger.warning(f"Failed to log resource usage: {e}")
-
+        
         # Train agent
         if steps >= current_phase_start + warmup_steps and steps % train_freq == 0:
             stats = agent.train(updates_per_step)
             gradient_steps += updates_per_step
             if isinstance(stats, dict):
-                # Track all loss types and gradient norms from stats
-                for loss_key, loss_value in stats.items():
-                    if loss_value is not None and (
-                        "loss" in loss_key.lower() or "grad_norm" in loss_key.lower()
-                    ):
-                        if loss_key not in all_losses_dict:
-                            all_losses_dict[loss_key] = []
-                        if loss_key not in current_episode_losses:
-                            current_episode_losses[loss_key] = []
-
-                        if isinstance(loss_value, list):
-                            all_losses_dict[loss_key].extend(loss_value)
-                            current_episode_losses[loss_key].extend(loss_value)
-                        else:
-                            all_losses_dict[loss_key].append(loss_value)
-                            current_episode_losses[loss_key].append(loss_value)
-
-                # Keep backward compatibility: extract single loss for old loss tracking
                 loss_list = None
-                if "loss" in stats and stats["loss"] is not None:
-                    loss_list = stats["loss"]
-                elif "critic_loss" in stats and stats["critic_loss"] is not None:
-                    loss_list = stats["critic_loss"]
-                elif "actor_loss" in stats and stats["actor_loss"] is not None:
-                    loss_list = stats["actor_loss"]
-
+                if 'loss' in stats and stats['loss'] is not None:
+                    loss_list = stats['loss']
+                elif 'critic_loss' in stats and stats['critic_loss'] is not None:
+                    loss_list = stats['critic_loss']
+                elif 'actor_loss' in stats and stats['actor_loss'] is not None:
+                    loss_list = stats['actor_loss']
+                
                 if loss_list is not None:
                     if isinstance(loss_list, list):
                         losses.extend(loss_list)
                     else:
                         losses.append(loss_list)
-
+        
         # Evaluation
         if eval_freq_steps is not None and steps - last_eval_step >= eval_freq_steps:
             if verbose:
                 print(f"\nEvaluating agent at step {steps}...")
-
+            
             temp_checkpoint_path = run_manager.models_dir / f"{run_name}_temp_eval.pt"
             agent.save(str(temp_checkpoint_path))
-
+            
             agent_config_dict = {
-                "type": curriculum.agent.type,
-                "hyperparameters": curriculum.agent.hyperparameters,
+                'type': curriculum.agent.type,
+                'hyperparameters': curriculum.agent.hyperparameters
             }
-
+            
             try:
-                # Convert to absolute path to ensure it works even after cwd changes
-                abs_checkpoint_path = os.path.abspath(str(temp_checkpoint_path))
                 eval_results = evaluate_agent(
-                    agent_path=abs_checkpoint_path,
+                    agent_path=str(temp_checkpoint_path),
                     agent_config_dict=agent_config_dict,
                     num_games=eval_num_games,
                     weak_opponent=eval_weak_opponent,
                     max_steps=max_episode_steps,
                     num_parallel=None,
-                    device=device,
-                )
-
-                evaluation_results.append(
-                    {
-                        "step": steps,
-                        "episode": completed_episodes,
-                        "win_rate": eval_results["win_rate"],
-                        "mean_reward": eval_results["mean_reward"],
-                        "std_reward": eval_results["std_reward"],
-                        "wins": eval_results["wins"],
-                        "losses": eval_results["losses"],
-                        "draws": eval_results["draws"],
-                    }
+                    device=device
                 )
-
+                
+                evaluation_results.append({
+                    'step': steps,
+                    'episode': completed_episodes,
+                    'win_rate': eval_results['win_rate'],
+                    'mean_reward': eval_results['mean_reward'],
+                    'std_reward': eval_results['std_reward'],
+                    'wins': eval_results['wins'],
+                    'losses': eval_results['losses'],
+                    'draws': eval_results['draws']
+                })
+                
                 if evaluation_results:
                     run_manager.save_evaluation_plot(run_name, evaluation_results)
                     run_manager.save_evaluation_csv(run_name, evaluation_results)
-
+                
                 if verbose:
-                    print(
-                        f"Evaluation results: Win rate: {eval_results['win_rate']:.2%}, "
-                        f"Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}"
-                    )
+                    print(f"Evaluation results: Win rate: {eval_results['win_rate']:.2%}, "
+                          f"Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}")
             finally:
                 if temp_checkpoint_path.exists():
                     os.remove(temp_checkpoint_path)
-
+            
             last_eval_step = steps
-
+        
         # Checkpoint saving
         if completed_episodes > 0 and completed_episodes % checkpoint_save_freq == 0:
             run_manager.save_checkpoint(
-                run_name,
-                completed_episodes,
-                agent,
+                run_name, completed_episodes, agent,
                 phase_index=phase_idx,
-                phase_episode=phase_local_episode,
-                episode_logs=episode_logs,
+                phase_episode=phase_local_episode
             )
-
+    
+    pbar.close()
+    
     # Save final results
     run_manager.save_config(run_name, _curriculum_to_dict(curriculum))
     run_manager.save_rewards_csv(run_name, rewards, phases=phases)
     run_manager.save_losses_csv(run_name, losses)
-    if episode_logs:
-        run_manager.save_episode_logs_csv(run_name, episode_logs)
-    if resource_logs:
-        run_manager.save_resources_csv(run_name, resource_logs)
-
+    
     if verbose:
-        print("\nTraining Summary:")
+        print(f"\nTraining Summary:")
         print(f"  Total episodes: {len(rewards)}")
         print(f"  Total steps: {steps}")
         print(f"  Total gradient steps: {gradient_steps}")
         print(f"  Losses collected: {len(losses)}")
         print(f"  Vectorized environments: {num_envs}")
-        print(
-            f"  Average steps per episode: {steps / len(rewards) if rewards else 0:.1f}"
-        )
-        print(
-            f"  Average episode length per env: {steps / num_envs / len(rewards) if rewards else 0:.1f}"
-        )
+        print(f"  Average steps per episode: {steps / len(rewards) if rewards else 0:.1f}")
+        print(f"  Average episode length per env: {steps / num_envs / len(rewards) if rewards else 0:.1f}")
         if losses:
-            print(f"  Mean loss: {sum(losses) / len(losses):.4f}")
-
+            print(f"  Mean loss: {sum(losses)/len(losses):.4f}")
+    
     run_manager.save_plots(run_name, rewards, losses)
-
+    
     if evaluation_results:
         run_manager.save_evaluation_plot(run_name, evaluation_results)
         run_manager.save_evaluation_csv(run_name, evaluation_results)
-
+    
     model_path = run_manager.get_model_path(run_name)
     agent.save(str(model_path))
-
+    
     current_vec_env.close()
-
+    
     return {
-        "run_name": run_name,
-        "final_reward": rewards[-1] if rewards else 0,
-        "mean_reward": np.mean(rewards[-100:])
-        if len(rewards) >= 100
-        else np.mean(rewards)
-        if rewards
-        else 0,
-        "total_episodes": len(rewards),
-        "total_steps": steps,
-        "total_gradient_steps": gradient_steps,
-        "evaluation_results": evaluation_results,
+        'run_name': run_name,
+        'final_reward': rewards[-1] if rewards else 0,
+        'mean_reward': np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards) if rewards else 0,
+        'total_episodes': len(rewards),
+        'total_steps': steps,
+        'total_gradient_steps': gradient_steps,
+        'evaluation_results': evaluation_results,
     }
 
 
 def _curriculum_to_dict(curriculum: CurriculumConfig) -> dict:
     """Convert curriculum config to dictionary for saving."""
     return {
-        "curriculum": {
-            "phases": [
+        'curriculum': {
+            'phases': [
                 {
-                    "name": phase.name,
-                    "episodes": phase.episodes,
-                    "environment": {
-                        "mode": phase.environment.mode,
-                        "keep_mode": phase.environment.keep_mode,
+                    'name': phase.name,
+                    'episodes': phase.episodes,
+                    'environment': {
+                        'mode': phase.environment.mode,
+                        'keep_mode': phase.environment.keep_mode,
                     },
-                    "opponent": {
-                        "type": phase.opponent.type,
-                        "weight": phase.opponent.weight,
-                        "checkpoint": phase.opponent.checkpoint,
-                        "deterministic": phase.opponent.deterministic,
-                        "opponents": phase.opponent.opponents,
-                    },
-                    "reward_shaping": None
-                    if phase.reward_shaping is None
-                    else {
-                        "N": phase.reward_shaping.N,
-                        "K": phase.reward_shaping.K,
-                        "CLOSENESS_START": phase.reward_shaping.CLOSENESS_START,
-                        "TOUCH_START": phase.reward_shaping.TOUCH_START,
-                        "CLOSENESS_FINAL": phase.reward_shaping.CLOSENESS_FINAL,
-                        "TOUCH_FINAL": phase.reward_shaping.TOUCH_FINAL,
-                        "DIRECTION_FINAL": phase.reward_shaping.DIRECTION_FINAL,
+                    'opponent': {
+                        'type': phase.opponent.type,
+                        'weight': phase.opponent.weight,
+                        'checkpoint': phase.opponent.checkpoint,
+                        'deterministic': phase.opponent.deterministic,
+                        'opponents': phase.opponent.opponents,
                     },
+                    'reward_shaping': None if phase.reward_shaping is None else {
+                        'N': phase.reward_shaping.N,
+                        'K': phase.reward_shaping.K,
+                        'CLOSENESS_START': phase.reward_shaping.CLOSENESS_START,
+                        'TOUCH_START': phase.reward_shaping.TOUCH_START,
+                        'CLOSENESS_FINAL': phase.reward_shaping.CLOSENESS_FINAL,
+                        'TOUCH_FINAL': phase.reward_shaping.TOUCH_FINAL,
+                        'DIRECTION_FINAL': phase.reward_shaping.DIRECTION_FINAL,
+                    }
                 }
                 for phase in curriculum.phases
             ]
         },
-        "hyperparameters": curriculum.hyperparameters,
-        "training": curriculum.training,
-        "agent": {
-            "type": curriculum.agent.type,
-            "hyperparameters": curriculum.agent.hyperparameters,
-        },
+        'hyperparameters': curriculum.hyperparameters,
+        'training': curriculum.training,
+        'agent': {
+            'type': curriculum.agent.type,
+            'hyperparameters': curriculum.agent.hyperparameters,
+        }
     }
diff --git a/src/rl_hockey/common/training/train_single_run.py b/src/rl_hockey/common/training/train_single_run.py
index 6632420..e9d2005 100644
--- a/src/rl_hockey/common/training/train_single_run.py
+++ b/src/rl_hockey/common/training/train_single_run.py
@@ -4,26 +4,9 @@ Can use either JSON config file or dict config.
 Supports vectorized environments for faster training.
 """
 
-import logging
-import sys
 from typing import Optional, Union
 
-# Configure logging (ensure it's configured before importing train_run)
-# Unbuffer stdout for immediate output in batch jobs
-if hasattr(sys.stdout, "reconfigure"):
-    sys.stdout.reconfigure(line_buffering=True)
-
-logging.basicConfig(
-    level=logging.INFO,
-    format="[%(asctime)s] [%(levelname)s] %(message)s",
-    datefmt="%Y-%m-%d %H:%M:%S",
-    handlers=[logging.StreamHandler(sys.stdout)],
-    force=True,  # Force reconfiguration if already configured
-)
-
-from rl_hockey.common.training.curriculum_manager import load_curriculum
-from rl_hockey.common.training.run_manager import RunManager
-from rl_hockey.common.training.train_run import _curriculum_to_dict, train_run
+from rl_hockey.common.training.train_run import train_run
 
 
 def train_single_run(
@@ -31,12 +14,12 @@ def train_single_run(
     base_output_dir: str = "results/runs",
     run_name: str = None,
     verbose: bool = True,
-    eval_freq_steps: int = 100_000,
-    eval_num_games: int = 100,
+    eval_freq_steps: int = 800000,
+    eval_num_games: int = 200,
     eval_weak_opponent: bool = True,
     device: Optional[Union[str, int]] = None,
     checkpoint_path: Optional[str] = None,
-    num_envs: int = 4,
+    num_envs: int = 1,
 ):
     """
     Train a single run with optional vectorized environments.
@@ -54,26 +37,6 @@ def train_single_run(
         num_envs: Number of parallel environments (1 = no vectorization, 4-8 recommended)
                     4 cores: use 2, 8 cores: use 4, 12+ cores: use 8 (max recommended)
     """
-    # Load curriculum to generate run_name and save config
-    curriculum = load_curriculum(config_path)
-    config_dict = _curriculum_to_dict(curriculum)
-
-    # Create RunManager to set up directory structure
-    run_manager = RunManager(base_output_dir=base_output_dir)
-
-    # Generate run_name if not provided
-    if run_name is None:
-        run_name = run_manager.generate_run_name(config_dict)
-
-    # Save config file before starting training
-    if verbose:
-        logging.info(f"Saving config file for run: {run_name}")
-    run_manager.save_config(run_name, config_dict)
-    if verbose:
-        logging.info(
-            f"Config saved to: {run_manager.get_run_directories(run_name)['config']}"
-        )
-
     return train_run(
         config_path,
         base_output_dir,
@@ -85,49 +48,32 @@ def train_single_run(
         device=device,
         checkpoint_path=checkpoint_path,
         num_envs=num_envs,
-        run_manager=run_manager,
     )
 
 
-
 if __name__ == "__main__":
-    import os
-
     import torch
 
-    # Device diagnostics (help debug "skipping cudagraphs due to cpu device" etc.)
-    cuda_vis = os.environ.get("CUDA_VISIBLE_DEVICES", "not set")
-    print(f"[Device] CUDA_VISIBLE_DEVICES={cuda_vis}")
-    print(f"[Device] torch.cuda.is_available()={torch.cuda.is_available()}")
-    if torch.cuda.is_available():
-        print(f"[Device] torch.cuda.device_count()={torch.cuda.device_count()}")
-
-    # Enable TF32 for better performance on Ampere+ GPUs
-    if torch.cuda.is_available():
-        torch.set_float32_matmul_precision("high")
-
-    path_to_config = "configs/curriculum_tdmpc2_bonus_decay.json"
+    path_to_config = "configs/curriculum_simple.json"
 
     # Auto-detect device
     if torch.cuda.is_available():
         device = "cuda:0"
-        print(f"[Device] Using GPU {device} ({torch.cuda.get_device_name(0)})")
     else:
         device = "cpu"
-        print(
-            "[Device] Using CPU (no GPU). TD-MPC2 repo expects GPU; use sbatch on a GPU partition."
-        )
 
     # Get num_envs from environment variable if set, otherwise use default
+    import os
+
     num_envs = int(
-        os.environ.get("NUM_ENVS", "1")
-    )  # Default to 4 for parallel environments
+        os.environ.get("NUM_ENVS", "24")
+    )  # Default to 24 for 24 CPU cores (max)
 
     train_single_run(
         path_to_config,
-        base_output_dir="results/tdmpc2_runs",
+        base_output_dir="results/hyperparameter_runs",
         device=device,
         num_envs=num_envs,
     )
 
-    # nohup python -u src/rl_hockey/common/training/train_single_run.py > results/tdmpc2_runs/train_single_run.log 2>&1 &
+    # nohup python src/rl_hockey/common/training/train_single_run.py > train_single_run.log 2>&1 &
diff --git a/src/rl_hockey/common/utils.py b/src/rl_hockey/common/utils.py
index e192a09..00a1ae1 100644
--- a/src/rl_hockey/common/utils.py
+++ b/src/rl_hockey/common/utils.py
@@ -1,8 +1,6 @@
 import numpy as np
 import torch
-import psutil
-import time
-from typing import Optional, Union, Dict, Any
+from typing import Optional, Union
 
 
 def mirror_state(state):
@@ -210,138 +208,3 @@ def set_cuda_device(device: Optional[Union[str, int]]):
     elif torch.cuda.is_available():
         torch.cuda.empty_cache()
 
-
-def get_resource_usage() -> Dict[str, Any]:
-    """
-    Collect GPU and CPU usage metrics.
-    
-    Returns:
-        Dictionary with resource usage metrics including:
-        - gpu_utilization: GPU utilization percentage (0-100)
-        - gpu_memory_used: GPU memory used in MB
-        - gpu_memory_total: Total GPU memory in MB
-        - gpu_memory_percent: GPU memory usage percentage
-        - gpu_temperature: GPU temperature in Celsius
-        - cpu_percent: CPU usage percentage (average across all cores)
-        - cpu_per_core: CPU usage per core (list of percentages)
-        - memory_used: System memory used in MB
-        - memory_total: Total system memory in MB
-        - memory_percent: System memory usage percentage
-        - load_avg: System load average (1, 5, 15 min)
-    """
-    metrics = {
-        'timestamp': time.time(),
-    }
-    
-    # CPU metrics
-    try:
-        cpu_percent = psutil.cpu_percent(interval=0.1)
-        cpu_per_core = psutil.cpu_percent(interval=0.1, percpu=True)
-        load_avg = psutil.getloadavg() if hasattr(psutil, 'getloadavg') else (0, 0, 0)
-        
-        metrics['cpu_percent'] = cpu_percent
-        metrics['cpu_per_core'] = cpu_per_core
-        metrics['cpu_cores'] = len(cpu_per_core)
-        metrics['load_avg_1min'] = load_avg[0] if len(load_avg) > 0 else 0
-        metrics['load_avg_5min'] = load_avg[1] if len(load_avg) > 1 else 0
-        metrics['load_avg_15min'] = load_avg[2] if len(load_avg) > 2 else 0
-    except Exception as e:
-        metrics['cpu_percent'] = 0
-        metrics['cpu_per_core'] = []
-        metrics['cpu_cores'] = 0
-        metrics['load_avg_1min'] = 0
-        metrics['load_avg_5min'] = 0
-        metrics['load_avg_15min'] = 0
-    
-    # Memory metrics
-    try:
-        mem = psutil.virtual_memory()
-        metrics['memory_used'] = mem.used / (1024 * 1024)  # MB
-        metrics['memory_total'] = mem.total / (1024 * 1024)  # MB
-        metrics['memory_percent'] = mem.percent
-        metrics['memory_available'] = mem.available / (1024 * 1024)  # MB
-    except Exception as e:
-        metrics['memory_used'] = 0
-        metrics['memory_total'] = 0
-        metrics['memory_percent'] = 0
-        metrics['memory_available'] = 0
-    
-    # GPU metrics
-    try:
-        if torch.cuda.is_available():
-            device = torch.cuda.current_device()
-            
-            # GPU utilization and memory
-            memory_allocated = torch.cuda.memory_allocated(device) / (1024 * 1024)  # MB
-            memory_reserved = torch.cuda.memory_reserved(device) / (1024 * 1024)  # MB
-            memory_total = torch.cuda.get_device_properties(device).total_memory / (1024 * 1024)  # MB
-            
-            # Try to get utilization via nvidia-smi (if available)
-            gpu_util = None
-            gpu_temp = None
-            try:
-                import subprocess
-                result = subprocess.run(
-                    ['nvidia-smi', '--query-gpu=utilization.gpu,temperature.gpu', '--format=csv,noheader,nounits'],
-                    capture_output=True,
-                    text=True,
-                    timeout=1
-                )
-                if result.returncode == 0:
-                    parts = result.stdout.strip().split(', ')
-                    if len(parts) >= 2:
-                        gpu_util = float(parts[0].strip())
-                        gpu_temp = float(parts[1].strip())
-            except Exception:
-                pass
-            
-            metrics['gpu_available'] = True
-            metrics['gpu_device'] = device
-            metrics['gpu_memory_allocated'] = memory_allocated
-            metrics['gpu_memory_reserved'] = memory_reserved
-            metrics['gpu_memory_total'] = memory_total
-            metrics['gpu_memory_percent'] = (memory_reserved / memory_total) * 100 if memory_total > 0 else 0
-            metrics['gpu_utilization'] = gpu_util if gpu_util is not None else 0
-            metrics['gpu_temperature'] = gpu_temp if gpu_temp is not None else 0
-        else:
-            metrics['gpu_available'] = False
-            metrics['gpu_device'] = None
-            metrics['gpu_memory_allocated'] = 0
-            metrics['gpu_memory_reserved'] = 0
-            metrics['gpu_memory_total'] = 0
-            metrics['gpu_memory_percent'] = 0
-            metrics['gpu_utilization'] = 0
-            metrics['gpu_temperature'] = 0
-    except Exception as e:
-        metrics['gpu_available'] = False
-        metrics['gpu_device'] = None
-        metrics['gpu_memory_allocated'] = 0
-        metrics['gpu_memory_reserved'] = 0
-        metrics['gpu_memory_total'] = 0
-        metrics['gpu_memory_percent'] = 0
-        metrics['gpu_utilization'] = 0
-        metrics['gpu_temperature'] = 0
-    
-    return metrics
-
-
-def compute_grad_norm(parameters, max_norm=float("inf")):
-    """
-    Compute the gradient norm for a set of parameters.
-    
-    This is a generic utility function that can be used by any agent to compute
-    gradient norms for logging purposes. The gradient norm is computed before
-    any clipping is applied.
-    
-    Args:
-        parameters: Iterable of parameters (e.g., model.parameters()) or a single model
-        max_norm: Maximum norm for clipping (default: inf, meaning no clipping, just compute norm)
-    
-    Returns:
-        Gradient norm as a torch.Tensor (scalar)
-    """
-    if isinstance(parameters, torch.nn.Module):
-        parameters = parameters.parameters()
-    
-    return torch.nn.utils.clip_grad_norm_(list(parameters), max_norm=max_norm)
-
diff --git a/src/rl_hockey/sac/crossq.py b/src/rl_hockey/sac/crossq.py
index 16bb36a..5ad5aa0 100644
--- a/src/rl_hockey/sac/crossq.py
+++ b/src/rl_hockey/sac/crossq.py
@@ -1,5 +1,4 @@
 import os
-
 import torch
 import torch.nn.functional as F
 import torch.optim as optim
@@ -7,6 +6,7 @@ import torch.optim as optim
 from rl_hockey.common import *
 from rl_hockey.sac.models import Actor, Critic
 
+
 DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
 
@@ -19,35 +19,30 @@ class SAC(Agent):
         self.action_dim = action_dim
 
         self.config = {
-            "batch_size": 256,
-            "learning_rate": 3e-4,
-            "discount": 0.99,
-            "alpha": 0.2,
-            "learn_alpha": True,
-            "noise": "normal",
-            "max_episode_steps": 1000,
+            'batch_size': 256,
+            'learning_rate': 3e-4,
+            'discount': 0.99,
+            'alpha': 0.2,
+            'learn_alpha': True,
+            'noise': 'normal',
+            'max_episode_steps': 1000,
         }
         self.config.update(user_config)
 
-        self.batch_size = self.config["batch_size"]
-        self.learning_rate = self.config["learning_rate"]
-        self.discount = self.config["discount"]
-        self.tau = self.config["tau"]
+        self.batch_size = self.config['batch_size']
+        self.learning_rate = self.config['learning_rate']
+        self.discount = self.config['discount']
+        self.tau = self.config['tau']
 
         self.actor = Actor(state_dim, action_dim).to(DEVICE)
 
         self.critic1 = Critic(state_dim, action_dim, [2048, 2048]).to(DEVICE)
         self.critic2 = Critic(state_dim, action_dim, [2048, 2048]).to(DEVICE)
 
-        self.critic_optimizer = optim.Adam(
-            list(self.critic1.parameters()) + list(self.critic2.parameters()),
-            lr=self.learning_rate,
-        )
-        self.actor_optimizer = optim.Adam(
-            self.actor.parameters(), lr=self.learning_rate
-        )
+        self.critic_optimizer = optim.Adam(list(self.critic1.parameters()) + list(self.critic2.parameters()), lr=self.learning_rate)
+        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)
 
-        if self.config["learn_alpha"]:
+        if self.config['learn_alpha']:
             self.target_entropy = -action_dim
 
             self.log_alpha = torch.zeros(1, requires_grad=True, device=DEVICE)
@@ -55,17 +50,15 @@ class SAC(Agent):
 
             self.alpha = self.log_alpha.exp()
         else:
-            self.alpha = self.config["alpha"]
+            self.alpha = self.config['alpha']
 
-        match self.config["noise"]:
-            case "normal":
+        match self.config['noise']:
+            case 'normal':
                 self.noise_dist = noise.NormalNoise(self.action_dim)
-            case "pink":
-                self.noise_dist = noise.PinkNoise(
-                    self.action_dim, self.config["max_episode_steps"]
-                )
+            case 'pink':
+                self.noise_dist = noise.PinkNoise(self.action_dim, self.config['max_episode_steps'])
             case _:
-                raise ValueError(f"Unknown noise type: {self.config['noise']}")
+                raise ValueError(f"Unknown noise type: {self.config['noise']}") 
 
     def act(self, state, deterministic=False):
         with torch.no_grad():
@@ -88,20 +81,16 @@ class SAC(Agent):
 
             q1 = self.critic1(state, action)
             q2 = self.critic2(state, action)
-            value = torch.min(q1, q2) - self.alpha * log_prob
+            value = torch.min(q1, q2) - self.alpha*log_prob
 
             return value.squeeze().item()
 
     def train(self, steps=1):
         critic_losses = []
         actor_losses = []
-        grad_norm_critic = []
-        grad_norm_actor = []
 
         for i in range(steps):
-            state, action, reward, next_state, done = self.buffer.sample(
-                self.batch_size
-            )
+            state, action, reward, next_state, done = self.buffer.sample(self.batch_size)
 
             state = torch.from_numpy(state).to(DEVICE)
             action = torch.from_numpy(action).to(DEVICE)
@@ -113,21 +102,15 @@ class SAC(Agent):
             # TODO use BN train here
             next_action, next_log_prob = self.actor.sample(next_state)
 
-            q1 = self.critic1(
-                torch.cat([state, next_state], dim=0),
-                torch.cat([action, next_action], dim=0),
-            )
+            q1 = self.critic1(torch.cat([state, next_state], dim=0), torch.cat([action, next_action], dim=0))
             q1, next_q1 = torch.chunk(q1, 2, dim=0)
 
-            q2 = self.critic2(
-                torch.cat([state, next_state], dim=0),
-                torch.cat([action, next_action], dim=0),
-            )
+            q2 = self.critic2(torch.cat([state, next_state], dim=0), torch.cat([action, next_action], dim=0))
             q2, next_q2 = torch.chunk(q2, 2, dim=0)
 
-            next_value = torch.min(next_q1, next_q2) - self.alpha * next_log_prob
+            next_value = torch.min(next_q1, next_q2) - self.alpha*next_log_prob
 
-            target = reward + (1 - done) * self.discount * next_value
+            target = reward + (1 - done)*self.discount*next_value
             target = target.detach()
 
             # update critic parameters
@@ -137,39 +120,27 @@ class SAC(Agent):
 
             self.critic_optimizer.zero_grad()
             critic_loss.backward(retain_graph=True)
-
-            # Compute gradient norm before step
-            grad_norm_critic_val = compute_grad_norm(
-                list(self.critic1.parameters()) + list(self.critic2.parameters())
-            )
-
             self.critic_optimizer.step()
 
             critic_losses.append(critic_loss.item())
-            grad_norm_critic.append(grad_norm_critic_val.item())
 
             # update actor parameters
             # TODO use BN eval here
             current_action, log_prob = self.actor.sample(state)
-
+            
             q1 = self.critic1(state, current_action)
             q2 = self.critic2(state, current_action)
-            actor_loss = self.alpha * log_prob - torch.min(q1, q2)
+            actor_loss = self.alpha*log_prob - torch.min(q1, q2)
             actor_loss = actor_loss.mean()
 
             self.actor_optimizer.zero_grad()
             actor_loss.backward()
-
-            # Compute gradient norm before step
-            grad_norm_actor_val = compute_grad_norm(self.actor.parameters())
-
             self.actor_optimizer.step()
 
             actor_losses.append(actor_loss.item())
-            grad_norm_actor.append(grad_norm_actor_val.item())
 
             # update temperature
-            if self.config["learn_alpha"]:
+            if self.config['learn_alpha']:
                 alpha_loss = -self.log_alpha * (log_prob.detach() + self.target_entropy)
                 alpha_loss = alpha_loss.mean()
 
@@ -180,42 +151,31 @@ class SAC(Agent):
                 self.alpha = self.log_alpha.exp()
 
             # update critic target parameters
-            for p, pt in zip(
-                self.critic1.parameters(), self.critic1_target.parameters()
-            ):
-                pt.data.copy_(self.tau * p.data + (1 - self.tau) * pt.data)
-
-            for p, pt in zip(
-                self.critic2.parameters(), self.critic2_target.parameters()
-            ):
-                pt.data.copy_(self.tau * p.data + (1 - self.tau) * pt.data)
-
-        return {
-            "critic_loss": critic_losses,
-            "actor_loss": actor_losses,
-            "grad_norm_critic": grad_norm_critic,
-            "grad_norm_actor": grad_norm_actor,
-        }
+            for p, pt in zip(self.critic1.parameters(), self.critic1_target.parameters()):
+                pt.data.copy_(self.tau*p.data + (1 - self.tau)*pt.data)
+
+            for p, pt in zip(self.critic2.parameters(), self.critic2_target.parameters()):
+                pt.data.copy_(self.tau*p.data + (1 - self.tau)*pt.data)
+
+        return {'critic_loss': critic_losses, 'actor_loss': actor_losses}
 
     def save(self, filepath):
         if not os.path.exists(os.path.dirname(filepath)):
             os.makedirs(os.path.dirname(filepath))
 
-        if not filepath.endswith(".pt"):
-            filepath += ".pt"
+        if not filepath.endswith('.pt'):
+            filepath += '.pt'
 
         checkpoint = {
-            "actor": self.actor.state_dict(),
-            "critic1": self.critic1.state_dict(),
-            "critic1_target": self.critic1_target.state_dict(),
-            "critic2": self.critic2.state_dict(),
-            "critic2_target": self.critic2_target.state_dict(),
-            "critic_optimizer": self.critic_optimizer.state_dict(),
-            "actor_optimizer": self.actor_optimizer.state_dict(),
-            "log_alpha": self.log_alpha if self.config["learn_alpha"] else None,
-            "alpha_optimizer": self.alpha_optimizer.state_dict()
-            if self.config["learn_alpha"]
-            else None,
+            'actor': self.actor.state_dict(),
+            'critic1': self.critic1.state_dict(),
+            'critic1_target': self.critic1_target.state_dict(),
+            'critic2': self.critic2.state_dict(),
+            'critic2_target': self.critic2_target.state_dict(),
+            'critic_optimizer': self.critic_optimizer.state_dict(),
+            'actor_optimizer': self.actor_optimizer.state_dict(),
+            'log_alpha': self.log_alpha if self.config['learn_alpha'] else None,
+            'alpha_optimizer': self.alpha_optimizer.state_dict() if self.config['learn_alpha'] else None,
         }
 
         torch.save(checkpoint, filepath)
@@ -223,19 +183,19 @@ class SAC(Agent):
     def load(self, filepath):
         checkpoint = torch.load(filepath)
 
-        self.actor.load_state_dict(checkpoint["actor"])
-        self.critic1.load_state_dict(checkpoint["critic1"])
-        self.critic1_target.load_state_dict(checkpoint["critic1_target"])
-        self.critic2.load_state_dict(checkpoint["critic2"])
-        self.critic2_target.load_state_dict(checkpoint["critic2_target"])
-        self.critic_optimizer.load_state_dict(checkpoint["critic_optimizer"])
-        self.actor_optimizer.load_state_dict(checkpoint["actor_optimizer"])
-
-        if self.config["learn_alpha"]:
-            self.log_alpha = checkpoint["log_alpha"]
-            self.alpha_optimizer.load_state_dict(checkpoint["alpha_optimizer"])
+        self.actor.load_state_dict(checkpoint['actor'])
+        self.critic1.load_state_dict(checkpoint['critic1'])
+        self.critic1_target.load_state_dict(checkpoint['critic1_target'])
+        self.critic2.load_state_dict(checkpoint['critic2'])
+        self.critic2_target.load_state_dict(checkpoint['critic2_target'])
+        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
+        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
+
+        if self.config['learn_alpha']:
+            self.log_alpha = checkpoint['log_alpha']
+            self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer'])
             self.alpha = self.log_alpha.exp()
 
     def on_episode_start(self, episode):
-        if self.config["noise"] == "pink":
+        if self.config['noise'] == 'pink':
             self.noise_dist.reset()
diff --git a/src/rl_hockey/sac/sac.py b/src/rl_hockey/sac/sac.py
index f89ed93..2ae779b 100644
--- a/src/rl_hockey/sac/sac.py
+++ b/src/rl_hockey/sac/sac.py
@@ -86,8 +86,8 @@ class SAC(Agent):
             action, _ = self.actor.sample(state, noise=noise, calc_log_prob=False)
 
             return action.squeeze(0).cpu().numpy()
-
-    def act_batch(self, states, deterministic=False, **kwargs):
+    
+    def act_batch(self, states, deterministic=False):
         """Process a batch of states at once (for vectorized environments)"""
         with torch.no_grad():
             states = torch.from_numpy(states).to(DEVICE)
@@ -97,9 +97,7 @@ class SAC(Agent):
                 noise = torch.zeros((batch_size, self.action_dim), device=DEVICE)
             else:
                 # Sample noise for each state in batch
-                noise = torch.stack(
-                    [self.noise_dist.sample().to(DEVICE) for _ in range(batch_size)]
-                )
+                noise = torch.stack([self.noise_dist.sample().to(DEVICE) for _ in range(batch_size)])
 
             actions, _ = self.actor.sample(states, noise=noise, calc_log_prob=False)
 
@@ -120,8 +118,6 @@ class SAC(Agent):
     def train(self, steps=1):
         critic_losses = []
         actor_losses = []
-        grad_norm_critic = []
-        grad_norm_actor = []
 
         for i in range(steps):
             state, action, reward, next_state, done = self.buffer.sample(
@@ -151,16 +147,9 @@ class SAC(Agent):
 
             self.critic_optimizer.zero_grad()
             critic_loss.backward(retain_graph=True)
-
-            # Compute gradient norm before step
-            grad_norm_critic_val = compute_grad_norm(
-                list(self.critic1.parameters()) + list(self.critic2.parameters())
-            )
-
             self.critic_optimizer.step()
 
             critic_losses.append(critic_loss.item())
-            grad_norm_critic.append(grad_norm_critic_val.item())
 
             # update actor parameters
             current_action, log_prob = self.actor.sample(state)
@@ -172,14 +161,9 @@ class SAC(Agent):
 
             self.actor_optimizer.zero_grad()
             actor_loss.backward()
-
-            # Compute gradient norm before step
-            grad_norm_actor_val = compute_grad_norm(self.actor.parameters())
-
             self.actor_optimizer.step()
 
             actor_losses.append(actor_loss.item())
-            grad_norm_actor.append(grad_norm_actor_val.item())
 
             # update temperature
             if self.config["learn_alpha"]:
@@ -203,12 +187,7 @@ class SAC(Agent):
             ):
                 pt.data.copy_(self.tau * p.data + (1 - self.tau) * pt.data)
 
-        return {
-            "critic_loss": critic_losses,
-            "actor_loss": actor_losses,
-            "grad_norm_critic": grad_norm_critic,
-            "grad_norm_actor": grad_norm_actor,
-        }
+        return {"critic_loss": critic_losses, "actor_loss": actor_losses}
 
     def save(self, filepath):
         if not os.path.exists(os.path.dirname(filepath)):
@@ -238,7 +217,7 @@ class SAC(Agent):
 
     def load(self, filepath):
         checkpoint = torch.load(filepath, map_location=DEVICE)
-
+        
         # Load state_dim, action_dim, and config if available (for compatibility)
         if "state_dim" in checkpoint:
             self.state_dim = checkpoint["state_dim"]
@@ -264,9 +243,7 @@ class SAC(Agent):
             else:
                 # Old checkpoint without log_alpha, initialize it
                 self.log_alpha = torch.zeros(1, requires_grad=True, device=DEVICE)
-                self.alpha_optimizer = optim.Adam(
-                    [self.log_alpha], lr=self.learning_rate
-                )
+                self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.learning_rate)
                 self.alpha = self.log_alpha.exp()
 
     def on_episode_start(self, episode):
diff --git a/src/rl_hockey/scripts/eval_sac.ipynb b/src/rl_hockey/scripts/eval_sac.ipynb
deleted file mode 100644
index 805af3e..0000000
--- a/src/rl_hockey/scripts/eval_sac.ipynb
+++ /dev/null
@@ -1,168 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 18,
-   "id": "204053dc",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "The autoreload extension is already loaded. To reload it, use:\n",
-      "  %reload_ext autoreload\n"
-     ]
-    }
-   ],
-   "source": [
-    "%load_ext autoreload\n",
-    "%autoreload 2"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 19,
-   "id": "1bf4643a",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import numpy as np\n",
-    "from tqdm import tqdm\n",
-    "import matplotlib.pyplot as plt\n",
-    "import hockey.hockey_env as h_env\n",
-    "\n",
-    "from rl_hockey.sac import SAC\n",
-    "from rl_hockey.common import utils"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 26,
-   "id": "b5add761",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)\n",
-    "\n",
-    "o_space = env.observation_space\n",
-    "ac_space = env.action_space"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 21,
-   "id": "2164aacb",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "agent = SAC(o_space.shape[0], action_dim=ac_space.shape[0] // 2, noise='pink', max_episode_steps=500)\n",
-    "agent.load(\"../../../results/hyperparameter_runs/2026-01-17_20-35-18/models/run_lr1e03_bs256_h128_128_128_4c1f51eb_20260117_203518_vec16.pt\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 22,
-   "id": "5b706e83",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "opponent = h_env.BasicOpponent(weak=False)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 23,
-   "id": "061b1e93",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "N = 100\n",
-    "win_count = 0\n",
-    "total_reward = 0"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 27,
-   "id": "9fdef477",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      " 46%|████▌     | 46/100 [00:48<00:57,  1.06s/it]\n"
-     ]
-    },
-    {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     action1 \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
-      "File \u001b[1;32mc:\\Users\\Jannik\\anaconda3\\envs\\rl-hockey\\lib\\site-packages\\hockey\\hockey_env.py:739\u001b[0m, in \u001b[0;36mHockeyEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    737\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    738\u001b[0m   pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 739\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m   pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
-      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
-     ]
-    }
-   ],
-   "source": [
-    "for i in tqdm(range(N)):\n",
-    "    state, _ = env.reset()\n",
-    "    for t in range(500):\n",
-    "        env.render(mode=\"human\")\n",
-    "\n",
-    "        done = False\n",
-    "        action1 = agent.act(state.astype(np.float32), deterministic=True)\n",
-    "        action2 = opponent.act(env.obs_agent_two())\n",
-    "\n",
-    "        (next_state, reward, done, trunc, info) = env.step(np.hstack([action1, action2]))\n",
-    "        state = next_state\n",
-    "\n",
-    "        total_reward += reward\n",
-    "\n",
-    "        if done or trunc:\n",
-    "            break\n",
-    "    \n",
-    "    if info['winner'] == 1:\n",
-    "        win_count += 1\n",
-    "\n",
-    "print(f\"Average Reward over {N} episodes: {total_reward / N}\")\n",
-    "print(f\"Win Rate over {N} episodes: {win_count / N}\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 28,
-   "id": "00cd8887",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "env.close()"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "rl-hockey",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.10.19"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/src/rl_hockey/scripts/replay_buffer_test.ipynb b/src/rl_hockey/scripts/replay_buffer_test.ipynb
deleted file mode 100644
index ce35f88..0000000
--- a/src/rl_hockey/scripts/replay_buffer_test.ipynb
+++ /dev/null
@@ -1,138 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "id": "f0b8dec9",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import numpy as np\n",
-    "from tqdm import tqdm\n",
-    "import matplotlib.pyplot as plt\n",
-    "import hockey.hockey_env as h_env\n",
-    "\n",
-    "from rl_hockey.td3 import TD3\n",
-    "from rl_hockey.common import utils\n",
-    "from rl_hockey.common.buffer import ReplayBuffer\n",
-    "from rl_hockey.common.prioritizedbuffer import PERMemory"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "id": "4b2529f0",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "env = h_env.HockeyEnv(mode=h_env.Mode.TRAIN_SHOOTING)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "id": "080dda47",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "state, _ = env.reset()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "id": "324a100c",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "normal_buffer = ReplayBuffer(max_size=10000)\n",
-    "prioritized_buffer = PERMemory(capacity=10000)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "id": "29f016a8",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "for _ in range(1000):\n",
-    "    action = env.action_space.sample()\n",
-    "    next_state, reward, done, truncated, info = env.step(action)\n",
-    "    transition = (state, action, reward, next_state, done)\n",
-    "    state = next_state\n",
-    "    normal_buffer.store(transition)\n",
-    "    prioritized_buffer.store(transition)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "id": "8d86f9cc",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "(32, 18) (32, 8) (32, 1) (32, 18) (32, 1)\n"
-     ]
-    }
-   ],
-   "source": [
-    "normal_batch = normal_buffer.sample(batch_size=32)\n",
-    "normal_states, normal_actions, normal_rewards, normal_next_states, normal_dones = normal_batch\n",
-    "print(normal_states.shape, normal_actions.shape, normal_rewards.shape, normal_next_states.shape, normal_dones.shape)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 13,
-   "id": "e7c138f0",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "(32, 18) (32, 8) (32, 1) (32, 18) (32, 1)\n"
-     ]
-    }
-   ],
-   "source": [
-    "prioritized_batch = prioritized_buffer.sample(batch_size=32)\n",
-    "(prioritized_states, prioritized_actions, prioritized_rewards, prioritized_next_states, prioritized_dones), indices, weights = prioritized_batch\n",
-    "print(prioritized_states.shape, prioritized_actions.shape, prioritized_rewards.shape, prioritized_next_states.shape, prioritized_dones.shape)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "a5fa5fee",
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "rlhockey",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.11.14"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/src/rl_hockey/scripts/reward_sprase.py b/src/rl_hockey/scripts/reward_sprase.py
deleted file mode 100644
index 1349ca5..0000000
--- a/src/rl_hockey/scripts/reward_sprase.py
+++ /dev/null
@@ -1,1065 +0,0 @@
-import os
-import shutil
-
-import hockey.hockey_env as h_env
-import matplotlib.pyplot as plt
-import numpy as np
-
-from rl_hockey.common.reward_backprop import apply_win_reward_backprop
-
-
-def play_random_games(num_games=100, max_steps=250):
-    """
-    Play multiple games with random actions for both players.
-
-    Args:
-        num_games: Number of games to play
-        max_steps: Maximum steps per episode
-
-    Returns:
-        Dictionary containing all episode data
-    """
-    env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)
-
-    # Storage for all episodes
-    all_episodes = {
-        "player1_wins": [],  # Episodes where player 1 won
-        "player2_wins": [],  # Episodes where player 2 won
-        "draws": [],  # Episodes that ended in a draw
-        "all": [],  # All episodes
-    }
-
-    print(f"Playing {num_games} games with random actions for both players...")
-    print("=" * 60)
-
-    for game_num in range(num_games):
-        obs, info = env.reset()
-        episode_rewards = []
-        episode_steps = 0
-
-        for step in range(max_steps):
-            # Random actions for both players
-            a1 = env.action_space.sample()
-            a2 = env.action_space.sample()
-            action = np.hstack([a1, a2])
-
-            # Step environment
-            step_result = env.step(action)
-            if len(step_result) == 5:
-                obs, reward, done, truncated, info = step_result
-            else:
-                obs, reward, done, info = step_result
-                truncated = False
-
-            # Store reward for player 1
-            episode_rewards.append(float(reward))
-            episode_steps += 1
-
-            if done or truncated:
-                break
-
-        # Get winner information
-        winner = info.get("winner", 0)
-
-        # Store episode data
-        episode_data = {
-            "game_num": game_num,
-            "rewards": episode_rewards,
-            "total_reward": sum(episode_rewards),
-            "mean_reward": np.mean(episode_rewards),
-            "steps": episode_steps,
-            "winner": winner,
-        }
-
-        all_episodes["all"].append(episode_data)
-
-        if winner == 1:
-            all_episodes["player1_wins"].append(episode_data)
-        elif winner == -1:
-            all_episodes["player2_wins"].append(episode_data)
-        else:
-            all_episodes["draws"].append(episode_data)
-
-        if (game_num + 1) % 10 == 0:
-            print(f"Completed {game_num + 1}/{num_games} games...")
-
-    env.close()
-    print("=" * 60)
-    print("All games completed!")
-    print()
-
-    return all_episodes
-
-
-def analyze_rewards(episodes):
-    """
-    Analyze reward distributions for different episode outcomes.
-
-    Args:
-        episodes: Dictionary with episode data separated by outcome
-    """
-    print("REWARD DISTRIBUTION ANALYSIS")
-    print("=" * 60)
-    print()
-
-    # Collect all rewards
-    all_rewards = []
-    for ep in episodes["all"]:
-        all_rewards.extend(ep["rewards"])
-
-    player1_win_rewards = []
-    for ep in episodes["player1_wins"]:
-        player1_win_rewards.extend(ep["rewards"])
-
-    player2_win_rewards = []
-    for ep in episodes["player2_wins"]:
-        player2_win_rewards.extend(ep["rewards"])
-
-    draw_rewards = []
-    for ep in episodes["draws"]:
-        draw_rewards.extend(ep["rewards"])
-
-    # Print statistics
-    print("OVERALL STATISTICS (All Episodes)")
-    print("-" * 60)
-    print(f"Total games: {len(episodes['all'])}")
-    print(
-        f"Player 1 wins: {len(episodes['player1_wins'])} ({100 * len(episodes['player1_wins']) / len(episodes['all']):.1f}%)"
-    )
-    print(
-        f"Player 2 wins: {len(episodes['player2_wins'])} ({100 * len(episodes['player2_wins']) / len(episodes['all']):.1f}%)"
-    )
-    print(
-        f"Draws: {len(episodes['draws'])} ({100 * len(episodes['draws']) / len(episodes['all']):.1f}%)"
-    )
-    print()
-
-    print("REWARD STATISTICS - ALL EPISODES")
-    print("-" * 60)
-    if all_rewards:
-        print(f"Total steps: {len(all_rewards)}")
-        print(f"Min reward: {min(all_rewards):.6f}")
-        print(f"Max reward: {max(all_rewards):.6f}")
-        print(f"Mean reward: {np.mean(all_rewards):.6f}")
-        print(f"Median reward: {np.median(all_rewards):.6f}")
-        print(f"Std reward: {np.std(all_rewards):.6f}")
-        positive_count = sum(1 for r in all_rewards if r > 1e-12)
-        negative_count = sum(1 for r in all_rewards if r < -1e-12)
-        zero_count = sum(1 for r in all_rewards if abs(r) <= 1e-12)
-        print(
-            f"Positive rewards: {positive_count} ({100 * positive_count / len(all_rewards):.2f}%)"
-        )
-        print(
-            f"Negative rewards: {negative_count} ({100 * negative_count / len(all_rewards):.2f}%)"
-        )
-        print(
-            f"Zero rewards: {zero_count} ({100 * zero_count / len(all_rewards):.2f}%)"
-        )
-    print()
-
-    print("REWARD STATISTICS - PLAYER 1 WINS")
-    print("-" * 60)
-    if player1_win_rewards:
-        print(f"Total steps: {len(player1_win_rewards)}")
-        print(f"Min reward: {min(player1_win_rewards):.6f}")
-        print(f"Max reward: {max(player1_win_rewards):.6f}")
-        print(f"Mean reward: {np.mean(player1_win_rewards):.6f}")
-        print(f"Median reward: {np.median(player1_win_rewards):.6f}")
-        print(f"Std reward: {np.std(player1_win_rewards):.6f}")
-        positive_count = sum(1 for r in player1_win_rewards if r > 1e-12)
-        negative_count = sum(1 for r in player1_win_rewards if r < -1e-12)
-        zero_count = sum(1 for r in player1_win_rewards if abs(r) <= 1e-12)
-        print(
-            f"Positive rewards: {positive_count} ({100 * positive_count / len(player1_win_rewards):.2f}%)"
-        )
-        print(
-            f"Negative rewards: {negative_count} ({100 * negative_count / len(player1_win_rewards):.2f}%)"
-        )
-        print(
-            f"Zero rewards: {zero_count} ({100 * zero_count / len(player1_win_rewards):.2f}%)"
-        )
-
-        # Episode-level statistics
-        total_rewards = [ep["total_reward"] for ep in episodes["player1_wins"]]
-        print("\nEpisode total rewards:")
-        print(f"  Min: {min(total_rewards):.6f}")
-        print(f"  Max: {max(total_rewards):.6f}")
-        print(f"  Mean: {np.mean(total_rewards):.6f}")
-        print(f"  Median: {np.median(total_rewards):.6f}")
-    else:
-        print("No episodes where Player 1 won.")
-    print()
-
-    print("REWARD STATISTICS - PLAYER 2 WINS (Player 1's perspective)")
-    print("-" * 60)
-    if player2_win_rewards:
-        print(f"Total steps: {len(player2_win_rewards)}")
-        print(f"Min reward: {min(player2_win_rewards):.6f}")
-        print(f"Max reward: {max(player2_win_rewards):.6f}")
-        print(f"Mean reward: {np.mean(player2_win_rewards):.6f}")
-        print(f"Median reward: {np.median(player2_win_rewards):.6f}")
-        print(f"Std reward: {np.std(player2_win_rewards):.6f}")
-        positive_count = sum(1 for r in player2_win_rewards if r > 1e-12)
-        negative_count = sum(1 for r in player2_win_rewards if r < -1e-12)
-        zero_count = sum(1 for r in player2_win_rewards if abs(r) <= 1e-12)
-        print(
-            f"Positive rewards: {positive_count} ({100 * positive_count / len(player2_win_rewards):.2f}%)"
-        )
-        print(
-            f"Negative rewards: {negative_count} ({100 * negative_count / len(player2_win_rewards):.2f}%)"
-        )
-        print(
-            f"Zero rewards: {zero_count} ({100 * zero_count / len(player2_win_rewards):.2f}%)"
-        )
-
-        # Episode-level statistics
-        total_rewards = [ep["total_reward"] for ep in episodes["player2_wins"]]
-        print("\nEpisode total rewards:")
-        print(f"  Min: {min(total_rewards):.6f}")
-        print(f"  Max: {max(total_rewards):.6f}")
-        print(f"  Mean: {np.mean(total_rewards):.6f}")
-        print(f"  Median: {np.median(total_rewards):.6f}")
-    else:
-        print("No episodes where Player 2 won.")
-    print()
-
-    print("REWARD STATISTICS - DRAWS")
-    print("-" * 60)
-    if draw_rewards:
-        print(f"Total steps: {len(draw_rewards)}")
-        print(f"Min reward: {min(draw_rewards):.6f}")
-        print(f"Max reward: {max(draw_rewards):.6f}")
-        print(f"Mean reward: {np.mean(draw_rewards):.6f}")
-        print(f"Median reward: {np.median(draw_rewards):.6f}")
-        print(f"Std reward: {np.std(draw_rewards):.6f}")
-        positive_count = sum(1 for r in draw_rewards if r > 1e-12)
-        negative_count = sum(1 for r in draw_rewards if r < -1e-12)
-        zero_count = sum(1 for r in draw_rewards if abs(r) <= 1e-12)
-        print(
-            f"Positive rewards: {positive_count} ({100 * positive_count / len(draw_rewards):.2f}%)"
-        )
-        print(
-            f"Negative rewards: {negative_count} ({100 * negative_count / len(draw_rewards):.2f}%)"
-        )
-        print(
-            f"Zero rewards: {zero_count} ({100 * zero_count / len(draw_rewards):.2f}%)"
-        )
-
-        # Episode-level statistics
-        total_rewards = [ep["total_reward"] for ep in episodes["draws"]]
-        print("\nEpisode total rewards:")
-        print(f"  Min: {min(total_rewards):.6f}")
-        print(f"  Max: {max(total_rewards):.6f}")
-        print(f"  Mean: {np.mean(total_rewards):.6f}")
-        print(f"  Median: {np.median(total_rewards):.6f}")
-    else:
-        print("No draw episodes.")
-    print()
-
-    return {
-        "all": all_rewards,
-        "player1_wins": player1_win_rewards,
-        "player2_wins": player2_win_rewards,
-        "draws": draw_rewards,
-    }
-
-
-def plot_reward_distributions(reward_data, episodes):
-    """
-    Create comprehensive plots of reward distributions.
-
-    Args:
-        reward_data: Dictionary with reward lists by outcome
-        episodes: Dictionary with episode data
-    """
-    import os
-
-    plt.figure(figsize=(16, 16))
-
-    # Plot 1: Overall reward distribution
-    ax1 = plt.subplot(4, 2, 1)
-    if reward_data["all"]:
-        ax1.hist(
-            reward_data["all"], bins=100, alpha=0.7, color="gray", edgecolor="black"
-        )
-        ax1.axvline(x=0, color="red", linestyle="--", linewidth=2, label="Zero")
-        ax1.set_xlabel("Reward")
-        ax1.set_ylabel("Frequency")
-        ax1.set_title("Overall Reward Distribution (All Episodes)")
-        ax1.legend()
-        ax1.grid(True, alpha=0.3)
-
-    # Plot 2: Player 1 wins - reward distribution
-    ax2 = plt.subplot(4, 2, 2)
-    if reward_data["player1_wins"]:
-        ax2.hist(
-            reward_data["player1_wins"],
-            bins=100,
-            alpha=0.7,
-            color="green",
-            edgecolor="black",
-        )
-        ax2.axvline(x=0, color="red", linestyle="--", linewidth=2, label="Zero")
-        ax2.set_xlabel("Reward")
-        ax2.set_ylabel("Frequency")
-        ax2.set_title(
-            f"Reward Distribution - Player 1 Wins ({len(episodes['player1_wins'])} episodes)"
-        )
-        ax2.legend()
-        ax2.grid(True, alpha=0.3)
-
-    # Plot 3: Player 2 wins - reward distribution (Player 1's perspective)
-    ax3 = plt.subplot(4, 2, 3)
-    if reward_data["player2_wins"]:
-        ax3.hist(
-            reward_data["player2_wins"],
-            bins=100,
-            alpha=0.7,
-            color="red",
-            edgecolor="black",
-        )
-        ax3.axvline(x=0, color="blue", linestyle="--", linewidth=2, label="Zero")
-        ax3.set_xlabel("Reward")
-        ax3.set_ylabel("Frequency")
-        ax3.set_title(
-            f"Reward Distribution - Player 2 Wins ({len(episodes['player2_wins'])} episodes)"
-        )
-        ax3.legend()
-        ax3.grid(True, alpha=0.3)
-
-    # Plot 4: Draws - reward distribution
-    ax4 = plt.subplot(4, 2, 4)
-    if reward_data["draws"]:
-        ax4.hist(
-            reward_data["draws"], bins=100, alpha=0.7, color="orange", edgecolor="black"
-        )
-        ax4.axvline(x=0, color="red", linestyle="--", linewidth=2, label="Zero")
-        ax4.set_xlabel("Reward")
-        ax4.set_ylabel("Frequency")
-        ax4.set_title(
-            f"Reward Distribution - Draws ({len(episodes['draws'])} episodes)"
-        )
-        ax4.legend()
-        ax4.grid(True, alpha=0.3)
-
-    # Plot 5: Comparison of reward distributions
-    ax5 = plt.subplot(4, 2, 5)
-    if reward_data["player1_wins"]:
-        ax5.hist(
-            reward_data["player1_wins"],
-            bins=50,
-            alpha=0.5,
-            color="green",
-            label=f"Player 1 Wins (n={len(reward_data['player1_wins'])})",
-            density=True,
-        )
-    if reward_data["player2_wins"]:
-        ax5.hist(
-            reward_data["player2_wins"],
-            bins=50,
-            alpha=0.5,
-            color="red",
-            label=f"Player 2 Wins (n={len(reward_data['player2_wins'])})",
-            density=True,
-        )
-    if reward_data["draws"]:
-        ax5.hist(
-            reward_data["draws"],
-            bins=50,
-            alpha=0.5,
-            color="orange",
-            label=f"Draws (n={len(reward_data['draws'])})",
-            density=True,
-        )
-    ax5.axvline(x=0, color="black", linestyle="--", linewidth=1)
-    ax5.set_xlabel("Reward")
-    ax5.set_ylabel("Density")
-    ax5.set_title("Normalized Reward Distribution Comparison")
-    ax5.legend()
-    ax5.grid(True, alpha=0.3)
-
-    # Plot 6: Episode total rewards by outcome
-    ax6 = plt.subplot(4, 2, 6)
-    if episodes["player1_wins"]:
-        p1_totals = [ep["total_reward"] for ep in episodes["player1_wins"]]
-        ax6.hist(
-            p1_totals,
-            bins=30,
-            alpha=0.5,
-            color="green",
-            label=f"Player 1 Wins (n={len(episodes['player1_wins'])})",
-            density=True,
-        )
-    if episodes["player2_wins"]:
-        p2_totals = [ep["total_reward"] for ep in episodes["player2_wins"]]
-        ax6.hist(
-            p2_totals,
-            bins=30,
-            alpha=0.5,
-            color="red",
-            label=f"Player 2 Wins (n={len(episodes['player2_wins'])})",
-            density=True,
-        )
-    if episodes["draws"]:
-        draw_totals = [ep["total_reward"] for ep in episodes["draws"]]
-        ax6.hist(
-            draw_totals,
-            bins=30,
-            alpha=0.5,
-            color="orange",
-            label=f"Draws (n={len(episodes['draws'])})",
-            density=True,
-        )
-    ax6.axvline(x=0, color="black", linestyle="--", linewidth=1)
-    ax6.set_xlabel("Episode Total Reward")
-    ax6.set_ylabel("Density")
-    ax6.set_title("Episode Total Reward Distribution by Outcome")
-    ax6.legend()
-    ax6.grid(True, alpha=0.3)
-
-    # Plot 7: Player 1 wins - Episode total rewards distribution with mean
-    ax7 = plt.subplot(4, 2, 7)
-    if episodes["player1_wins"]:
-        p1_totals = [ep["total_reward"] for ep in episodes["player1_wins"]]
-        mean_total = np.mean(p1_totals)
-        ax7.hist(
-            p1_totals,
-            bins=30,
-            alpha=0.7,
-            color="green",
-            edgecolor="black",
-        )
-        ax7.axvline(
-            x=mean_total,
-            color="red",
-            linestyle="--",
-            linewidth=2,
-            label=f"Mean: {mean_total:.4f}",
-        )
-        ax7.axvline(x=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax7.set_xlabel("Episode Total Reward")
-        ax7.set_ylabel("Frequency")
-        ax7.set_title(
-            f"Player 1 Wins - Episode Total Rewards (n={len(episodes['player1_wins'])})"
-        )
-        ax7.legend()
-        ax7.grid(True, alpha=0.3)
-
-    # Plot 8: Player 1 wins - Step-by-step reward mean across episodes
-    ax8 = plt.subplot(4, 2, 8)
-    if episodes["player1_wins"]:
-        # Find maximum episode length
-        max_length = max(len(ep["rewards"]) for ep in episodes["player1_wins"])
-
-        # Pad all episodes to same length with NaN, then compute mean per step
-        padded_rewards = []
-        for ep in episodes["player1_wins"]:
-            padded = ep["rewards"] + [np.nan] * (max_length - len(ep["rewards"]))
-            padded_rewards.append(padded)
-
-        # Compute mean reward per step (ignoring NaN)
-        mean_per_step = []
-        for step_idx in range(max_length):
-            step_rewards = [
-                padded[step_idx]
-                for padded in padded_rewards
-                if not np.isnan(padded[step_idx])
-            ]
-            if step_rewards:
-                mean_per_step.append(np.mean(step_rewards))
-            else:
-                mean_per_step.append(np.nan)
-
-        steps = np.arange(len(mean_per_step))
-        ax8.plot(
-            steps,
-            mean_per_step,
-            color="green",
-            linewidth=2,
-            label="Mean reward per step",
-        )
-        ax8.axhline(y=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax8.set_xlabel("Step in Episode")
-        ax8.set_ylabel("Mean Reward")
-        ax8.set_title(
-            f"Player 1 Wins - Mean Reward per Step Across Episodes (n={len(episodes['player1_wins'])})"
-        )
-        ax8.legend()
-        ax8.grid(True, alpha=0.3)
-
-    plt.tight_layout()
-    figures_dir = "src/rl_hockey/scripts/figures"
-    os.makedirs(figures_dir, exist_ok=True)
-    output_file = os.path.join(figures_dir, "reward_distribution_analysis.png")
-    plt.savefig(output_file, dpi=150, bbox_inches="tight")
-    print(f"Figure saved to: {output_file}")
-    plt.close()
-
-
-def plot_individual_player1_win_episodes(
-    episodes,
-    output_dir=None,
-    win_reward_bonus=0.0,
-    win_reward_discount=0.98,
-):
-    """
-    Plot and save individual reward distributions for each episode where Player 1 wins.
-
-    Args:
-        episodes: Dictionary with episode data
-        output_dir: Directory to save individual episode plots (default: figures/player1_win_episodes)
-        win_reward_bonus: Bonus reward to add to each step in a winning episode
-        win_reward_discount: Discount factor for applying win reward bonus
-    """
-    import os
-
-    if output_dir is None:
-        output_dir = "src/rl_hockey/scripts/figures/player1_win_episodes"
-
-    # Create output directory if it doesn't exist
-    os.makedirs(output_dir, exist_ok=True)
-
-    player1_win_episodes = episodes["player1_wins"]
-
-    if not player1_win_episodes:
-        print("No episodes where Player 1 won. Skipping individual episode plots.")
-        return
-
-    print(
-        f"\nCreating individual plots for {len(player1_win_episodes)} Player 1 win episodes..."
-    )
-
-    for idx, episode in enumerate(player1_win_episodes):
-        fig, axes = plt.subplots(2, 1, figsize=(10, 8))
-
-        rewards = np.array(episode["rewards"], dtype=np.float32)
-        game_num = episode["game_num"]
-        total_reward = episode["total_reward"]
-        mean_reward = episode["mean_reward"]
-        steps = episode["steps"]
-        winner = episode["winner"]
-
-        # Apply backpropagation to get final rewards
-        final_rewards, original_rewards, bonus_rewards = apply_win_reward_backprop(
-            rewards,
-            winner=winner,
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-            use_torch=False,
-        )
-        final_total_reward = np.sum(final_rewards)
-        final_mean_reward = np.mean(final_rewards)
-
-        # Plot 1: Reward distribution histogram
-        ax1 = axes[0]
-        ax1.hist(
-            rewards,
-            bins=min(50, len(rewards)),
-            alpha=0.7,
-            color="green",
-            edgecolor="black",
-        )
-        ax1.axvline(x=0, color="red", linestyle="--", linewidth=2, label="Zero")
-        ax1.axvline(
-            x=mean_reward,
-            color="blue",
-            linestyle="--",
-            linewidth=2,
-            label=f"Mean: {mean_reward:.4f}",
-        )
-        ax1.set_xlabel("Reward")
-        ax1.set_ylabel("Frequency")
-        ax1.set_title(
-            f"Player 1 Win - Episode {game_num} - Reward Distribution\n"
-            f"Total Reward: {total_reward:.4f}, Mean: {mean_reward:.4f}, Steps: {steps}"
-        )
-        ax1.legend()
-        ax1.grid(True, alpha=0.3)
-
-        # Plot 2: Reward over time (step-by-step)
-        ax2 = axes[1]
-        steps_array = np.arange(len(rewards))
-        # Plot original rewards
-        ax2.plot(
-            steps_array,
-            original_rewards,
-            color="green",
-            linewidth=1.5,
-            alpha=0.7,
-            label="Original Rewards",
-        )
-        # Plot final rewards after backpropagation
-        ax2.plot(
-            steps_array,
-            final_rewards,
-            color="purple",
-            linewidth=2,
-            linestyle="-",
-            label=f"After Backprop (bonus={win_reward_bonus}, γ={win_reward_discount})",
-        )
-        ax2.axhline(y=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax2.axhline(
-            y=mean_reward,
-            color="blue",
-            linestyle="--",
-            linewidth=1.5,
-            alpha=0.7,
-            label=f"Original Mean: {mean_reward:.4f}",
-        )
-        ax2.axhline(
-            y=final_mean_reward,
-            color="orange",
-            linestyle="--",
-            linewidth=1.5,
-            alpha=0.7,
-            label=f"Final Mean: {final_mean_reward:.4f}",
-        )
-        ax2.fill_between(
-            steps_array,
-            original_rewards,
-            0,
-            alpha=0.2,
-            color="green",
-            where=(original_rewards >= 0),
-        )
-        ax2.fill_between(
-            steps_array,
-            original_rewards,
-            0,
-            alpha=0.2,
-            color="red",
-            where=(original_rewards < 0),
-        )
-        ax2.set_xlabel("Step in Episode")
-        ax2.set_ylabel("Reward")
-        ax2.set_title(
-            f"Reward Over Time - Episode {game_num}\n"
-            f"Original Total: {total_reward:.4f}, Final Total: {final_total_reward:.4f}, "
-            f"Bonus Added: {np.sum(bonus_rewards):.4f}"
-        )
-        ax2.legend()
-        ax2.grid(True, alpha=0.3)
-
-        plt.tight_layout()
-        output_file = os.path.join(
-            output_dir, f"player1_win_episode_{game_num:03d}.png"
-        )
-        plt.savefig(output_file, dpi=150, bbox_inches="tight")
-        plt.close()
-
-        if (idx + 1) % 10 == 0:
-            print(f"  Saved {idx + 1}/{len(player1_win_episodes)} episode plots...")
-
-    print(f"All individual episode plots saved to: {output_dir}/")
-    print(f"Total: {len(player1_win_episodes)} plots created")
-
-
-def plot_individual_player2_win_episodes(episodes, output_dir=None):
-    """
-    Plot and save individual reward distributions for each episode where Player 2 wins (Player 1 loses).
-
-    Args:
-        episodes: Dictionary with episode data
-        output_dir: Directory to save individual episode plots (default: figures/player2_win_episodes)
-    """
-    import os
-
-    if output_dir is None:
-        output_dir = "src/rl_hockey/scripts/figures/player2_win_episodes"
-
-    # Create output directory if it doesn't exist
-    os.makedirs(output_dir, exist_ok=True)
-
-    player2_win_episodes = episodes["player2_wins"]
-
-    if not player2_win_episodes:
-        print("No episodes where Player 2 won. Skipping individual episode plots.")
-        return
-
-    print(
-        f"\nCreating individual plots for {len(player2_win_episodes)} Player 2 win episodes (Player 1 losses)..."
-    )
-
-    for idx, episode in enumerate(player2_win_episodes):
-        fig, axes = plt.subplots(2, 1, figsize=(10, 8))
-
-        rewards = episode["rewards"]
-        game_num = episode["game_num"]
-        total_reward = episode["total_reward"]
-        mean_reward = episode["mean_reward"]
-        steps = episode["steps"]
-
-        # Plot 1: Reward distribution histogram
-        ax1 = axes[0]
-        ax1.hist(
-            rewards,
-            bins=min(50, len(rewards)),
-            alpha=0.7,
-            color="red",
-            edgecolor="black",
-        )
-        ax1.axvline(x=0, color="blue", linestyle="--", linewidth=2, label="Zero")
-        ax1.axvline(
-            x=mean_reward,
-            color="blue",
-            linestyle="--",
-            linewidth=2,
-            label=f"Mean: {mean_reward:.4f}",
-        )
-        ax1.set_xlabel("Reward")
-        ax1.set_ylabel("Frequency")
-        ax1.set_title(
-            f"Player 2 Win (Player 1 Loss) - Episode {game_num} - Reward Distribution\n"
-            f"Total Reward: {total_reward:.4f}, Mean: {mean_reward:.4f}, Steps: {steps}"
-        )
-        ax1.legend()
-        ax1.grid(True, alpha=0.3)
-
-        # Plot 2: Reward over time (step-by-step)
-        ax2 = axes[1]
-        steps_array = np.arange(len(rewards))
-        ax2.plot(steps_array, rewards, color="red", linewidth=1.5, alpha=0.7)
-        ax2.axhline(y=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax2.axhline(
-            y=mean_reward,
-            color="blue",
-            linestyle="--",
-            linewidth=2,
-            label=f"Mean: {mean_reward:.4f}",
-        )
-        ax2.fill_between(
-            steps_array,
-            rewards,
-            0,
-            alpha=0.3,
-            color="green",
-            where=(np.array(rewards) >= 0),
-        )
-        ax2.fill_between(
-            steps_array,
-            rewards,
-            0,
-            alpha=0.3,
-            color="red",
-            where=(np.array(rewards) < 0),
-        )
-        ax2.set_xlabel("Step in Episode")
-        ax2.set_ylabel("Reward")
-        ax2.set_title(f"Reward Over Time - Episode {game_num}")
-        ax2.legend()
-        ax2.grid(True, alpha=0.3)
-
-        plt.tight_layout()
-        output_file = os.path.join(
-            output_dir, f"player2_win_episode_{game_num:03d}.png"
-        )
-        plt.savefig(output_file, dpi=150, bbox_inches="tight")
-        plt.close()
-
-        if (idx + 1) % 10 == 0:
-            print(f"  Saved {idx + 1}/{len(player2_win_episodes)} episode plots...")
-
-    print(f"All individual episode plots saved to: {output_dir}/")
-    print(f"Total: {len(player2_win_episodes)} plots created")
-
-
-def visualize_reward_backprop(
-    episodes,
-    win_reward_bonus=1.0,
-    win_reward_discount=0.99,
-    output_dir=None,
-):
-    """
-    Visualize how reward backpropagation affects rewards at each time step.
-
-    Creates plots showing original rewards, bonus rewards, and final rewards
-    after backpropagation for winning episodes.
-
-    Args:
-        episodes: Dictionary with episode data, must have 'player1_wins' key
-        win_reward_bonus: Bonus reward to add to each step in a winning episode
-        win_reward_discount: Discount factor for applying win reward bonus
-        output_dir: Directory to save plots (default: figures/reward_backprop)
-    """
-    import os
-
-    if output_dir is None:
-        output_dir = "src/rl_hockey/scripts/figures/reward_backprop"
-
-    os.makedirs(output_dir, exist_ok=True)
-
-    player1_win_episodes = episodes.get("player1_wins", [])
-
-    if not player1_win_episodes:
-        print("No episodes where Player 1 won. Skipping backpropagation visualization.")
-        return
-
-    print(
-        f"\nCreating reward backpropagation visualizations for {len(player1_win_episodes)} Player 1 win episodes..."
-    )
-
-    for idx, episode in enumerate(player1_win_episodes):
-        rewards = np.array(episode["rewards"], dtype=np.float32)
-        game_num = episode["game_num"]
-        winner = episode["winner"]
-
-        # Apply backpropagation
-        final_rewards, original_rewards, bonus_rewards = apply_win_reward_backprop(
-            rewards,
-            winner=winner,
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-            use_torch=False,
-        )
-
-        # Create figure with 3 subplots
-        fig, axes = plt.subplots(3, 1, figsize=(12, 10))
-
-        steps = np.arange(len(rewards))
-
-        # Plot 1: Original rewards
-        ax1 = axes[0]
-        ax1.plot(
-            steps, original_rewards, color="blue", linewidth=2, label="Original Rewards"
-        )
-        ax1.fill_between(
-            steps,
-            original_rewards,
-            0,
-            alpha=0.3,
-            color="green",
-            where=(original_rewards >= 0),
-        )
-        ax1.fill_between(
-            steps,
-            original_rewards,
-            0,
-            alpha=0.3,
-            color="red",
-            where=(original_rewards < 0),
-        )
-        ax1.axhline(y=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax1.set_xlabel("Step in Episode")
-        ax1.set_ylabel("Reward")
-        ax1.set_title(f"Episode {game_num} - Original Rewards (Before Backpropagation)")
-        ax1.legend()
-        ax1.grid(True, alpha=0.3)
-
-        # Plot 2: Bonus rewards
-        ax2 = axes[1]
-        ax2.plot(
-            steps, bonus_rewards, color="orange", linewidth=2, label="Bonus Rewards"
-        )
-        ax2.fill_between(steps, bonus_rewards, 0, alpha=0.3, color="orange")
-        ax2.axhline(y=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax2.set_xlabel("Step in Episode")
-        ax2.set_ylabel("Bonus Reward")
-        ax2.set_title(
-            f"Episode {game_num} - Bonus Rewards (win_bonus={win_reward_bonus}, "
-            f"discount={win_reward_discount})"
-        )
-        ax2.legend()
-        ax2.grid(True, alpha=0.3)
-
-        # Plot 3: Final rewards (original + bonus)
-        ax3 = axes[2]
-        ax3.plot(
-            steps, final_rewards, color="green", linewidth=2, label="Final Rewards"
-        )
-        ax3.plot(
-            steps,
-            original_rewards,
-            color="blue",
-            linewidth=1.5,
-            linestyle="--",
-            alpha=0.5,
-            label="Original Rewards",
-        )
-        ax3.fill_between(
-            steps,
-            final_rewards,
-            0,
-            alpha=0.3,
-            color="green",
-            where=(final_rewards >= 0),
-        )
-        ax3.fill_between(
-            steps,
-            final_rewards,
-            0,
-            alpha=0.3,
-            color="red",
-            where=(final_rewards < 0),
-        )
-        ax3.axhline(y=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-        ax3.set_xlabel("Step in Episode")
-        ax3.set_ylabel("Reward")
-        ax3.set_title(
-            f"Episode {game_num} - Final Rewards (After Backpropagation)\n"
-            f"Total Original: {np.sum(original_rewards):.4f}, "
-            f"Total Bonus: {np.sum(bonus_rewards):.4f}, "
-            f"Total Final: {np.sum(final_rewards):.4f}"
-        )
-        ax3.legend()
-        ax3.grid(True, alpha=0.3)
-
-        plt.tight_layout()
-        output_file = os.path.join(
-            output_dir, f"reward_backprop_episode_{game_num:03d}.png"
-        )
-        plt.savefig(output_file, dpi=150, bbox_inches="tight")
-        plt.close()
-
-        if (idx + 1) % 10 == 0:
-            print(
-                f"  Saved {idx + 1}/{len(player1_win_episodes)} backpropagation plots..."
-            )
-
-    # Create summary plot comparing original vs final rewards across all episodes
-    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
-
-    # Collect all rewards
-    all_original_totals = []
-    all_final_totals = []
-    all_bonus_totals = []
-
-    for episode in player1_win_episodes:
-        rewards = np.array(episode["rewards"], dtype=np.float32)
-        winner = episode["winner"]
-        final_rewards, original_rewards, bonus_rewards = apply_win_reward_backprop(
-            rewards,
-            winner=winner,
-            win_reward_bonus=win_reward_bonus,
-            win_reward_discount=win_reward_discount,
-            use_torch=False,
-        )
-        all_original_totals.append(np.sum(original_rewards))
-        all_final_totals.append(np.sum(final_rewards))
-        all_bonus_totals.append(np.sum(bonus_rewards))
-
-    # Plot 1: Distribution of episode totals
-    ax1 = axes[0]
-    ax1.hist(
-        all_original_totals,
-        bins=30,
-        alpha=0.5,
-        color="blue",
-        label=f"Original Total Rewards (mean={np.mean(all_original_totals):.4f})",
-        edgecolor="black",
-    )
-    ax1.hist(
-        all_final_totals,
-        bins=30,
-        alpha=0.5,
-        color="green",
-        label=f"Final Total Rewards (mean={np.mean(all_final_totals):.4f})",
-        edgecolor="black",
-    )
-    ax1.axvline(x=0, color="black", linestyle=":", linewidth=1, alpha=0.5)
-    ax1.set_xlabel("Episode Total Reward")
-    ax1.set_ylabel("Frequency")
-    ax1.set_title(
-        f"Distribution of Episode Total Rewards\n"
-        f"Before vs After Backpropagation (n={len(player1_win_episodes)} episodes)"
-    )
-    ax1.legend()
-    ax1.grid(True, alpha=0.3)
-
-    # Plot 2: Scatter plot showing change
-    ax2 = axes[1]
-    ax2.scatter(
-        all_original_totals,
-        all_final_totals,
-        alpha=0.6,
-        s=50,
-        color="green",
-        edgecolors="black",
-        linewidths=0.5,
-    )
-    # Add diagonal line (y=x)
-    min_val = min(min(all_original_totals), min(all_final_totals))
-    max_val = max(max(all_original_totals), max(all_final_totals))
-    ax2.plot(
-        [min_val, max_val],
-        [min_val, max_val],
-        "r--",
-        linewidth=2,
-        label="No Change (y=x)",
-        alpha=0.5,
-    )
-    ax2.set_xlabel("Original Total Reward")
-    ax2.set_ylabel("Final Total Reward")
-    ax2.set_title(
-        f"Reward Change After Backpropagation\n"
-        f"Mean Bonus Added: {np.mean(all_bonus_totals):.4f}"
-    )
-    ax2.legend()
-    ax2.grid(True, alpha=0.3)
-
-    plt.tight_layout()
-    summary_file = os.path.join(output_dir, "reward_backprop_summary.png")
-    plt.savefig(summary_file, dpi=150, bbox_inches="tight")
-    plt.close()
-
-    print(f"All backpropagation visualizations saved to: {output_dir}/")
-    print(f"Total: {len(player1_win_episodes)} individual plots + 1 summary plot")
-
-
-def main():
-    """Main function to run the reward analysis."""
-    # Clean up old figures and directories
-    figures_base_dir = "src/rl_hockey/scripts/figures"
-    directories_to_remove = [
-        os.path.join(figures_base_dir, "player1_win_episodes"),
-        os.path.join(figures_base_dir, "player2_win_episodes"),
-        os.path.join(figures_base_dir, "reward_backprop"),
-    ]
-    files_to_remove = [
-        os.path.join(figures_base_dir, "reward_distribution_analysis.png"),
-        os.path.join(figures_base_dir, "reward_backprop_summary.png"),
-    ]
-
-    print("Cleaning up old figures and directories...")
-    for dir_path in directories_to_remove:
-        if os.path.exists(dir_path):
-            shutil.rmtree(dir_path)
-            print(f"  Removed directory: {dir_path}")
-
-    for file_path in files_to_remove:
-        if os.path.exists(file_path):
-            os.remove(file_path)
-            print(f"  Removed file: {file_path}")
-
-    print("Cleanup complete!\n")
-
-    # Play 100 games with random actions
-    episodes = play_random_games(num_games=100, max_steps=250)
-
-    # Analyze rewards
-    reward_data = analyze_rewards(episodes)
-
-    # Plot distributions
-    plot_reward_distributions(reward_data, episodes)
-
-    # Plot individual Player 1 win episodes (with backpropagation visualization)
-    plot_individual_player1_win_episodes(
-        episodes,
-        win_reward_bonus=10.0,
-        win_reward_discount=0.92,
-    )
-
-    # Plot individual Player 2 win episodes (Player 1 losses)
-    plot_individual_player2_win_episodes(episodes)
-
-    # Visualize reward backpropagation for winning episodes
-    visualize_reward_backprop(
-        episodes,
-        win_reward_bonus=10.0,
-        win_reward_discount=0.92,
-    )
-
-    print("Analysis complete!")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/src/rl_hockey/scripts/run_agent_to_mp4.py b/src/rl_hockey/scripts/run_agent_to_mp4.py
deleted file mode 100644
index 2f14e46..0000000
--- a/src/rl_hockey/scripts/run_agent_to_mp4.py
+++ /dev/null
@@ -1,938 +0,0 @@
-import logging
-import os
-import sys
-import time
-import warnings
-from datetime import datetime
-
-import hockey.hockey_env as h_env
-import numpy as np
-
-from rl_hockey.common.utils import (
-    discrete_to_continuous_action_with_fineness,
-    get_discrete_action_dim,
-)
-
-warnings.filterwarnings("ignore", category=UserWarning)
-os.environ["PYGAME_HIDE_SUPPORT_PROMPT"] = "1"
-
-logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
-logger = logging.getLogger("agent_video")
-
-
-class ALSAFilter:
-    def __init__(self, original_stderr):
-        self.original_stderr = original_stderr
-
-    def write(self, message):
-        if "ALSA" not in message and "pkg_resources" not in message:
-            self.original_stderr.write(message)
-
-    def flush(self):
-        self.original_stderr.flush()
-
-
-if sys.platform == "linux":
-    sys.stderr = ALSAFilter(sys.stderr)
-
-# Allow MODEL_PATH to be overridden via environment variable
-# If relative path, convert to absolute based on script location or PROJECT_DIR
-_DEFAULT_MODEL_PATH = "results/tdmpc2_runs/2026-01-19_16-45-53/models/TDMPC2_run_lr3e04_bs512_h128_128_128_f2354251_20260119_164553_ep000220.pt"
-_MODEL_PATH_ENV = os.environ.get("MODEL_PATH", _DEFAULT_MODEL_PATH)
-
-if os.path.isabs(_MODEL_PATH_ENV):
-    MODEL_PATH = _MODEL_PATH_ENV
-else:
-    # Try to resolve relative path based on PROJECT_DIR or script location
-    project_dir = os.environ.get("PROJECT_DIR")
-    if project_dir and os.path.exists(project_dir):
-        MODEL_PATH = os.path.join(project_dir, _MODEL_PATH_ENV)
-    else:
-        # Fall back to relative path from script location
-        script_dir = os.path.dirname(os.path.abspath(__file__))
-        # Script is at: PROJECT_ROOT/src/rl_hockey/scripts/
-        # Go up 3 levels to reach PROJECT_ROOT
-        project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
-        MODEL_PATH = os.path.join(project_root, _MODEL_PATH_ENV)
-NUM_GAMES = 25
-OPPONENT_TYPE = "basic_strong"
-PAUSE_BETWEEN_GAMES = 1.5
-FRAME_DELAY = (
-    0.05  # Delay in video playback (seconds per frame). Execution runs at full speed.
-)
-MAX_STEPS = 250
-VIDEO_FPS = 50
-ACTION_FINENESS = None  # Set to None to auto-detect, or specify 3, 5, 7, etc.
-ENV_MODE = "NORMAL"  # "NORMAL" (250 steps), "TRAIN_SHOOTING" (80 steps), or "TRAIN_DEFENSE" (80 steps)
-
-
-def infer_fineness_from_action_dim(action_dim, keep_mode=True):
-    """
-    Infer the fineness parameter from the action dimension.
-    Returns None if it doesn't match a known fineness pattern.
-    """
-    # Try common fineness values: 3, 5, 7, 9, etc.
-    for fineness in [3, 5, 7, 9, 11, 13, 15]:
-        expected_dim = get_discrete_action_dim(fineness=fineness, keep_mode=keep_mode)
-        if expected_dim == action_dim:
-            return fineness
-    return None
-
-
-def detect_algorithm_from_filename(model_path):
-    """Detect algorithm type from model filename."""
-    filename = os.path.basename(model_path)
-    if "TDMPC2" in filename or "tdmpc2" in filename.lower():
-        return "TDMPC2"
-    elif "DDDQN" in filename or "ddqn" in filename.lower():
-        return "DDDQN"
-    elif "SAC" in filename or "sac" in filename.lower():
-        return "SAC"
-    return None
-
-
-def load_agent(model_path, state_dim, action_dim, algorithm=None):
-    """Load agent from checkpoint, auto-detecting algorithm if not specified."""
-    if algorithm is None:
-        algorithm = detect_algorithm_from_filename(model_path)
-        if algorithm is None:
-            raise ValueError(
-                f"Could not detect algorithm from filename: {model_path}. "
-                f"Please specify algorithm explicitly or use a filename containing TDMPC2, DDDQN, or SAC."
-            )
-        logger.info(f"Auto-detected algorithm: {algorithm}")
-
-    logger.info(f"Loading {algorithm} model from: {model_path}")
-
-    if algorithm == "TDMPC2":
-        import torch
-
-        from rl_hockey.TD_MPC2.tdmpc2 import TDMPC2
-
-        checkpoint = torch.load(model_path, map_location="cpu")
-        obs_dim = checkpoint.get("obs_dim", state_dim)
-        action_dim_checkpoint = checkpoint.get("action_dim", action_dim)
-        latent_dim = checkpoint.get("latent_dim", 512)
-        hidden_dim = checkpoint.get("hidden_dim", [256, 256, 256])
-        num_q = checkpoint.get("num_q", 5)
-
-        agent = TDMPC2(
-            obs_dim=obs_dim,
-            action_dim=action_dim_checkpoint,
-            latent_dim=latent_dim,
-            hidden_dim=hidden_dim,
-            num_q=num_q,
-        )
-        agent.load(model_path)
-        logger.info("TDMPC2 model loaded successfully!")
-        return agent, "TDMPC2"
-
-    elif algorithm == "DDDQN":
-        from rl_hockey.DDDQN import DDDQN
-
-        agent = DDDQN(
-            state_dim=state_dim, action_dim=action_dim, hidden_dim=[256, 256, 256]
-        )
-        agent.load(model_path)
-        logger.info("DDDQN model loaded successfully!")
-        return agent, "DDDQN"
-
-    elif algorithm == "SAC":
-        from rl_hockey.sac.sac import SAC
-
-        agent = SAC(state_dim=state_dim, action_dim=action_dim)
-        agent.load(model_path)
-        logger.info("SAC model loaded successfully!")
-        return agent, "SAC"
-
-    else:
-        raise ValueError(f"Unsupported algorithm: {algorithm}")
-
-
-def create_blank_frames(frame_shape, duration_seconds, fps=50):
-    num_frames = int(duration_seconds * fps)
-    blank_frame = np.zeros(frame_shape, dtype=np.uint8)
-    # Use list comprehension with copy to avoid repeated allocations
-    return [blank_frame.copy() for _ in range(num_frames)]
-
-
-def apply_frame_delay(frames, frame_delay, fps=50):
-    """
-    Duplicate frames to create delay effect in video without slowing down execution.
-    If frame_delay > 0, each frame is duplicated to create the delay in playback.
-    """
-    if frame_delay <= 0:
-        return frames
-
-    frames_per_step = int(frame_delay * fps)
-    delayed_frames = []
-    for frame in frames:
-        # Duplicate each frame to create the delay effect
-        delayed_frames.extend([frame] * frames_per_step)
-
-    return delayed_frames
-
-
-def log_stats_summary(game_stats, game_num, step_count):
-    """
-    Log statistics summary for a game.
-
-    Args:
-        game_stats: list of stats dictionaries, one per step
-        game_num: game number
-        step_count: number of steps in the game
-    """
-    if not game_stats:
-        return
-
-    # Aggregate stats across all steps
-    agg_stats = {
-        "q_min": [],
-        "q_max": [],
-        "q_mean": [],
-        "q_std": [],
-        "q_values_all": [],
-        "q_spread": [],
-        "q_coefficient_of_variation": [],
-        "q_policy_min": [],
-        "q_policy_max": [],
-        "q_policy_mean": [],
-        "dynamics_latent_change_norm": [],
-        "dynamics_prediction_error_norm": [],
-        "dynamics_prediction_error_mse": [],
-        "reward_pred": [],
-        "reward_pred_terminal": [],  # Terminal step rewards
-        "encoder_latent_norm": [],
-        "action_norm": [],
-        "action_policy_diff_norm": [],
-        "action_smoothness": [],
-        "latent_smoothness": [],
-        "mppi_elite_return_min": [],
-        "mppi_elite_return_max": [],
-        "mppi_elite_return_mean": [],
-        "mppi_elite_return_std": [],
-        "mppi_final_std": [],
-        "mppi_std_convergence": [],
-    }
-
-    for step_stat in game_stats:
-        agg_stats["q_min"].append(step_stat.get("q_min", 0))
-        agg_stats["q_max"].append(step_stat.get("q_max", 0))
-        agg_stats["q_mean"].append(step_stat.get("q_mean", 0))
-        agg_stats["q_std"].append(step_stat.get("q_std", 0))
-        if "q_values" in step_stat:
-            agg_stats["q_values_all"].extend(step_stat["q_values"])
-        agg_stats["q_spread"].append(step_stat.get("q_spread", 0))
-        agg_stats["q_coefficient_of_variation"].append(
-            step_stat.get("q_coefficient_of_variation", 0)
-        )
-        agg_stats["q_policy_min"].append(step_stat.get("q_policy_min", 0))
-        agg_stats["q_policy_max"].append(step_stat.get("q_policy_max", 0))
-        agg_stats["q_policy_mean"].append(step_stat.get("q_policy_mean", 0))
-        agg_stats["dynamics_latent_change_norm"].append(
-            step_stat.get("dynamics_latent_change_norm", 0)
-        )
-        # Handle None values for dynamics prediction error (first step)
-        dynamics_pred_error_norm = step_stat.get("dynamics_prediction_error_norm")
-        if dynamics_pred_error_norm is not None:
-            agg_stats["dynamics_prediction_error_norm"].append(dynamics_pred_error_norm)
-        dynamics_pred_error_mse = step_stat.get("dynamics_prediction_error_mse")
-        if dynamics_pred_error_mse is not None:
-            agg_stats["dynamics_prediction_error_mse"].append(dynamics_pred_error_mse)
-        agg_stats["reward_pred"].append(step_stat.get("reward_pred", 0))
-        # Track terminal reward separately
-        if "terminal_reward_actual" in step_stat:
-            agg_stats["reward_pred_terminal"].append(
-                {
-                    "predicted": step_stat.get("reward_pred", 0),
-                    "actual": step_stat.get("terminal_reward_actual", 0),
-                    "winner": step_stat.get("winner_info", 0),
-                }
-            )
-        agg_stats["encoder_latent_norm"].append(step_stat.get("encoder_latent_norm", 0))
-        agg_stats["action_norm"].append(step_stat.get("action_norm", 0))
-        agg_stats["action_policy_diff_norm"].append(
-            step_stat.get("action_policy_diff_norm", 0)
-        )
-        # Handle None values for smoothness (first step)
-        action_smoothness = step_stat.get("action_smoothness")
-        if action_smoothness is not None:
-            agg_stats["action_smoothness"].append(action_smoothness)
-        latent_smoothness = step_stat.get("latent_smoothness")
-        if latent_smoothness is not None:
-            agg_stats["latent_smoothness"].append(latent_smoothness)
-        # MPC/Planning stats
-        if "mppi_elite_return_min" in step_stat:
-            agg_stats["mppi_elite_return_min"].append(
-                step_stat["mppi_elite_return_min"]
-            )
-            agg_stats["mppi_elite_return_max"].append(
-                step_stat["mppi_elite_return_max"]
-            )
-            agg_stats["mppi_elite_return_mean"].append(
-                step_stat["mppi_elite_return_mean"]
-            )
-            agg_stats["mppi_elite_return_std"].append(
-                step_stat["mppi_elite_return_std"]
-            )
-        if "mppi_final_std" in step_stat:
-            agg_stats["mppi_final_std"].append(step_stat["mppi_final_std"])
-        if "mppi_std_convergence" in step_stat:
-            agg_stats["mppi_std_convergence"].append(step_stat["mppi_std_convergence"])
-
-    # Compute summary statistics
-    logger.info(f"  TDMPC2 Stats Summary (Game {game_num}, {step_count} steps):")
-    logger.info(
-        f"    Q-values (selected action): min={min(agg_stats['q_min']):.4f}, "
-        f"max={max(agg_stats['q_max']):.4f}, mean={np.mean(agg_stats['q_mean']):.4f}"
-    )
-    if agg_stats["q_values_all"]:
-        logger.info(
-            f"    Q-values (all networks): min={min(agg_stats['q_values_all']):.4f}, "
-            f"max={max(agg_stats['q_values_all']):.4f}, "
-            f"mean={np.mean(agg_stats['q_values_all']):.4f}"
-        )
-    logger.info(
-        f"    Q-values (policy action): min={min(agg_stats['q_policy_min']):.4f}, "
-        f"max={max(agg_stats['q_policy_max']):.4f}, mean={np.mean(agg_stats['q_policy_mean']):.4f}"
-    )
-    logger.info(
-        f"    Dynamics (latent change norm): min={min(agg_stats['dynamics_latent_change_norm']):.4f}, "
-        f"max={max(agg_stats['dynamics_latent_change_norm']):.4f}, "
-        f"mean={np.mean(agg_stats['dynamics_latent_change_norm']):.4f}"
-    )
-    if agg_stats["dynamics_prediction_error_norm"]:
-        logger.info(
-            f"    Dynamics prediction error (norm): min={min(agg_stats['dynamics_prediction_error_norm']):.4f}, "
-            f"max={max(agg_stats['dynamics_prediction_error_norm']):.4f}, "
-            f"mean={np.mean(agg_stats['dynamics_prediction_error_norm']):.4f}"
-        )
-    if agg_stats["dynamics_prediction_error_mse"]:
-        logger.info(
-            f"    Dynamics prediction error (MSE): min={min(agg_stats['dynamics_prediction_error_mse']):.4f}, "
-            f"max={max(agg_stats['dynamics_prediction_error_mse']):.4f}, "
-            f"mean={np.mean(agg_stats['dynamics_prediction_error_mse']):.4f}"
-        )
-
-    # Collect and log multi-step dynamics prediction errors (1-, 5-, 10-step ahead)
-    multi_step_errors_by_horizon = {}
-    for step_stat in game_stats:
-        if "multi_step_errors" in step_stat:
-            for horizon, error_info in step_stat["multi_step_errors"].items():
-                if horizon not in multi_step_errors_by_horizon:
-                    multi_step_errors_by_horizon[horizon] = {
-                        "error_norm": [],
-                        "error_mse": [],
-                        "error_mean": [],
-                        "error_std": [],
-                    }
-                multi_step_errors_by_horizon[horizon]["error_norm"].append(
-                    error_info["error_norm"]
-                )
-                multi_step_errors_by_horizon[horizon]["error_mse"].append(
-                    error_info["error_mse"]
-                )
-                multi_step_errors_by_horizon[horizon]["error_mean"].append(
-                    error_info["error_mean"]
-                )
-                multi_step_errors_by_horizon[horizon]["error_std"].append(
-                    error_info["error_std"]
-                )
-
-    if multi_step_errors_by_horizon:
-        logger.info(
-            "  DYNAMICS MODEL ACCURACY BY HORIZON (latent L2 error, lower=better):"
-        )
-        for horizon in sorted(multi_step_errors_by_horizon.keys()):
-            errors = multi_step_errors_by_horizon[horizon]
-            mean_norm = np.mean(errors["error_norm"])
-            mean_mse = np.mean(errors["error_mse"])
-            n = len(errors["error_norm"])
-            logger.info(
-                f"    {horizon}-step: mean_norm={mean_norm:.4f}, mean_mse={mean_mse:.4f} (n={n})"
-            )
-
-    logger.info(
-        f"    Reward prediction: min={min(agg_stats['reward_pred']):.4f}, "
-        f"max={max(agg_stats['reward_pred']):.4f}, mean={np.mean(agg_stats['reward_pred']):.4f}"
-    )
-    logger.info(
-        f"    Encoder (latent norm): min={min(agg_stats['encoder_latent_norm']):.4f}, "
-        f"max={max(agg_stats['encoder_latent_norm']):.4f}, "
-        f"mean={np.mean(agg_stats['encoder_latent_norm']):.4f}"
-    )
-    logger.info(
-        f"    Action norm: min={min(agg_stats['action_norm']):.4f}, "
-        f"max={max(agg_stats['action_norm']):.4f}, mean={np.mean(agg_stats['action_norm']):.4f}"
-    )
-    logger.info("")
-    logger.info("  Uncertainty Metrics:")
-    if agg_stats["q_spread"]:
-        logger.info(
-            f"    Q-value spread: min={min(agg_stats['q_spread']):.4f}, "
-            f"max={max(agg_stats['q_spread']):.4f}, mean={np.mean(agg_stats['q_spread']):.4f}"
-        )
-    if agg_stats["q_coefficient_of_variation"]:
-        logger.info(
-            f"    Q-value CV (std/mean): min={min(agg_stats['q_coefficient_of_variation']):.4f}, "
-            f"max={max(agg_stats['q_coefficient_of_variation']):.4f}, "
-            f"mean={np.mean(agg_stats['q_coefficient_of_variation']):.4f}"
-        )
-    logger.info("")
-    logger.info("  Action Analysis:")
-    if agg_stats["action_policy_diff_norm"]:
-        logger.info(
-            f"    Action-Policy diff norm: min={min(agg_stats['action_policy_diff_norm']):.4f}, "
-            f"max={max(agg_stats['action_policy_diff_norm']):.4f}, "
-            f"mean={np.mean(agg_stats['action_policy_diff_norm']):.4f}"
-        )
-    if agg_stats["action_smoothness"]:
-        logger.info(
-            f"    Action smoothness: min={min(agg_stats['action_smoothness']):.4f}, "
-            f"max={max(agg_stats['action_smoothness']):.4f}, "
-            f"mean={np.mean(agg_stats['action_smoothness']):.4f}"
-        )
-    if agg_stats["latent_smoothness"]:
-        logger.info(
-            f"    Latent smoothness: min={min(agg_stats['latent_smoothness']):.4f}, "
-            f"max={max(agg_stats['latent_smoothness']):.4f}, "
-            f"mean={np.mean(agg_stats['latent_smoothness']):.4f}"
-        )
-    logger.info("")
-    logger.info("  MPC Planning Diagnostics:")
-    if agg_stats["mppi_elite_return_mean"]:
-        logger.info(
-            f"    Elite returns: min={min(agg_stats['mppi_elite_return_min']):.4f}, "
-            f"max={max(agg_stats['mppi_elite_return_max']):.4f}, "
-            f"mean={np.mean(agg_stats['mppi_elite_return_mean']):.4f}, "
-            f"std={np.mean(agg_stats['mppi_elite_return_std']):.4f}"
-        )
-    if agg_stats["mppi_final_std"]:
-        logger.info(
-            f"    Final sampling std: min={min(agg_stats['mppi_final_std']):.4f}, "
-            f"max={max(agg_stats['mppi_final_std']):.4f}, "
-            f"mean={np.mean(agg_stats['mppi_final_std']):.4f}"
-        )
-    if agg_stats["mppi_std_convergence"]:
-        logger.info(
-            f"    Std convergence (init-final): min={min(agg_stats['mppi_std_convergence']):.4f}, "
-            f"max={max(agg_stats['mppi_std_convergence']):.4f}, "
-            f"mean={np.mean(agg_stats['mppi_std_convergence']):.4f}"
-        )
-
-
-def run_game(
-    env,
-    agent,
-    opponent,
-    game_num,
-    max_steps=250,
-    action_fineness=None,
-    algorithm="DDDQN",
-    collect_stats=False,
-):
-    """
-    Run a game at full speed (no delays during execution).
-    Frame delays are applied later when creating the video.
-    """
-    obs, info = env.reset()
-    obs_agent2 = env.obs_agent_two()
-    frames = []
-    step_count = 0
-    total_reward = 0
-    # Pre-compute action dimension for random opponent to avoid repeated calls
-    if opponent is None:
-        if action_fineness is not None:
-            # Use fineness-based action dimension
-            action_dim = 4 if env.keep_mode else 3
-        else:
-            action_dim = len(env.discrete_to_continous_action(0))
-
-    # Collect stats if requested (only for TDMPC2)
-    game_stats = []
-    prev_action_p1 = None
-    prev_latent = None
-    prev_predicted_next_latent = None
-    episode_reward = 0  # Track actual episode reward
-
-    # Multi-step prediction tracking: retrospectively evaluate dynamics model
-    # Store (latent_state, action) pairs for retrospective evaluation
-    latent_action_history = []  # List of (latent_state, action) tuples
-
-    for step in range(max_steps):
-        frame = env.render(mode="rgb_array")
-        frames.append(frame)
-        # Convert observation to float32 for agent
-        obs_float = obs.astype(np.float32) if obs.dtype != np.float32 else obs
-
-        # Handle action selection based on algorithm type
-        step_stats = None
-        if algorithm == "TDMPC2":
-            # Use act_with_stats if collecting stats
-            if collect_stats and hasattr(agent, "act_with_stats"):
-                action_p1, step_stats = agent.act_with_stats(
-                    obs_float,
-                    deterministic=True,
-                    prev_action=prev_action_p1,
-                    prev_latent=prev_latent,
-                    prev_predicted_next_latent=prev_predicted_next_latent,
-                    t0=(step == 0),
-                )
-                game_stats.append(step_stats)
-                # Extract latent, action, and predicted next latent for next iteration
-                if step_stats and "_latent_state" in step_stats:
-                    prev_latent = step_stats["_latent_state"]
-                if step_stats and "_predicted_next_latent" in step_stats:
-                    prev_predicted_next_latent = step_stats["_predicted_next_latent"]
-
-                # Store latent and action for retrospective multi-step evaluation
-                if collect_stats:
-                    current_latent = step_stats.get("_latent_state")
-                    if current_latent is not None:
-                        latent_action_history.append(
-                            (current_latent.copy(), action_p1.copy())
-                        )
-
-                prev_action_p1 = (
-                    action_p1.copy()
-                    if hasattr(action_p1, "copy")
-                    else np.array(action_p1)
-                )
-            else:
-                action_p1 = agent.act(obs_float, deterministic=True, t0=(step == 0))
-                prev_action_p1 = (
-                    action_p1.copy() if hasattr(action_p1, "copy") else action_p1
-                )
-            # Ensure action is properly shaped (should be (action_dim,))
-            if isinstance(action_p1, (list, tuple)):
-                action_p1 = np.array(action_p1)
-            if action_p1.ndim > 1:
-                action_p1 = action_p1.flatten()
-        elif algorithm == "SAC":
-            # Continuous action algorithms return actions directly
-            action_p1 = agent.act(obs_float, deterministic=True)
-            # Ensure action is properly shaped (should be (action_dim,))
-            if isinstance(action_p1, (list, tuple)):
-                action_p1 = np.array(action_p1)
-            if action_p1.ndim > 1:
-                action_p1 = action_p1.flatten()
-        else:
-            # Discrete action algorithms (DDDQN) need conversion
-            discrete_action = agent.act(obs_float, deterministic=True)
-            # Use fineness-based conversion if fineness is specified
-            if action_fineness is not None:
-                action_p1 = discrete_to_continuous_action_with_fineness(
-                    discrete_action, fineness=action_fineness, keep_mode=env.keep_mode
-                )
-            else:
-                action_p1 = env.discrete_to_continous_action(discrete_action)
-
-        if opponent is not None:
-            action_p2 = opponent.act(obs_agent2)
-        else:
-            action_p2 = np.random.uniform(-1, 1, action_dim)
-        action = np.hstack([action_p1, action_p2])
-        obs, reward, done, truncated, info = env.step(action)
-        obs_agent2 = env.obs_agent_two()
-        total_reward += reward
-        episode_reward += reward
-        step_count += 1
-
-        # Retrospective multi-step evaluation: look back and evaluate predictions
-        if (
-            algorithm == "TDMPC2"
-            and collect_stats
-            and hasattr(agent, "rollout_dynamics_multi_step")
-        ):
-            # Encode current observation to get actual latent state
-            import torch
-
-            obs_tensor = torch.FloatTensor(obs.astype(np.float32)).to(agent.device)
-            actual_latent = (
-                agent.encoder(obs_tensor.unsqueeze(0)).squeeze(0).detach().cpu().numpy()
-            )
-
-            # Look back at past states and evaluate multi-step predictions
-            # Check horizons: 1, 5, 10 steps back
-            evaluation_horizons = [1, 5, 10]
-            for horizon in evaluation_horizons:
-                if step >= horizon and len(latent_action_history) >= horizon:
-                    # Get the state and actions from 'horizon' steps ago
-                    past_idx = len(latent_action_history) - horizon
-                    past_latent, past_action = latent_action_history[past_idx]
-
-                    # Get the sequence of actions that were actually taken
-                    action_sequence = [
-                        latent_action_history[i][1]
-                        for i in range(past_idx, len(latent_action_history))
-                    ]
-
-                    # Roll out dynamics model from past state using actual actions
-                    if len(action_sequence) >= horizon:
-                        predictions = agent.rollout_dynamics_multi_step(
-                            past_latent, action_sequence[:horizon], max_horizon=horizon
-                        )
-
-                        if horizon in predictions:
-                            predicted_latent = predictions[horizon]
-                            error = actual_latent - predicted_latent
-
-                            # Store error in the stats from when the prediction was made
-                            prediction_step = step - horizon
-                            if prediction_step >= 0 and prediction_step < len(
-                                game_stats
-                            ):
-                                pred_stats = game_stats[prediction_step]
-                                if "multi_step_errors" not in pred_stats:
-                                    pred_stats["multi_step_errors"] = {}
-                                pred_stats["multi_step_errors"][horizon] = {
-                                    "error_norm": np.linalg.norm(error),
-                                    "error_mse": np.mean(error**2),
-                                    "error_mean": np.mean(error),
-                                    "error_std": np.std(error),
-                                }
-
-        # Track terminal reward prediction if this is the final step
-        if (done or truncated) and step_stats is not None:
-            step_stats["terminal_reward_actual"] = episode_reward
-            step_stats["winner_info"] = info.get("winner", 0)
-
-        if done or truncated:
-            break
-
-    # Log stats summary if collected
-    if collect_stats and algorithm == "TDMPC2" and game_stats:
-        log_stats_summary(game_stats, game_num, step_count)
-
-    winner = info.get("winner", 0)
-    return frames, step_count, total_reward, winner, info
-
-
-def find_available_models(base_dir=None, max_results=10):
-    """Find available model files in the results directory."""
-    if base_dir is None:
-        base_dir = os.environ.get("PROJECT_DIR", os.getcwd())
-
-    model_files = []
-    results_dir = os.path.join(base_dir, "results")
-
-    # Search in common model locations
-    search_paths = [
-        os.path.join(results_dir, "tdmpc2_runs"),
-        os.path.join(results_dir, "runs"),
-        os.path.join(results_dir, "hyperparameter_runs"),
-    ]
-
-    for search_path in search_paths:
-        if os.path.exists(search_path):
-            for root, dirs, files in os.walk(search_path):
-                for file in files:
-                    if file.endswith(".pt"):
-                        full_path = os.path.join(root, file)
-                        rel_path = os.path.relpath(full_path, base_dir)
-                        model_files.append((full_path, rel_path))
-
-    # Sort by modification time (newest first)
-    model_files.sort(key=lambda x: os.path.getmtime(x[0]), reverse=True)
-    return model_files[:max_results]
-
-
-def get_video_filename(base_folder="videos", base_name="agent_games"):
-    now = datetime.now()
-    dt_str = now.strftime("%Y-%m-%d_%H-%M-%S")
-    if not os.path.exists(base_folder):
-        os.makedirs(base_folder)
-    filename = f"{base_name}_{dt_str}.mp4"
-    return os.path.join(base_folder, filename)
-
-
-def main(
-    model_path=MODEL_PATH,
-    num_games=NUM_GAMES,
-    opponent_type=OPPONENT_TYPE,
-    pause_between_games=PAUSE_BETWEEN_GAMES,
-    frame_delay=FRAME_DELAY,
-    max_steps=MAX_STEPS,
-    video_fps=VIDEO_FPS,
-    action_fineness=ACTION_FINENESS,
-    env_mode=ENV_MODE,
-):
-    # Validate model path exists
-    if not os.path.exists(model_path):
-        abs_path = os.path.abspath(model_path)
-        error_msg = (
-            f"Model file not found: {model_path}\n"
-            f"Absolute path: {abs_path}\n"
-            f"Current working directory: {os.getcwd()}\n\n"
-        )
-
-        # Try to find available models
-        try:
-            available_models = find_available_models()
-            if available_models:
-                error_msg += f"Found {len(available_models)} available model files:\n"
-                for i, (full_path, rel_path) in enumerate(available_models[:5], 1):
-                    mtime = os.path.getmtime(full_path)
-                    mtime_str = time.strftime(
-                        "%Y-%m-%d %H:%M:%S", time.localtime(mtime)
-                    )
-                    error_msg += f"  {i}. {rel_path} (modified: {mtime_str})\n"
-                if len(available_models) > 5:
-                    error_msg += f"  ... and {len(available_models) - 5} more\n"
-                error_msg += (
-                    "\nTo use one of these models, set MODEL_PATH environment variable:\n"
-                    f"  export MODEL_PATH='$PROJECT_DIR/{available_models[0][1]}'\n"
-                    "Or modify the MODEL_PATH constant in the script.\n"
-                )
-            else:
-                error_msg += (
-                    "No model files found in results directories.\n"
-                    "Please check if models exist or set MODEL_PATH to the correct path.\n"
-                )
-        except Exception as e:
-            error_msg += f"Could not search for available models: {e}\n"
-
-        raise FileNotFoundError(error_msg)
-
-    output_file = get_video_filename()
-    logger.info("=" * 60)
-    logger.info("Agent Video Recording")
-    logger.info("=" * 60)
-    logger.info(f"Model: {model_path}")
-    logger.info(f"Model path (absolute): {os.path.abspath(model_path)}")
-    logger.info(f"Output: {output_file}")
-    logger.info(f"Games: {num_games}")
-    logger.info(f"Opponent: {opponent_type}")
-    logger.info(f"Environment mode: {env_mode}")
-    logger.info(f"Max steps per game: {max_steps}")
-    logger.info(f"Frame delay in video: {frame_delay}s per frame")
-    logger.info(f"Video FPS: {video_fps}")
-    logger.info("=" * 60)
-    logger.info("Creating hockey environment...")
-    # Map string mode to enum
-    mode_map = {
-        "NORMAL": h_env.Mode.NORMAL,
-        "TRAIN_SHOOTING": h_env.Mode.TRAIN_SHOOTING,
-        "TRAIN_DEFENSE": h_env.Mode.TRAIN_DEFENSE,
-    }
-    if env_mode not in mode_map:
-        raise ValueError(
-            f"Invalid env_mode: {env_mode}. Must be one of {list(mode_map.keys())}"
-        )
-    env = h_env.HockeyEnv(mode=mode_map[env_mode])
-    # Log actual environment limit after creation
-    env_limit = env.max_timesteps if hasattr(env, "max_timesteps") else "unknown"
-    logger.info(f"Environment created: mode={env_mode}, max_timesteps={env_limit}")
-    if env_limit != "unknown" and max_steps > env_limit:
-        logger.warning(
-            f"Warning: max_steps ({max_steps}) exceeds environment limit ({env_limit}). Games will end at {env_limit} steps."
-        )
-    state_dim = env.observation_space.shape[0]
-
-    # Detect algorithm type and load appropriate agent
-    algorithm = detect_algorithm_from_filename(model_path)
-
-    # Load checkpoint to get the actual action dimension from the model
-    import torch
-
-    checkpoint = torch.load(model_path, map_location="cpu")
-    actual_action_dim = checkpoint.get("action_dim", None)
-
-    if algorithm == "TDMPC2" or algorithm == "SAC":
-        # Continuous action algorithms don't need fineness handling
-        action_dim = (
-            actual_action_dim
-            if actual_action_dim is not None
-            else (4 if env.keep_mode else 3)
-        )
-        logger.info(f"State dimension: {state_dim}")
-        logger.info(f"Action dimension: {action_dim}")
-        agent, detected_algorithm = load_agent(
-            model_path, state_dim, action_dim, algorithm=algorithm
-        )
-        action_fineness = None
-    else:
-        # Discrete action algorithms (DDDQN) need fineness handling
-        if action_fineness is not None:
-            discrete_action_dim = get_discrete_action_dim(
-                fineness=action_fineness, keep_mode=env.keep_mode
-            )
-            logger.info(f"Using specified fineness: {action_fineness}")
-            if (
-                actual_action_dim is not None
-                and discrete_action_dim != actual_action_dim
-            ):
-                logger.warning(
-                    f"Warning: Specified fineness {action_fineness} gives action_dim {discrete_action_dim}, but model has {actual_action_dim}"
-                )
-                discrete_action_dim = actual_action_dim  # Use model's action_dim
-        elif actual_action_dim is not None:
-            # Try to infer fineness from the model's action_dim
-            inferred_fineness = infer_fineness_from_action_dim(
-                actual_action_dim, keep_mode=env.keep_mode
-            )
-            if inferred_fineness is not None:
-                action_fineness = inferred_fineness
-                discrete_action_dim = actual_action_dim
-                logger.info(
-                    f"Auto-detected fineness: {action_fineness} from model action_dim: {actual_action_dim}"
-                )
-            else:
-                # Fall back to using model's action_dim but warn about fineness
-                discrete_action_dim = actual_action_dim
-                logger.warning(
-                    f"Could not infer fineness from action_dim {actual_action_dim}"
-                )
-                logger.warning(
-                    "Assuming default fineness=3. If actions seem incorrect, specify action_fineness parameter"
-                )
-                action_fineness = None  # Will use env.discrete_to_continous_action
-        else:
-            # Fall back to default (fineness=3)
-            discrete_action_dim = 7 if not env.keep_mode else 8
-            logger.info(
-                f"Using default action dimension (fineness=3): {discrete_action_dim}"
-            )
-            logger.info(
-                "If your model uses a different fineness, specify action_fineness parameter"
-            )
-            action_fineness = None  # Will use env.discrete_to_continous_action
-
-        logger.info(f"State dimension: {state_dim}")
-        logger.info(f"Action dimension: {discrete_action_dim}")
-        if action_fineness is not None:
-            logger.info(f"Action fineness: {action_fineness}")
-        agent, detected_algorithm = load_agent(
-            model_path, state_dim, discrete_action_dim, algorithm=algorithm
-        )
-
-    algorithm = detected_algorithm
-    opponent = None
-    if opponent_type == "basic_weak":
-        opponent = h_env.BasicOpponent(weak=True)
-        logger.info("Using weak BasicOpponent")
-    elif opponent_type == "basic_strong":
-        opponent = h_env.BasicOpponent(weak=False)
-        logger.info("Using strong BasicOpponent")
-    elif opponent_type == "random":
-        opponent = None
-        logger.info("Using random actions for player 2")
-    else:
-        raise ValueError(f"Unknown opponent_type: {opponent_type}")
-    # Get frame shape without extra reset
-    obs_temp, _ = env.reset()
-    frame_temp = env.render(mode="rgb_array")
-    frame_shape = frame_temp.shape
-    logger.info(f"Frame shape: {frame_shape}")
-    logger.info(f"Running {num_games} games at full speed...")
-    all_frames = []
-    game_results = []
-    start_time = time.time()
-    # Enable stats collection for TDMPC2
-    collect_stats = algorithm == "TDMPC2"
-    if collect_stats:
-        logger.info("Stats collection enabled for TDMPC2")
-    for game_num in range(1, num_games + 1):
-        logger.info(f"Game {game_num}/{num_games}...")
-        frames, steps, reward, winner, info = run_game(
-            env,
-            agent,
-            opponent,
-            game_num,
-            max_steps=max_steps,
-            action_fineness=action_fineness,
-            algorithm=algorithm,
-            collect_stats=collect_stats,
-        )
-        all_frames.extend(frames)
-        game_results.append(
-            {"game": game_num, "steps": steps, "reward": reward, "winner": winner}
-        )
-        winner_str = ""
-        if winner == 1:
-            winner_str = "Player 1 (Agent) wins!"
-        elif winner == -1:
-            winner_str = "Player 2 (Opponent) wins!"
-        else:
-            winner_str = "Draw"
-        logger.info(f"  Steps: {steps}, Reward: {reward:.2f}, {winner_str}")
-        if game_num < num_games:
-            logger.info(f"  Adding {pause_between_games}s pause (blank screen)...")
-            blank_frames = create_blank_frames(
-                frame_shape, pause_between_games, fps=video_fps
-            )
-            all_frames.extend(blank_frames)
-    execution_time = time.time() - start_time
-    logger.info(f"Games completed in {execution_time:.2f} seconds")
-    env.close()
-    logger.info("=" * 60)
-    logger.info("Game Summary")
-    logger.info("=" * 60)
-    for result in game_results:
-        winner_str = (
-            "Agent"
-            if result["winner"] == 1
-            else ("Opponent" if result["winner"] == -1 else "Draw")
-        )
-        logger.info(
-            f"Game {result['game']}: {result['steps']} steps, "
-            f"Reward: {result['reward']:.2f}, Winner: {winner_str}"
-        )
-    wins = sum(1 for r in game_results if r["winner"] == 1)
-    losses = sum(1 for r in game_results if r["winner"] == -1)
-    draws = sum(1 for r in game_results if r["winner"] == 0)
-    logger.info(f"Overall: {wins} wins, {losses} losses, {draws} draws")
-    if all_frames:
-        try:
-            import imageio
-
-            # Apply frame delay by duplicating frames (creates delay in video without slowing execution)
-            logger.info(f"Applying frame delay of {frame_delay}s per frame...")
-            original_frame_count = len(all_frames)
-            all_frames = apply_frame_delay(all_frames, frame_delay, fps=video_fps)
-            logger.info(
-                f"Expanded from {original_frame_count} to {len(all_frames)} frames for video"
-            )
-
-            logger.info(f"Saving {len(all_frames)} frames as MP4 video...")
-            logger.info(
-                f"Estimated video duration: {len(all_frames) / video_fps / 60:.1f} minutes"
-            )
-            logger.info("This may take 15-60 minutes depending on your CPU...")
-            encoding_start = time.time()
-            # Optimize video encoding for speed: use faster preset
-            # Note: imageio automatically handles pixel format, so we don't need to specify it
-            imageio.mimsave(
-                output_file,
-                all_frames,
-                fps=video_fps,
-                codec="libx264",
-                quality=8,
-                ffmpeg_params=["-preset", "fast"],
-            )
-            encoding_time = time.time() - encoding_start
-            logger.info(f"Video encoding completed in {encoding_time / 60:.1f} minutes")
-            file_size = os.path.getsize(output_file) / (1024 * 1024)
-            logger.info(f"Video saved to '{output_file}'")
-            logger.info(f"File size: {file_size:.2f} MB")
-        except ImportError:
-            logger.error("=" * 60)
-            logger.error("ERROR: imageio not installed!")
-            logger.error("=" * 60)
-            logger.error("Please install imageio to save videos:")
-            logger.error("  pip install imageio imageio-ffmpeg")
-            logger.warning("Saving frames as numpy array instead...")
-            np.savez(output_file.replace(".mp4", ".npz"), frames=np.array(all_frames))
-            logger.info(f"Frames saved to '{output_file.replace('.mp4', '.npz')}'")
-        except Exception as e:
-            logger.error(f"Error saving video: {e}")
-            logger.warning("Saving frames as numpy array as backup...")
-            np.savez(output_file.replace(".mp4", ".npz"), frames=np.array(all_frames))
-            logger.info(f"Frames saved to '{output_file.replace('.mp4', '.npz')}'")
-    else:
-        logger.warning("No frames collected - video not saved.")
-    logger.info("Done!")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/src/rl_hockey/scripts/run_ddqn_agent_to_mp4.py b/src/rl_hockey/scripts/run_ddqn_agent_to_mp4.py
new file mode 100644
index 0000000..3f0d8c0
--- /dev/null
+++ b/src/rl_hockey/scripts/run_ddqn_agent_to_mp4.py
@@ -0,0 +1,316 @@
+import warnings
+import os
+import sys
+import numpy as np
+import hockey.hockey_env as h_env
+import time
+from datetime import datetime
+import logging
+from rl_hockey.common.utils import get_discrete_action_dim, discrete_to_continuous_action_with_fineness
+
+warnings.filterwarnings('ignore', category=UserWarning)
+os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = '1'
+
+logging.basicConfig(
+    level=logging.INFO,
+    format='[%(levelname)s] %(message)s'
+)
+logger = logging.getLogger("ddqn_video")
+
+class ALSAFilter:
+    def __init__(self, original_stderr):
+        self.original_stderr = original_stderr
+
+    def write(self, message):
+        if 'ALSA' not in message and 'pkg_resources' not in message:
+            self.original_stderr.write(message)
+
+    def flush(self):
+        self.original_stderr.flush()
+
+if sys.platform == 'linux':
+    sys.stderr = ALSAFilter(sys.stderr)
+
+MODEL_PATH = "results/hyperparameter_runs/2026-01-03_13-43-03/models/run_lr5e04_bs512_h256_512_256_7e411fc0_20260103_134303_vec8_ep045000.pt"
+NUM_GAMES = 20
+OPPONENT_TYPE = "basic_strong"
+PAUSE_BETWEEN_GAMES = 1.5
+FRAME_DELAY = 0.05  # Delay in video playback (seconds per frame). Execution runs at full speed.
+MAX_STEPS = 250
+VIDEO_FPS = 50
+ACTION_FINENESS = None  # Set to None to auto-detect, or specify 3, 5, 7, etc.
+ENV_MODE = "NORMAL"  # "NORMAL" (250 steps), "TRAIN_SHOOTING" (80 steps), or "TRAIN_DEFENSE" (80 steps)
+
+def infer_fineness_from_action_dim(action_dim, keep_mode=True):
+    """
+    Infer the fineness parameter from the action dimension.
+    Returns None if it doesn't match a known fineness pattern.
+    """
+    # Try common fineness values: 3, 5, 7, 9, etc.
+    for fineness in [3, 5, 7, 9, 11, 13, 15]:
+        expected_dim = get_discrete_action_dim(fineness=fineness, keep_mode=keep_mode)
+        if expected_dim == action_dim:
+            return fineness
+    return None
+
+def load_ddqn_agent(model_path, state_dim, action_dim):
+    from rl_hockey.DDDQN import DDDQN
+    agent = DDDQN(
+        state_dim=state_dim,
+        action_dim=action_dim,
+        hidden_dim=[256, 256, 256]
+    )
+    logger.info(f"Loading model from: {model_path}")
+    agent.load(model_path)
+    logger.info("Model loaded successfully!")
+    return agent
+
+def create_blank_frames(frame_shape, duration_seconds, fps=50):
+    num_frames = int(duration_seconds * fps)
+    blank_frame = np.zeros(frame_shape, dtype=np.uint8)
+    # Use list comprehension with copy to avoid repeated allocations
+    return [blank_frame.copy() for _ in range(num_frames)]
+
+def apply_frame_delay(frames, frame_delay, fps=50):
+    """
+    Duplicate frames to create delay effect in video without slowing down execution.
+    If frame_delay > 0, each frame is duplicated to create the delay in playback.
+    """
+    if frame_delay <= 0:
+        return frames
+    
+    frames_per_step = int(frame_delay * fps)
+    delayed_frames = []
+    for frame in frames:
+        # Duplicate each frame to create the delay effect
+        delayed_frames.extend([frame] * frames_per_step)
+    
+    return delayed_frames
+
+def run_game(env, agent, opponent, game_num, max_steps=250, action_fineness=None):
+    """
+    Run a game at full speed (no delays during execution).
+    Frame delays are applied later when creating the video.
+    """
+    obs, info = env.reset()
+    obs_agent2 = env.obs_agent_two()
+    frames = []
+    step_count = 0
+    total_reward = 0
+    # Pre-compute action dimension for random opponent to avoid repeated calls
+    if opponent is None:
+        if action_fineness is not None:
+            # Use fineness-based action dimension
+            action_dim = 4 if env.keep_mode else 3
+        else:
+            action_dim = len(env.discrete_to_continous_action(0))
+    for step in range(max_steps):
+        frame = env.render(mode="rgb_array")
+        frames.append(frame)
+        # Convert observation to float32 for agent
+        obs_float = obs.astype(np.float32) if obs.dtype != np.float32 else obs
+        discrete_action = agent.act(obs_float, deterministic=True)
+        # Use fineness-based conversion if fineness is specified
+        if action_fineness is not None:
+            action_p1 = discrete_to_continuous_action_with_fineness(
+                discrete_action, 
+                fineness=action_fineness, 
+                keep_mode=env.keep_mode
+            )
+        else:
+            action_p1 = env.discrete_to_continous_action(discrete_action)
+        if opponent is not None:
+            action_p2 = opponent.act(obs_agent2)
+        else:
+            action_p2 = np.random.uniform(-1, 1, action_dim)
+        action = np.hstack([action_p1, action_p2])
+        obs, reward, done, truncated, info = env.step(action)
+        obs_agent2 = env.obs_agent_two()
+        total_reward += reward
+        step_count += 1
+        if done or truncated:
+            break
+    winner = info.get('winner', 0)
+    return frames, step_count, total_reward, winner, info
+
+def get_video_filename(base_folder="videos", base_name="ddqn_games"):
+    now = datetime.now()
+    dt_str = now.strftime("%Y-%m-%d_%H-%M-%S")
+    if not os.path.exists(base_folder):
+        os.makedirs(base_folder)
+    filename = f"{base_name}_{dt_str}.mp4"
+    return os.path.join(base_folder, filename)
+
+def main(model_path=MODEL_PATH, num_games=NUM_GAMES, opponent_type=OPPONENT_TYPE, pause_between_games=PAUSE_BETWEEN_GAMES, frame_delay=FRAME_DELAY, max_steps=MAX_STEPS, video_fps=VIDEO_FPS, action_fineness=ACTION_FINENESS, env_mode=ENV_MODE):
+    output_file = get_video_filename()
+    logger.info("="*60)
+    logger.info("DDDQN Agent Video Recording")
+    logger.info("="*60)
+    logger.info(f"Model: {model_path}")
+    logger.info(f"Output: {output_file}")
+    logger.info(f"Games: {num_games}")
+    logger.info(f"Opponent: {opponent_type}")
+    logger.info(f"Environment mode: {env_mode}")
+    logger.info(f"Max steps per game: {max_steps}")
+    logger.info(f"Frame delay in video: {frame_delay}s per frame")
+    logger.info(f"Video FPS: {video_fps}")
+    logger.info("="*60)
+    logger.info("Creating hockey environment...")
+    # Map string mode to enum
+    mode_map = {
+        "NORMAL": h_env.Mode.NORMAL,
+        "TRAIN_SHOOTING": h_env.Mode.TRAIN_SHOOTING,
+        "TRAIN_DEFENSE": h_env.Mode.TRAIN_DEFENSE
+    }
+    if env_mode not in mode_map:
+        raise ValueError(f"Invalid env_mode: {env_mode}. Must be one of {list(mode_map.keys())}")
+    env = h_env.HockeyEnv(mode=mode_map[env_mode])
+    # Log actual environment limit after creation
+    env_limit = env.max_timesteps if hasattr(env, 'max_timesteps') else 'unknown'
+    logger.info(f"Environment created: mode={env_mode}, max_timesteps={env_limit}")
+    if env_limit != 'unknown' and max_steps > env_limit:
+        logger.warning(f"Warning: max_steps ({max_steps}) exceeds environment limit ({env_limit}). Games will end at {env_limit} steps.")
+    state_dim = env.observation_space.shape[0]
+    
+    # Load checkpoint to get the actual action dimension from the model
+    import torch
+    checkpoint = torch.load(model_path, map_location='cpu')
+    actual_action_dim = checkpoint.get('action_dim', None)
+    
+    # Determine action dimension and fineness
+    if action_fineness is not None:
+        discrete_action_dim = get_discrete_action_dim(fineness=action_fineness, keep_mode=env.keep_mode)
+        logger.info(f"Using specified fineness: {action_fineness}")
+        if actual_action_dim is not None and discrete_action_dim != actual_action_dim:
+            logger.warning(f"Warning: Specified fineness {action_fineness} gives action_dim {discrete_action_dim}, but model has {actual_action_dim}")
+            discrete_action_dim = actual_action_dim  # Use model's action_dim
+    elif actual_action_dim is not None:
+        # Try to infer fineness from the model's action_dim
+        inferred_fineness = infer_fineness_from_action_dim(actual_action_dim, keep_mode=env.keep_mode)
+        if inferred_fineness is not None:
+            action_fineness = inferred_fineness
+            discrete_action_dim = actual_action_dim
+            logger.info(f"Auto-detected fineness: {action_fineness} from model action_dim: {actual_action_dim}")
+        else:
+            # Fall back to using model's action_dim but warn about fineness
+            discrete_action_dim = actual_action_dim
+            logger.warning(f"Could not infer fineness from action_dim {actual_action_dim}")
+            logger.warning("Assuming default fineness=3. If actions seem incorrect, specify action_fineness parameter")
+            action_fineness = None  # Will use env.discrete_to_continous_action
+    else:
+        # Fall back to default (fineness=3)
+        discrete_action_dim = 7 if not env.keep_mode else 8
+        logger.info(f"Using default action dimension (fineness=3): {discrete_action_dim}")
+        logger.info("If your model uses a different fineness, specify action_fineness parameter")
+        action_fineness = None  # Will use env.discrete_to_continous_action
+    
+    logger.info(f"State dimension: {state_dim}")
+    logger.info(f"Action dimension: {discrete_action_dim}")
+    if action_fineness is not None:
+        logger.info(f"Action fineness: {action_fineness}")
+    agent = load_ddqn_agent(model_path, state_dim, discrete_action_dim)
+    opponent = None
+    if opponent_type == "basic_weak":
+        opponent = h_env.BasicOpponent(weak=True)
+        logger.info("Using weak BasicOpponent")
+    elif opponent_type == "basic_strong":
+        opponent = h_env.BasicOpponent(weak=False)
+        logger.info("Using strong BasicOpponent")
+    elif opponent_type == "random":
+        opponent = None
+        logger.info("Using random actions for player 2")
+    else:
+        raise ValueError(f"Unknown opponent_type: {opponent_type}")
+    # Get frame shape without extra reset
+    obs_temp, _ = env.reset()
+    frame_temp = env.render(mode="rgb_array")
+    frame_shape = frame_temp.shape
+    logger.info(f"Frame shape: {frame_shape}")
+    logger.info(f"Running {num_games} games at full speed...")
+    all_frames = []
+    game_results = []
+    start_time = time.time()
+    for game_num in range(1, num_games + 1):
+        logger.info(f"Game {game_num}/{num_games}...")
+        frames, steps, reward, winner, info = run_game(env, agent, opponent, game_num, max_steps=max_steps, action_fineness=action_fineness)
+        all_frames.extend(frames)
+        game_results.append({
+            'game': game_num,
+            'steps': steps,
+            'reward': reward,
+            'winner': winner
+        })
+        winner_str = ""
+        if winner == 1:
+            winner_str = "Player 1 (Agent) wins!"
+        elif winner == -1:
+            winner_str = "Player 2 (Opponent) wins!"
+        else:
+            winner_str = "Draw"
+        logger.info(f"  Steps: {steps}, Reward: {reward:.2f}, {winner_str}")
+        if game_num < num_games:
+            logger.info(f"  Adding {pause_between_games}s pause (blank screen)...")
+            blank_frames = create_blank_frames(frame_shape, pause_between_games, fps=video_fps)
+            all_frames.extend(blank_frames)
+    execution_time = time.time() - start_time
+    logger.info(f"Games completed in {execution_time:.2f} seconds")
+    env.close()
+    logger.info("="*60)
+    logger.info("Game Summary")
+    logger.info("="*60)
+    for result in game_results:
+        winner_str = "Agent" if result['winner'] == 1 else ("Opponent" if result['winner'] == -1 else "Draw")
+        logger.info(f"Game {result['game']}: {result['steps']} steps, "
+                    f"Reward: {result['reward']:.2f}, Winner: {winner_str}")
+    wins = sum(1 for r in game_results if r['winner'] == 1)
+    losses = sum(1 for r in game_results if r['winner'] == -1)
+    draws = sum(1 for r in game_results if r['winner'] == 0)
+    logger.info(f"Overall: {wins} wins, {losses} losses, {draws} draws")
+    if all_frames:
+        try:
+            import imageio
+            # Apply frame delay by duplicating frames (creates delay in video without slowing execution)
+            logger.info(f"Applying frame delay of {frame_delay}s per frame...")
+            original_frame_count = len(all_frames)
+            all_frames = apply_frame_delay(all_frames, frame_delay, fps=video_fps)
+            logger.info(f"Expanded from {original_frame_count} to {len(all_frames)} frames for video")
+            
+            logger.info(f"Saving {len(all_frames)} frames as MP4 video...")
+            logger.info(f"Estimated video duration: {len(all_frames) / video_fps / 60:.1f} minutes")
+            logger.info("This may take 15-60 minutes depending on your CPU...")
+            encoding_start = time.time()
+            # Optimize video encoding for speed: use faster preset
+            # Note: imageio automatically handles pixel format, so we don't need to specify it
+            imageio.mimsave(
+                output_file, 
+                all_frames, 
+                fps=video_fps, 
+                codec='libx264', 
+                quality=8,
+                ffmpeg_params=['-preset', 'fast']
+            )
+            encoding_time = time.time() - encoding_start
+            logger.info(f"Video encoding completed in {encoding_time / 60:.1f} minutes")
+            file_size = os.path.getsize(output_file) / (1024*1024)
+            logger.info(f"Video saved to '{output_file}'")
+            logger.info(f"File size: {file_size:.2f} MB")
+        except ImportError:
+            logger.error("="*60)
+            logger.error("ERROR: imageio not installed!")
+            logger.error("="*60)
+            logger.error("Please install imageio to save videos:")
+            logger.error("  pip install imageio imageio-ffmpeg")
+            logger.warning("Saving frames as numpy array instead...")
+            np.savez(output_file.replace('.mp4', '.npz'), frames=np.array(all_frames))
+            logger.info(f"Frames saved to '{output_file.replace('.mp4', '.npz')}'")
+        except Exception as e:
+            logger.error(f"Error saving video: {e}")
+            logger.warning("Saving frames as numpy array as backup...")
+            np.savez(output_file.replace('.mp4', '.npz'), frames=np.array(all_frames))
+            logger.info(f"Frames saved to '{output_file.replace('.mp4', '.npz')}'")
+    else:
+        logger.warning("No frames collected - video not saved.")
+    logger.info("Done!")
+
+if __name__ == "__main__":
+    main()
diff --git a/src/rl_hockey/scripts/train_gym_td3.ipynb b/src/rl_hockey/scripts/train_gym_td3.ipynb
deleted file mode 100644
index 85710aa..0000000
--- a/src/rl_hockey/scripts/train_gym_td3.ipynb
+++ /dev/null
@@ -1,368 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "id": "7ef02867",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import gymnasium as gym\n",
-    "from tqdm import tqdm\n",
-    "import matplotlib.pyplot as plt\n",
-    "\n",
-    "from rl_hockey.td3 import TD3"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "id": "a4d6b2bc",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "env_name = 'Pendulum-v1'\n",
-    "# env_name = 'LunarLanderContinuous-v3'"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "id": "ad3e16d9",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/Users/nselcheung/opt/anaconda3/envs/rlhockey/lib/python3.11/site-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
-      "  gym.logger.warn(\n",
-      "/Users/nselcheung/opt/anaconda3/envs/rlhockey/lib/python3.11/site-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
-      "  gym.logger.warn(\n"
-     ]
-    }
-   ],
-   "source": [
-    "env = gym.make(env_name)\n",
-    "env = gym.wrappers.RescaleAction(env, min_action=-1.0, max_action=1.0)\n",
-    "\n",
-    "o_space = env.observation_space\n",
-    "ac_space = env.action_space"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 15,
-   "id": "4c848d4f",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "max_episodes = 500\n",
-    "max_episode_steps = 500\n",
-    "updates_per_step = 1\n",
-    "warmup=10000"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 16,
-   "id": "e80ee6f6",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "new_config = {\n",
-    "    \"actor_lr\": 3e-4,\n",
-    "    \"critic_lr\": 3e-4,\n",
-    "    \"batch_size\": 256,\n",
-    "}"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 17,
-   "id": "49c3fb39",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "agent = TD3(o_space.shape[0], action_dim=ac_space.shape[0], **new_config)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 18,
-   "id": "f2a02ac4",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "critic_losses = []\n",
-    "actor_losses = []\n",
-    "rewards = []\n",
-    "gradient_steps = 0\n",
-    "steps = 0"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 19,
-   "id": "985d63b7",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "500"
-      ]
-     },
-     "execution_count": 19,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "max_episode_steps"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 20,
-   "id": "719d9333",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "Pendulum-v1: 100%|██████████| 500/500 [05:15<00:00,  1.58it/s, total_reward=-3.73]   \n"
-     ]
-    }
-   ],
-   "source": [
-    "pbar = tqdm(range(max_episodes), desc=env_name)\n",
-    "for i in pbar:    \n",
-    "    total_reward = 0\n",
-    "    state, _ = env.reset()\n",
-    "\n",
-    "    agent.on_episode_start(i)\n",
-    "\n",
-    "    for t in range(max_episode_steps):\n",
-    "        done = False\n",
-    "        if steps < warmup:\n",
-    "            action = env.action_space.sample()\n",
-    "        else:\n",
-    "            action = agent.act(state)\n",
-    "            \n",
-    "        (next_state, reward, done, trunc, _) = env.step(action)\n",
-    "        agent.store_transition((state, action, reward, next_state, done))            \n",
-    "        state = next_state\n",
-    "\n",
-    "        steps += 1\n",
-    "\n",
-    "        if steps >= warmup:\n",
-    "            stats = agent.train(updates_per_step)\n",
-    "            total_reward += reward\n",
-    "            gradient_steps += updates_per_step\n",
-    "            critic_losses.extend(stats['critic_loss'])\n",
-    "            actor_losses.extend(stats['actor_loss'])\n",
-    "\n",
-    "        if done or trunc:\n",
-    "            break\n",
-    "\n",
-    "    agent.on_episode_end(i)\n",
-    "\n",
-    "    rewards.append(total_reward)    \n",
-    "    \n",
-    "    pbar.set_postfix({\n",
-    "        'total_reward': total_reward\n",
-    "    })\n",
-    "\n",
-    "agent.save(f'../../../models/td3/{env_name}_{gradient_steps//1000}k.pt')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 21,
-   "id": "82fe945f",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def moving_average(data, window_size):\n",
-    "    return [sum(data[max(0, i - window_size + 1):i + 1]) / (min(i + 1, window_size)) for i in range(len(data))]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 22,
-   "id": "f1e393d6",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "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",
-      "text/plain": [
-       "<Figure size 640x480 with 1 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "plt.plot(moving_average(rewards, 10))\n",
-    "plt.xlabel('Episodes')\n",
-    "plt.ylabel('Total Reward')\n",
-    "plt.title('Total Reward per Episode')\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 23,
-   "id": "c04e9765",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "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",
-      "text/plain": [
-       "<Figure size 640x480 with 1 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "plt.plot(moving_average(critic_losses, 100))\n",
-    "plt.xlabel('Training Steps')\n",
-    "plt.ylabel('Critic Loss')\n",
-    "plt.title('Critic Loss over Time')\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 24,
-   "id": "d0b7533a",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "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",
-      "text/plain": [
-       "<Figure size 640x480 with 1 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "plt.plot(moving_average(actor_losses, 100))\n",
-    "plt.xlabel('Training Steps')\n",
-    "plt.ylabel('Actor Loss')\n",
-    "plt.title('Actor Loss over Time')\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 25,
-   "id": "8415b0f0",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/Users/nselcheung/opt/anaconda3/envs/rlhockey/lib/python3.11/site-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
-      "  gym.logger.warn(\n",
-      "/Users/nselcheung/opt/anaconda3/envs/rlhockey/lib/python3.11/site-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
-      "  gym.logger.warn(\n"
-     ]
-    }
-   ],
-   "source": [
-    "env = gym.make(env_name, render_mode='human')\n",
-    "env = gym.wrappers.RescaleAction(env, min_action=-1.0, max_action=1.0)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 26,
-   "id": "5c19f7d6",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/Users/nselcheung/opt/anaconda3/envs/rlhockey/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
-      "  from pkg_resources import resource_stream, resource_exists\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "total_reward: -0.2538817475856251\n"
-     ]
-    }
-   ],
-   "source": [
-    "total_reward = 0\n",
-    "state, _ = env.reset()\n",
-    "for t in range(max_episode_steps):\n",
-    "    done = False\n",
-    "    action = agent.act(state)\n",
-    "    (next_state, reward, done, trunc, _) = env.step(action)\n",
-    "    state = next_state\n",
-    "\n",
-    "    total_reward += reward\n",
-    "\n",
-    "    if done or trunc:\n",
-    "        break\n",
-    "\n",
-    "print(f'total_reward: {total_reward}')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 27,
-   "id": "d1ea8f32",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "env.close()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "39439780",
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "rlhockey",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.11.14"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/src/rl_hockey/scripts/train_hockey_td3.ipynb b/src/rl_hockey/scripts/train_hockey_td3.ipynb
deleted file mode 100644
index 5c1d306..0000000
--- a/src/rl_hockey/scripts/train_hockey_td3.ipynb
+++ /dev/null
@@ -1,644 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "id": "7ef02867",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import numpy as np\n",
-    "from tqdm import tqdm\n",
-    "import matplotlib.pyplot as plt\n",
-    "import hockey.hockey_env as h_env\n",
-    "\n",
-    "from rl_hockey.td3 import TD3\n",
-    "from rl_hockey.common import utils"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "id": "eb062da2",
-   "metadata": {},
-   "source": [
-    "# Train easy and hard bot"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "id": "a4bea507",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)\n",
-    "\n",
-    "state_dim = env.observation_space.shape[0]\n",
-    "action_dim = env.action_space.shape[0] // 2\n",
-    "max_action = float(env.action_space.high.min())"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "id": "a2a06a41",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "max_episodes = 500\n",
-    "updates_per_step = 1\n",
-    "warmup_steps = 10\n",
-    "max_episode_steps = 500"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "id": "e8893b51",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "new_config = {\n",
-    "    \"critic_lr\": 3e-4,\n",
-    "    \"actor_lr\": 3e-4,\n",
-    "    \"critic_dim\": 256,\n",
-    "    \"actor_dim\": 256,\n",
-    "    \"actor_n_layers\": 1,\n",
-    "    \"critic_n_layers\": 1,\n",
-    "    \"batch_size\": 256,\n",
-    "    \"discount\": 0.999,\n",
-    "    \"action_min\": float(-max_action),\n",
-    "    \"action_max\": float(max_action),\n",
-    "    \"policy_update_delay\": 2,\n",
-    "    \"tau\": 0.005,\n",
-    "    \"noise_type\": \"normal\",\n",
-    "    \"exploration_noise\": 0.15,\n",
-    "    \"policy_noise\": 0.1,\n",
-    "    \"noise_clip\": 0.5,\n",
-    "    \"target_network_update_steps\": 2,\n",
-    "    \"verbose\": True,\n",
-    "    \"prioritized_replay\": True,\n",
-    "}"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "id": "5ef8a690",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Initialized Actor:\n",
-      "Actor(\n",
-      "  (net): Sequential(\n",
-      "    (0): Linear(in_features=18, out_features=256, bias=True)\n",
-      "    (1): ReLU()\n",
-      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
-      "    (3): ReLU()\n",
-      "    (4): Linear(in_features=256, out_features=4, bias=True)\n",
-      "    (5): Tanh()\n",
-      "  )\n",
-      ")\n",
-      "Initialized Critics:\n",
-      "Sequential(\n",
-      "  (0): Linear(in_features=22, out_features=256, bias=True)\n",
-      "  (1): ReLU()\n",
-      "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
-      "  (3): ReLU()\n",
-      "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
-      ")\n"
-     ]
-    }
-   ],
-   "source": [
-    "# agent = SAC(o_space.shape[0], action_dim=ac_space.shape[0], noise='pink', max_episode_steps=max_episode_steps)\n",
-    "agent = TD3(state_dim, action_dim=action_dim, **new_config)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "id": "726d619d",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "critic_losses = []\n",
-    "actor_losses = []\n",
-    "rewards = []\n",
-    "steps = 0\n",
-    "gradient_steps = 0\n",
-    "game_outcomes = []"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 7,
-   "id": "279e071f",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "strong_opponent = h_env.BasicOpponent(weak=False)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "id": "ac7cb467",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def evaluate_policy(agent, easy=True, num_eval_rounds=10):\n",
-    "    eval_env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)\n",
-    "    if easy:\n",
-    "        opponent = h_env.BasicOpponent(weak=True)\n",
-    "    else:\n",
-    "        opponent = h_env.BasicOpponent(weak=False)\n",
-    "\n",
-    "    mean_reward = 0.0\n",
-    "    winners = []\n",
-    "    print(\"-\" * 20, \"Evaluation\", \"-\" * 20)\n",
-    "\n",
-    "    for _ in range(num_eval_rounds):\n",
-    "        (state, _), done = eval_env.reset(), False\n",
-    "        state2 = eval_env.obs_agent_two()\n",
-    "        while not done:\n",
-    "            action = agent.act(state, deterministic=True)\n",
-    "            action2 = opponent.act(np.array(state2))\n",
-    "            state, reward, done, trunc, info = eval_env.step(np.hstack([action, action2]))\n",
-    "            state2 = eval_env.obs_agent_two()\n",
-    "            mean_reward += reward\n",
-    "        \n",
-    "        winners.append(info[\"winner\"])\n",
-    "\n",
-    "    mean_reward /= num_eval_rounds\n",
-    "    winrate = winners.count(1) / num_eval_rounds\n",
-    "    print(f\"Overall Scores, {num_eval_rounds} games:\")\n",
-    "    print(f\"Mean reward: {mean_reward:.3f}\")\n",
-    "    print(f\"Win rate: {winrate:.3f}\")\n",
-    "    eval_env.close()\n",
-    "\n",
-    "    return winrate, mean_reward"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "id": "793402e2",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "-------------------- Evaluation --------------------\n",
-      "Overall Scores, 10 games:\n",
-      "Mean reward: -26.323\n",
-      "Win rate: 0.000\n"
-     ]
-    },
-    {
-     "data": {
-      "text/plain": [
-       "(0.0, -26.322871484093213)"
-      ]
-     },
-     "execution_count": 9,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "evaluate_policy(agent, easy=True, num_eval_rounds=10)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 10,
-   "id": "f05e1fcc",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:   0%|          | 0/510 [00:00<?, ?it/s]"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  19%|█▉        | 99/510 [06:39<25:07,  3.67s/it, total_reward=-14.7, episode_length=61, winrate=0.000, eval_mean_reward=0.000] "
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "-------------------- Evaluation --------------------\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  20%|█▉        | 100/510 [06:46<31:22,  4.59s/it, total_reward=-30.2, episode_length=250, winrate=0.000, eval_mean_reward=-18.023]"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Overall Scores, 10 games:\n",
-      "Mean reward: -18.023\n",
-      "Win rate: 0.000\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  39%|███▉      | 199/510 [14:15<30:44,  5.93s/it, total_reward=-8.19, episode_length=250, winrate=0.000, eval_mean_reward=0.000]  "
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "-------------------- Evaluation --------------------\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  39%|███▉      | 200/510 [14:22<32:28,  6.29s/it, total_reward=-33.8, episode_length=250, winrate=0.100, eval_mean_reward=-22.223]"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Overall Scores, 10 games:\n",
-      "Mean reward: -22.223\n",
-      "Win rate: 0.100\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  59%|█████▊    | 299/510 [22:34<15:02,  4.28s/it, total_reward=-10.9, episode_length=26, winrate=0.000, eval_mean_reward=0.000]   "
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "-------------------- Evaluation --------------------\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  59%|█████▉    | 300/510 [22:37<13:06,  3.75s/it, total_reward=-17.5, episode_length=65, winrate=0.000, eval_mean_reward=-24.087]"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Overall Scores, 10 games:\n",
-      "Mean reward: -24.087\n",
-      "Win rate: 0.000\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  78%|███████▊  | 399/510 [33:36<11:13,  6.07s/it, total_reward=8.45, episode_length=69, winrate=0.000, eval_mean_reward=0.000]   "
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "-------------------- Evaluation --------------------\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  78%|███████▊  | 400/510 [33:48<14:24,  7.86s/it, total_reward=-52.5, episode_length=250, winrate=0.000, eval_mean_reward=-27.652]"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Overall Scores, 10 games:\n",
-      "Mean reward: -27.652\n",
-      "Win rate: 0.000\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "TRAIN BABY TRAIN:  85%|████████▌ | 436/510 [39:09<06:38,  5.39s/it, total_reward=-42, episode_length=250, winrate=0.000, eval_mean_reward=0.000]    \n"
-     ]
-    },
-    {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m warmup_steps:  \u001b[38;5;66;03m# mirroring enables 2 transitions per step\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates_per_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     gradient_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m updates_per_step\n\u001b[1;32m     41\u001b[0m     critic_losses\u001b[38;5;241m.\u001b[39mextend(stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
-      "File \u001b[0;32m/mnt/beegfs/home/stud389/RL_CheungMaenzerAbraham_Hockey/src/rl_hockey/td3/td3.py:267\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    264\u001b[0m     critic_loss \u001b[38;5;241m=\u001b[39m c1_loss \u001b[38;5;241m+\u001b[39m c2_loss\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 267\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Clip the gradients\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m#     self.twincritic_online.parameters(), max_norm=0.5\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
-      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-     ]
-    }
-   ],
-   "source": [
-    "pbar = tqdm(range(max_episodes + warmup_steps), desc=\"TRAIN BABY TRAIN\")\n",
-    "for i in pbar:    \n",
-    "    total_reward = 0\n",
-    "    state, info = env.reset()\n",
-    "    player2_state = env.obs_agent_two()\n",
-    "    done = False\n",
-    "    winrate = 0\n",
-    "    evaluation_mean_reward = 0\n",
-    "\n",
-    "    agent.on_episode_start(i)\n",
-    "\n",
-    "    for t in range(max_episode_steps):\n",
-    "        done = False\n",
-    "        if steps < warmup_steps:\n",
-    "            # Use Strong Opponent to generate transitions\n",
-    "\n",
-    "            # Player 1 (warm up player, the one we are trying to learn from, aka the agent state)\n",
-    "            action = strong_opponent.act(np.array(state))\n",
-    "\n",
-    "            # Player 2, uses state 2\n",
-    "            action2 = strong_opponent.act(np.array(player2_state))\n",
-    "        else:\n",
-    "            action = agent.act(state) # Agent's action\n",
-    "\n",
-    "            # Player 2 action\n",
-    "            action2 = strong_opponent.act(np.array(player2_state))\n",
-    "        \n",
-    "        (next_state, reward, done, trunc, _) = env.step(np.hstack([action, action2]))\n",
-    "        agent.store_transition((state, action, reward, next_state, done))\n",
-    "        \n",
-    "        state = next_state\n",
-    "        player2_state = env.obs_agent_two()  # Update player 2 state\n",
-    "\n",
-    "        steps += 1\n",
-    "        total_reward += reward\n",
-    "\n",
-    "        if steps >= warmup_steps:  # mirroring enables 2 transitions per step\n",
-    "            stats = agent.train(updates_per_step)\n",
-    "\n",
-    "            gradient_steps += updates_per_step\n",
-    "            critic_losses.extend(stats['critic_loss'])\n",
-    "            actor_losses.extend(stats['actor_loss'])\n",
-    "\n",
-    "        if done or trunc:\n",
-    "            break\n",
-    "\n",
-    "    agent.on_episode_end(i)\n",
-    "\n",
-    "    rewards.append(total_reward)   \n",
-    "    if reward == 0:\n",
-    "        game_outcomes.append(\"draw\")\n",
-    "    elif reward == 1:\n",
-    "        game_outcomes.append(\"win\")\n",
-    "    else:\n",
-    "        game_outcomes.append(\"loss\") \n",
-    "\n",
-    "    if (i + 1) % 100 == 0 and i >= warmup_steps:\n",
-    "        winrate, evaluation_mean_reward = evaluate_policy(agent, easy=True, num_eval_rounds=10)\n",
-    "    \n",
-    "    pbar.set_postfix({\n",
-    "        'total_reward': total_reward,\n",
-    "        'episode_length': t,\n",
-    "        'winrate': f\"{winrate:.3f}\",\n",
-    "        'eval_mean_reward': f\"{evaluation_mean_reward:.3f}\"\n",
-    "    })\n",
-    "\n",
-    "# agent.save(f'../../../models/td3/{run_name}_{gradient_steps//1000}k.pt')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 12,
-   "id": "010dd386",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def moving_average(data, window_size):\n",
-    "    moving_averages = []\n",
-    "    for i in range(len(data)):\n",
-    "        window_start = max(0, i - window_size + 1)\n",
-    "        window = data[window_start:i + 1]\n",
-    "        moving_averages.append(sum(window) / len(window))\n",
-    "    \n",
-    "    return moving_averages"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 13,
-   "id": "1d968c3d",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "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",
-      "text/plain": [
-       "<Figure size 640x480 with 1 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "plt.plot(moving_average(rewards, 10))\n",
-    "plt.xlabel('Episodes')\n",
-    "plt.ylabel('Total Reward')\n",
-    "plt.title('Total Reward per Episode')\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 14,
-   "id": "d42287c6",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "np.float64(9.981851898769605)"
-      ]
-     },
-     "execution_count": 14,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "np.max(rewards)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 15,
-   "id": "ec7f2a3e",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWJpJREFUeJzt3Qd4FFXXB/CTngBJSIAUSIBAIPTeQu+hg4ogooACSlOKiiBFFBQEpagUG/IJKu2lKL33ANJ7CTWBEEoa6W2/59ywy26y6Ts7u7P/3/MMuzM7O3MzQ3ZP7j33XiuVSqUiAAAAAIWwlrsAAAAAAIaE4AYAAAAUBcENAAAAKAqCGwAAAFAUBDcAAACgKAhuAAAAQFEQ3AAAAICiILgBAAAARUFwAwAAAIqC4AZAAaysrGjGjBn52rdixYo0ZMgQycsE0sE9BMgdghsAmd26dYvef/99qlSpEjk6OpKLiwu1aNGCFi1aRImJiYU65rFjx0SwEx0dTVJ/yfbo0UPSc1iCAwcOiAA1PwsA5M02H/sAgES2bt1Kr7/+Ojk4ONCgQYOoVq1alJKSQkeOHKFPPvmELl++TD///HOex+EgyNbWVie4+eKLL0QNTcmSJXX2vX79Ollb4+8aU1K9enVauXKlzrbJkydTiRIlaMqUKdn2xz0EyB2CGwCZ3Llzh9544w2qUKEC7du3j7y9vTWvjR49mkJCQkTwk5OMjAwRCHFtDy/5xYEUyIPnKU5KSiInJyed7Z6envTWW2/pbJszZw6VLl0623aGewiQO/z5BiCTuXPnUlxcHP322286gY2av78/jR07VrPOTRJjxoyhP//8k2rWrCm+4Hbs2JEt54YfudaH+fn5aZoz7t69m2O+BjdfjR8/XrzGx/Xx8RE1SU+fPi3yz5mWlkYzZ86kypUri2PzOT777DNKTk7W2e/UqVMUFBQkvtD5y5/L/u677+rss3r1amrYsCE5OzuL5rvatWuL5ru8xMfH00cffUS+vr6iDAEBAfTtt9+KYEONa83atWunN4gsV64c9e3bV2fbwoULxX3gwJKDE25ajIqK0ttst3PnTmrUqJH4uX766Scqqqz3cMWKFeIec43fhx9+SGXKlBE1dlwmDoD5/vL9dHNzE8vEiRN1fvaC/EwA5gA1NwAy+ffff0WeTfPmzfP9Hq7hWbt2rQhyOAjgL7msXn31Vbpx4wb9/ffftGDBArEf4y88fTjAatWqFV29elUEEw0aNBBBzT///ENhYWGa9xfWsGHD6P/+7/9EcMABxokTJ2j27NnifBs3bhT7PH78mDp37izKOGnSJPHFzMHYhg0bNMfZvXs3DRgwgDp06EDffPON2MbHOHr0qE4QmBV/iffq1Yv2799PQ4cOpXr16olggwPABw8eiGvE+vfvLwLDR48ekZeXl+b9HDA8fPhQ1LKp8Zc+BxTvvPOOCCa4Fu7HH3+ks2fPivLY2dnpNCFxufk9w4cPF4GVVD744ANRdm6SPH78uGjS5GvJzZTly5enr7/+mrZt20bz5s0TwRwHPIX5mQBMngoAjC4mJob/bFb17t073+/h/a2trVWXL1/W+9rnn3+uWZ83b57YdufOnWz7VqhQQTV48GDN+vTp08W+GzZsyLZvRkZGrmXiY3Xv3j3H18+dOyeOPWzYMJ3tH3/8sdi+b98+sb5x40ax/t9//+V4rLFjx6pcXFxUaWlpqoLYtGmTOPasWbN0tvft21dlZWWlCgkJEevXr18X+/3www86+40aNUpVokQJVUJCglg/fPiw2O/PP//U2W/Hjh3ZtvP14W38WkHVrFlT1aZNG72vZb2Hv//+uzhPUFCQzj0LDAwUP+OIESM02/j6+fj46By7ID8TgDlAsxSADGJjY8UjN68URJs2bahGjRoGLcv//vc/qlu3Lr3yyivZXitq7xyuJWATJkzQ2c41OEydU6ROet6yZQulpqbqPRbvw81LXINT0DLY2NiI2oisZeC4cPv27WK9atWqolZnzZo1mn3S09Np/fr11LNnT02ezLp168jV1ZU6deokarjUCzeXcQIw1xBp4+Y1bm4zBq6Z0r5nTZs2FT8jb1fja8FNZLdv39ZsK+jPBGDqENwAyIDzRdjz588L9D7+opSiKzo3UUjh3r17omcW5w9p46YTDlb4dXXQ9tprr4nmFG4G6927N/3+++86eTmjRo0SAUjXrl1FThA3oalzjvIqQ9myZbMFktxDSf26GjdNcRMMN1epu2hzkxlvV7t58ybFxMSQh4eHaEbTXriJj/eX+p7lhJuetHHAwjjXKOt27Vyagv5MAKYOOTcAMgU3/IV76dKlAr0vay8bc5FXDRC/zjUknCfCuUicE8PBy3fffSe2ce0Bf/GeO3dOvMa1LbxwAMR5I5zTYwgcxHAXbK7JGDdunMhv4kCgS5cuOom3XBZO7NYna26TMe8Z18rkd7t2QnFBfyYAU4fgBkAm3IuGEz6Dg4MpMDDQoMcuSHMS92IqaJCVX9zNnb84uWZAXVPCIiIiRA8efl1bs2bNxPLVV1/RX3/9RQMHDhQ9pDgpmdnb24smIl74uFybw72Ppk2blq12SLsMe/bsEbVk2rU3165d07yuXcvSpEkT0TTFSduc0NynTx+drtd8vfh4PNCiuQabWSnxZwLLhmYpAJlwd9zixYuLL27+stfXXJSfbs768HFZfkYo5uag8+fPa3ouacvaXbigunXrJh65i7G2+fPni8fu3buLR24iyXouzn9h6qapZ8+e6bzOzV116tTR2SenMnDuDPf80ca9pDgI5GaurLU3XFu0fPlykXei3STF+vXrJ47H3dv1dXuXelRoKSjxZwLLhpobABn/WubaCf7y5FoN7RGKuesuN40Udg4oTgRlPLotd2Hmbrxc26EOerRxl2huEuKRkrkpiN8bGRkpuoIvW7ZMJBvnhgcbnDVrVrbt9evXF8HL4MGDRQ0Vf0Fybs3JkydFMxLXiKjHleH1JUuWiKRmvi5cy/LLL7+I5jt1gMRBIJerffv2IueGc2V++OEHEQRp1wplxT83n4evBXcv559n165dtHnzZtH0xOfL+kX/8ccfi8Xd3Z06duyo8zr/DNxtmruzczMZd2Hn68u1U3zPOCDVHhPHHCjxZwILJ3d3LQBLd+PGDdXw4cNVFStWVNnb26ucnZ1VLVq0EF2Sk5KSNPvxr+vo0aP1HiNrV3A2c+ZMVbly5UT3ce1u4Vm7EbNnz56pxowZI/bnMnBXYd7n6dOnuZZd3dVZ3zJ06FCxT2pqquqLL75Q+fn5qezs7FS+vr6qyZMn6/xsZ86cUQ0YMEBVvnx5lYODg8rDw0PVo0cP1alTpzT7rF+/XtW5c2fxGpeR933//fdV4eHheV7j58+fq8aPH68qW7asKEOVKlVEd/mcurrz9dfXhV3bzz//rGrYsKHKyclJ3LPatWurJk6cqHr48GG+u8obuit41q70/H+Ctz958kRnO7+3ePHihfqZAMyBFf8jd4AFAAAAYCjIuQEAAABFQXADAAAAioLgBgAAABQFwQ0AAAAoCoIbAAAAUBQENwAAAKAoFjeIHw/Z/vDhQzEMe1FnPAYAAADj4JFreIBPnpePRyjPjcUFNxzYZJ0hFwAAAMxDaGioGKU8NxYX3KgnzuOLw0O7AwAAgOmLjY0VlRPaE+DmxOKCG3VTFAc2CG4AAADMS35SSpBQDAAAAIqC4AYAAAAUBcENAAAAKAqCGwAAAFAUBDcAAACgKAhuAAAAQFEQ3AAAAICiILgBAAAARUFwAwAAAIqC4AYAAAAUBcENAAAAKAqCGwAAAFAUBDcAAABgMIkp6SQ3BDcAAABgEFM3XaTq03fQpQcxJCcENwAAAGAQq47fF4/f771JckJwAwAAAAZlZUWyQnADAAAAioLgBgAAABQFwQ0AAAAoCoIbAAAAMCgrkjfpBsENAAAAKIqt3AUAAAAA87bkQAhZy91FSguCGwAAACi0qPgUmrvjus42ueMcNEsBAABAoSWlyT/dQlYIbgAAAKDQ0jNUZGoQ3AAAAEC+xSal6qxnZJDJQXADAAAA+bLhTBjVmbGLftz3cu6odFX2mhvk3AAAAIBZ+GT9BfH47a4bmm0ZeoIbPZuMCjU3AAAAUGgZyLkBAAAAJYmMT8m2DTU3AAAAYLbSUHMDAAAASg9uVCRv0g1ybgAAAKBQVCoV7bgUTqYG0y8AAABAgQfsaztvP919lqB3P+TcAAAAgNm5m0Ngw+ROw0GzFAAAACgKghsAAAAwMCQUAwAAABgMam4AAAAgX+xt8hc27Ln6mOSE4AYAAADyRe7xa/ILwQ0AAADki9xdvPMLwQ0AAACY7VQL+iC4AQAAAEVBcAMAAACKImtwM3v2bGrcuDE5OzuTh4cH9enTh65fv57re1asWEFWVlY6i6Ojo9HKDAAAAKZN1uDm4MGDNHr0aDp+/Djt3r2bUlNTqXPnzhQfH5/r+1xcXCg8PFyz3Lt3z2hlBgAAANMm68SZO3bsyFYrwzU4p0+fptatW+f4Pq6t8fLyMkIJAQAAwNyYVM5NTEyMeHR3d891v7i4OKpQoQL5+vpS79696fLlyznum5ycTLGxsToLAAAA5C06IYUmb7hIp+9FkTkxmeAmIyODxo0bRy1atKBatWrluF9AQAAtX76cNm/eTKtWrRLva968OYWFheWY1+Pq6qpZOCACAACAvH255Qr9ffI+vbb0GBVECQdZG4ZMJ7jh3JtLly7R6tWrc90vMDCQBg0aRPXq1aM2bdrQhg0bqEyZMvTTTz/p3X/y5MmiRki9hIaGSvQTAAAAKMvtJ7nnwOYkLjmN5CRvaPXCmDFjaMuWLXTo0CHy8fEp0Hvt7Oyofv36FBISovd1BwcHsQAAAEDBWFmRWZK15kalUonAZuPGjbRv3z7y8/Mr8DHS09Pp4sWL5O3tLUkZAQAAwLzYyt0U9ddff4n8GR7r5tGjR2I758Y4OTmJ59wEVa5cOZE7w7788ktq1qwZ+fv7U3R0NM2bN090BR82bJicPwoAAIDiWGk9f31ZwfJuLDa4Wbp0qXhs27atzvbff/+dhgwZIp7fv3+frK1fVjBFRUXR8OHDRSDk5uZGDRs2pGPHjlGNGjWMXHoAAADL8d9d8+kxZSt3s1ReDhw4oLO+YMECsQAAAACYdG8pAAAAMC1n7keTOUJwAwAAABoJKWmiZeXwzSdkrkyiKzgAAAAY152n8bQy+B6936YSebpkTkB9MSyGev54hHrXK0ubzz0021uC4AYAAMACvbLkKEUnpNKFsGhaP7K52LZ4f+aYceYc2DA0SwEAAFig6IRU8Xgu1DzzanKD4AYAAADMekTirBDcAAAAgGCtkOgGwQ0AAIAFs9KOZ5QR2yC4AQAAgEwZGXkPrmsOUHMDAAAAwvZLmXM8mjsENwAAAKAoCG4AAAAsmJVSEm20ILgBAACwYBn5mMTa3CC4AQAAsGBpCkki1obgBgAAABQFwQ0AAICFC771jDrOP0hKgYkzAQAALNyAX46TkqDmBgAAwEJcDY+lx8+TSOlQcwMAAKBw6Rkq2nAmjD5Zf0Gsh3zVlZQMwQ0AAIDCfbfrOi05cEuzvuLYXVIyNEsBAAAo3M+Hbuus774SQUqG4AYAAMDC2Fgrb1RibQhuAAAAFEylUmUbqO/YrWeSnrOMswPJCcENAACAgp0PizH6Oef1rUNyQnADAACgYEmp6UY/Z+sqZUhOCG4AAAAULDQywajnG9+xKlnLnNODruAAAAAKlJqeQSduR9Lj58lGPa+VCeQqI7gBAABQoG93XqefsnQBN4Z6viVJbghuAAAAFOiP4HtG717evpoHta4qb74NQ3ADAACgQBkq3e7fUpsYFEDvt6lMpgAJxQAAAAqkMvL5XJzsyFQguAEAAFDAQH2ztlyhTWcf6EyWaUw96niTqUCzFAAAgJlJScugmMRUzUjAe64+pl+P3BHP+9QvJ0twY20K3aReQM0NAACAmen2/WFq/NUeuv0kTqz/dzfS4prBcoPgBgAAwMyEPM4ManZcfiQe/cuUyNZMZWzGrinKDYIbAAAAM5XxIqDI2iKkkiHOQHADAAAABZa1RkZdWaKd73Ij4jmlZmQY/eqmyXDOnCChGAAAwAxwT6hxa87RkOYVNdsuP4yhkatOU2WtZqnOCw5RWVdHo5fPhGIbBDcAAADmgAMbtuLYXc22nZcj9O77MCaJjM0V49wAAACAEhS3t6FDn7QjJ3sbMhVIKAYAAIBC83J1pPKlipEpQXADAAAAhSZHz6y8ILgxoGuPYqnP4qN071m8IQ8LAAAWbtVx487wnZs9E9pQCYeX/ZHebelHpga9pQyoy8LD4rHNvAN0d053Qx4aAAAs2NRNl8hUeLo40LnpncSIxHeexlMVD90BBE0BghsAAADINysrK7K1yWz4qerpTKYIzVIAAACQq+GtXjY9mc70mDlDcAMAAAC58nJ1MsnZv3OC4AYAAADyPe2DGcQ2CG4AAADA/Lp75wY1NwAAAJCjf8a0oJLF7Myq5ga9pQAAAEAvF0dbquNTkpwdtYIbM0gpRs2NBOxsTP/GAwAA5KVmWddsOTfWZvAVh+BGAtojNwIAABRGeoaK9l6NoKdxybJfwAyV7jg3pg7fwhIws7wrAAAwQXO2X6VfDt8h06DSPDP90AY1N5KISUyV5sAAAGARzt6Pki2wWfRGvVx7S5lBxY28zVKzZ8+mxo0bk7OzM3l4eFCfPn3o+vXreb5v3bp1VK1aNXJ0dKTatWvTtm3byJSYW5c5AAAwLRPWnpft3L7uxTTPVS9qbHQSis0gupE1uDl48CCNHj2ajh8/Trt376bU1FTq3LkzxcfnPKv2sWPHaMCAATR06FA6e/asCIh4uXTJdCYVAwAAKIhDN57Qd7uuizwbejEhpVzuPInP9se6l6sjzXm1Nv0woD6ZA1lzbnbs2KGzvmLFClGDc/r0aWrdurXe9yxatIi6dOlCn3zyiVifOXOmCIx+/PFHWrZsmVHKDQAAYEiDlp8Uj36li9OrDXxkvbg2OXSHeqNJeTIXJtVbKiYmRjy6u7vnuE9wcDB17NhRZ1tQUJDYbiqc7GzkLgIAAJih8JgkuYtA1lrBjblmWZhMb6mMjAwaN24ctWjRgmrVqpXjfo8ePSJPT0+dbbzO2/VJTk4Wi1psbCxJTd1GCQAAUBCmMCmlrXbNjZl+nZlMzQ3n3nDezOrVqw2etOzq6qpZfH19yZjjAQAAAJhTb1sbnZob8/xCM4ngZsyYMbRlyxbav38/+fjk3tbo5eVFEREROtt4nbfrM3nyZNHcpV5CQ0NJcub5fwEAAGS27OAtk6q5UZnp95mswQ0P58yBzcaNG2nfvn3k5+eX53sCAwNp7969Ots4oZi36+Pg4EAuLi46i9TMNdIFAABpXX/0nPr/FEwnbj8z2UttrYCcG2u5m6JWrVpFf/31lxjrhvNmeElMTNTsM2jQIFH7ojZ27FjRy+q7776ja9eu0YwZM+jUqVMiSDIVaJYCAAB9hv3xH524E0n9fz5ushfIwcaa6vhkzinVt6G8PbfMMqF46dKl4rFt27Y623///XcaMmSIeH7//n2ytn4ZgzVv3lwEQ1OnTqXPPvuMqlSpQps2bco1CdnYMsy1Hg8AACQVEZP3PFEfyTiAH7OztabV7zWja4+eU33fkmSOZA1utGcZzcmBAweybXv99dfFYqoQ2wAAQGH970yYUS7eW83K06rj9zXrXFtzPzKBapdzJUc7G2pQ3o3Mlcl0BQcAAADjqFS6OM3qU1snuNk0qgWlq1RkZ2MSfY2KxPx/AgAAAHMh/zA2wu0X0zusHNpEPC5+s4FIJFZCYMNQcwMAAGBh3Ivbi8dWVcrQ3TndSWmUEaIBAACYmXvP4kXuaX7yTw3Nw9mBlAw1NwAAADJoM+8A+bg5UWJKumTnKG5vQz+93Yje+u0EWRIENwAAADKl3IRFvRzXTQoli9lTyyqlydKgWQoAAMDAoxCHPI4ziWtqZSIJzMaGmhsAAAADeZ6USkELD4nnIV91JVuZex9ZWWhwg5obAAAAA3nyPNmkpuKxetEQ5le6uEUNNovgBgAAQILpd2y0JqA0tpm9a5KjnTXN71dXrO8e35ouzOhMlgLNUgAAAAaSnvHyub7YxljNRG8HVqQ3m1bQBFjcPOaikAH68sNyflIAAACJpWu1Ra06cZ8yZGybstETXb3XupJ4nNytGikZam4AAAAkCG6mbbokxpl5tYFPthwYuXzWrTp92KEKlXBQ9tc/am4AAAAMhCee1HblYazJXdsSCg9sGIIbAAAACWpu1FYG36UuCw/R49gkXGcjUX74BgAAYCR3Xsy2rW3a5svi8btdNygxVbqpFuAl1NwAAAAYyMfrzuf4Wop2VyqQFIIbAAAAiWjPHWWhgwXLAsENAACARHZcfvRyBdGN0SC4AQAAKKQLYdH0ypKjdOpuZJ77bjjzwKDXuUH5kgY9npIguAEAACikAT8fp7P3o6nvsmCjX8NlbzWkdgFlaPmQRkY/t6lDcAMAAFBI8Snp+UomloKHiyP9/k4Tal/Nkz7qVNWo5zZ1CG4AAAAMYP3pMKNdxyYV3XXWP+hQhSqV0Z3525IhuAEAADAzveuXlbsIJg3BDQAAgJmx1je9uHxzdJocBDeGvJjo5gcAAEag7+tmZp9a4nFshyoWfw8w/YIh/7NxJJ1l0jQAAICiKmZvQwlaycv6tPAvTddmdiFHOxuLv+BFrrmJjY2lTZs20dWrVy3+YgIAgGV4FpdMYVEJRjtfuwCPfO2HwKaQNTf9+vWj1q1b05gxYygxMZEaNWpEd+/eJZVKRatXr6bXXnuNLBVapQAAlC3kcRz5uDlRw1l7jHbOHnW8kU4jdc3NoUOHqFWrVuL5xo0bRVATHR1N33//Pc2aNYssmb78LgAAUIbDN59Qx/kHqc4Xu4x63hFtKmfbhu8bAwc3MTEx5O6e2b9+x44doqamWLFi1L17d7p58yZZMivU3QAAKNbbv50Ujylpxpvd+/IXQVSrnGu2b5fT96KMVgaLCG58fX0pODiY4uPjRXDTuXNnsT0qKoocHR3Jomn97+MaLQAAME/pGSqauP48/X3yvqy1JcUd9GeP3HtmvHwfiwhuxo0bRwMHDiQfHx8qW7YstW3bVtNcVbt2bbJk2v/vEdsAAJivXZcf0dpTYTR5w0VKSEmj3VciqEcd0xk4b27fOnIXQVkJxaNGjaImTZpQaGgoderUiaytM+OjSpUqIedGK7rJUKnIGs1UAABm6ebjOM3zsavPieDGVJya2pFKl3CQuxjKG+eGe0jxwtLT0+nixYvUvHlzcnNzI0umnXOTgVYpAACTb3pKTc/Q231649kHmuemENiIcdReQGAjUbPUb7/9pgls2rRpQw0aNBC5OAcOHCBLpl1zo0LHPQAAk9Z10SGqNm0HxSalyjrivL2NNR34ODPFA2QKbtavX09169YVz//991+6c+cOXbt2jcaPH09TpkwhS4acGwAA83EjIrPp6dTdyGyv2RghuuEu3u2redDO8a2pYmnM6C1rcPP06VPy8vISz7dt20avv/46Va1ald59913RPGXJtKsNkVAMAKCwiSkN7NMuAbR8SGPyexHY/D28meTntBQFDm48PT3pypUrokmKu4JzUjFLSEggGxvLns9C+1eBE4oBAMA8xygzRs2N9h/ErK6vq856/fIlNc8/aO8vHt9qVl7ycllkQvE777wjpmDw9vYWN6Zjx45i+4kTJ6hatWpk0XRybgAAwBwkpmafkNLWmEk3eoKslUObUGClUpr1qp7OdH1WF3KwtexKBMmCmxkzZlCtWrVEV3BuknJwyOyOxrU2kyZNIkuGmhsAAPOz9WI4davtrbPNWo7gRuuU3CPK1ka3cQWBjcRdwfv27Ztt2+DBg8nSaf8yqIw3OjcAABRBhp6xO2xkGI4Y80XJmHPDDh48SD179iR/f3+x9OrViw4fPkyWDjU3AADmR1+OpCw1Nxj4Vb7gZtWqVSLPhifL/PDDD8Xi5OREHTp0oL/++ossmU5vKVlLAgAA+aVv0FU5am5kiKcUq8DNUl999RXNnTtXjGujxgHO/PnzaebMmfTmm28auoxmCb2lAADMg3qi46vhsaILeICXs+S9pZYObJDrH8jG6IquZAWuubl9+7ZoksqKm6Z4QD9Lpj0TOIIbAADzmYaBJ8jsuugwBS08RClpGZI3S3XNksDMtE9pb1uorBEobM0NT7Owd+9ekWujbc+ePeI1eAHtUgAAZuHyw1jaf/2JZj0pLZ2SUrJ3DzeUfo189G7nmpvhrfwoKiGVKpYqJtn5LUGBg5uPPvpINEOdO3dOTJbJjh49SitWrKBFixaRJdOOZzBxJgCAeXielKazvvTALTqpZ0oGQ8hrrJop3WtIcl5LU+DgZuTIkWL6he+++47Wrl0rtlWvXp3WrFlDvXv3lqKMZgnNUgAA8giPSaTDN59S73pl8zU2TEp6RrbgRgrtAspgrBpTHufmlVdeEYu26Oho0VvKkhOKtXsTpqWjXQoAQA5dFh6mmMRUCotKpAmdquq89igmiaZuukiDm1fUybmR2qSu1ah/I6RuGIvBMpbu3btHb7/9tqEOZ/YOh7xsvwUAAOPhwIYdvP5YZ/vBG0+o2ey9tOfqY3r7t5NGvSU8A7hbcXujntOSIR1bot5S5d2RDAYAYEoGLzduQAPyQXBjQNoVmx7OjoY8NAAAFBCSAywXghuJIKEYAADAxBOKv//++1xff/DggSHKo5g/E/RMVQIAABaiVjkXuvQgVu5iWKx8BzcLFizIc5/y5csXtTyKgZobAAB58R+ZcclplJyaTqVKOBj13I0quCO4MYfgRoqpFQ4dOkTz5s2j06dPU3h4OG3cuJH69OmT4/4HDhygdu3aZdvO7+Wxd+SmXVmDmhsAAHmpSEW1Pt8pnp+f3tmo53bA9AmWm3MTHx9PdevWpcWLFxfofdevXxcBjXrx8PAgU4C5pQAATId2s9D2S+HGPTnmvTS/QfwMpWvXrmIpKA5mSpYsSaYMzVIAAKbj0E2MPWZJzLK3VL169cjb25s6deok5rUyFTrNUjKWAwAAdKWkSfOpvO3DVnq387QPQTU9cRssseamoDigWbZsGTVq1IiSk5Pp119/pbZt29KJEyeoQYMGet/D+/GiFhsba/QmKgAAkNeeqxGSHLdGWReq4lGCbj6OE+ufdqlGWy8+pKEt/cjOxor+PhlKnaojyDE2swpuAgICxKLGs5LfunVL9ORauXKl3vfMnj2bvvjiC6OUTzuewazgAADGl5iSbvRzepd00gQ3I9tWFosaBzlgBs1S27Zto507M7PPtfG27du3k7E1adKEQkJCcnx98uTJFBMTo1lCQ0ONUq4MRDcAALLNK2VMyB1WQHAzadIkSk9P19sMw68Z27lz50RzVU4cHBzIxcVFZ5Gy2+HL5wAAYAw807f6D0prGTJJrRDdmH+z1M2bN6lGjRrZtlerVi3XGhR94uLidN7DY+lwsOLu7i4GBORaFx75+I8//hCvL1y4kPz8/KhmzZqUlJQkcm727dtHu3btIlOg2yyF8AYAQGo7LoXTiFVnqHsdb1r8ZgOykSHS6FmnLB24/gQTJptzcOPq6kq3b9+mihUr6mznIKV48eIFOtapU6d0BuWbMGGCeBw8eDCtWLFCjGFz//59zespKSn00UcfiYCnWLFiVKdOHdqzZ4/egf3khtgGAKDo0jNU9DA6kXzdi+l9/ZN1F8Tj1gvhNKJ1DJVzc5L0so/tUIUW7b2ps+3VBuVE+QK8nCU9N0gY3PTu3ZvGjRsnRhOuXLmyJrDhoKNXr14FOhb3dMqtVxEHONomTpwoFlOl/ZOg5gYAoOg+/Pssbb0YTlO6VachLSqSnY1uu9Pz5DTN854/HqHZr9aW5LIv7F+PetUtS9bWVtmCGysrK2ri5y7JecFIwc3cuXOpS5cuohnKx8dHbAsLC6NWrVrRt99+W8hiKA/yiQEAio4DG/bVtqu0+0oErR0RmOv+kzdcNOhlv/11N5FTwwEMKLxZ6tixY7R79246f/48OTk5ieah1q1bS1NCs50VHDk3AACGdPJuJD1PSiVnRzujXViuqclJWVdHo5UDjDDODUewnTt3Fgvk0FsKsQ0AgMEN+OU4bfmgFV1/9JxS0jJkucLF7G0oISWdmlUqJcv5wUDBzffff0/vvfceOTo6iue5+fDDD/NzSMVDzg0AgDSTYXK376CFh2S7vNvHtqJ/zz+kQc11O9aAmQU3PALwwIEDRXDDz3Or0bHk4AYjFAMASC9ZphobtQqlitOY9lVkLQMYILjh8Wf0PYecIecGAKBwYpNSqd+yYHqzaXm9r1efvgOXFnJV4LEcv/zyS0pISMi2PTExUbxmyXS7gstYEAAAM1Znxi669ug5Td98We6igKUENzwJJY8snBUHPMaaoNIcoOYGAABAHtaF+dLW19+fu4XztAmWTDugQc0NAED+LN4fQiuD74rnIY+fy3LZVrzTmHzdpR3dGEywK7ibm5sIanipWrWqToDDE2lybc6IESPIkmGEYgCAggmNTKB5O6+L528HVqSO8+XpBdU2wIMOT2xPFSdtleX8IFNww5NWcs3Eu+++K5qfeDA/NXt7ezHXVGBg7iNHWhKk3AAA5C0mMVXz/EF0ouyXbFhLP/r1SGbHmcYV3eQuDkgd3PBkloxn5W7evDnZ2RlvhEhz7AqOnBsAgLylpr/s1h0uU3DDM4qrTe1Rgz4OCqAD1x9Tc//SspQHjBTcxMbGkouLi3hev3590TOKF33U+1k6DOIHAJC3A9efyF7j/W3fujrrjnY21KXWy4AHFBrccL5NeHg4eXh4UMmSJfUmFKsTjTn/Bogy5B1jCgDALNQvX1LzfNuLSTKNzcneRpbzgszBzb59+zQ9ofbv3y9hccxX1mYo1NwAAOTNye5lYHHpQYzRLtlHnarSd7tvGO18YILBTZs2bcRjWloaHTx4UCQV+/j4SF02s4aEYgCAvKVpjZsREZss+SX75rXa5F7cgZ48l/5cYCazgtva2tK8efNo0KBB0pXITGWdBRwJxQAAeUvXCm7uR2Yf/d7Q+jfOnNIhKTWd1p8OpdZVy0h+TjDx4Ia1b99e1N5w12/IGQbxAwBLxwFEVEIKebs66QQz/X4KptP3oqilf2nqVtt4ibtr3w/USRreMKqF0c4NJh7cdO3alSZNmkQXL16khg0bUvHixXVe79WrF1mirM1QyLkBAEvX4buDYuyaBf3r0tYLj2hS12o0ePlJzXg2R0KeisVYmvhZ9ij6lqTAwc2oUaPE4/z587O9ht5SOTdTAQBYGnUQM37NefF47VGsUQfqa+rnTifuRBrtfGDGc0tlZGTkuFhyN/CsOTbIuQEA0BUWZdxB+tZoNUOBZSlwcAP6pWfrCo4rBQCWy1T+wOtY3VPuIoApBzc81k2NGjXEaMVZxcTEUM2aNenQIXkmPDMFOy490llHzg0AWKLN5x7Q8dvPaMmBW2QKvh9Qj6b1qEGHJ7aTuyhgqhNnDh8+XO/0CjyJ5vvvv08LFiyg1q1bkyWKiE3SWUfNDQBYmuuPntPY1efEc2fHAqd0GtTC/vXEYzF7Wxra0k/WsoAJ19ycP3+eunTpkuPrnTt3ptOnTxuqXGbPVKpkAQCM9Qfe/wXf1azbWmefpkdKXDPTucbLJigvV0ejnh9MS75D64iIiFxnAucB/p48eTkBmqWxIt1fZMQ2AKA0aekZ9P2+EAqq6Uk1y7rqvNZm3n5KSn05qV5UQqpRy+biZEe7rkRo1uOT04x6fjDTmpty5crRpUuXcnz9woUL5O1tubOoqrKMdIOcGwBQmskbLtL3e29S9++PZHtNO7CRQ9b5nG1t0F/GkuX77nfr1o2mTZtGSUm6uSUsMTGRPv/8c+rRo4ehy2e2kHMDAEqz7nSYrIHLuhE5d+22zhLd8OjHYLny3Sw1depU2rBhA1WtWpXGjBlDAQEBYvu1a9do8eLFYoybKVOmkKXK2iyFmhsAUHpe4YWwGKrq6Uw/HZK2Z9Sd2d0pJS2D7G1z/nvcxsqKOlTzoL3XHmeuGznnB8w0uPH09KRjx47RyJEjafLkyZqEWR6VOCgoSAQ4vA9kCouSfgI4AABDi0tOo+L2NpSSnkEOtjaa7edDo3X2++LfK7Ti2MsEYqnlFtgwrrgpXcLBaOUB01agvnoVKlSgbdu2UVRUFIWEhIgAp0qVKuTm5kaWLmt7798nQ2n2q3XkKg4AQIFxANN78VHN+qDACvRl71riufZ2ZszAJj+4d1bWwVTBchUq44qDmcaNG1OTJk0Q2OSgRx3LTa4GAPO0NMvAe38E3xOPe7R6IalVKqM7abKxZB2Mj/NwNo5qLhKIs443BpYL6eQSqVymhFSHBgCQhL48lVN3I2nYH6eybQ+LNO48UWq+7sVomNagfI0rulP98pmtB4dvGm+GcTBtCG4kgkH8AMDcWOsJbvouC9a7L+fkyCUN3VEhDwhuJIKWXwAwN5cfxJCpebVBuWzbfNyc9O7bxM/dCCUCcyDv5B8Khq7gAGDKktPSqd+yYKru7UKfdqlGbsXt6fbTeDI1ZfT0gHo7sAI9ikmi9tU8dLb7lSpOJ+9EGrF0YKoQ3EgESfsAYKqePE+m3j8eoYcxSXQ+LIZW/xdKM3rWIFOkr/mLu6hP7ZG9vNZoi4AX8F9BImgSBgBT9dG68yKw0Tbj3ytkSvw9Mjtl9KxbNt/v6VGnbK7NVmA5UHNjpLmmAABMxbn7UWTqtn7Ykh7HJoveUfnVwr807RjXinzd8v8eUCbU3BgIj9Ssbcv5cEMdGgDAoMzhTy9ueipIYKNWzcuFijvg73ZLh+BGIg+i5RkDAgAgK56XKSk1XbOemPLyOYASIbgBAFCwjAwVNZu9l+p+sUsEOQzjxIDSoe7OQDD/LACYam+jyPgU8Tw8JpEqlJJn2oSc7BzXmnjswLWnQumXw3fkLg4oBGpuAAAsxInbkTR721UyFY0quFGAlzNV8XSmKd1Nsys6mCfU3AAAWIiJ/7tApiLA05kWD2ygs417Ok3ecJE+7hwgW7lAGRDcAAAomKkOKLpzfGu9PZ02jmohS3lAWRDcAAAowI2I5zRi1Wmq4lGCLobF0JK3GlI935IYcwssEnJuDCTLMDcAAEb10drzdPtJPO28HCFGHx656jSlpWfQm7+cMLk7YW+Drx6QFv6HAQAoQKLWODYsPCaJVhy7S+dCo8kUTOlWXfO8WeVSspYFlA/BDQCAAuirPJ611XR6Rg1vXUnzXGWqiUCgGAhuAAAUwBSbxo9Oai8eW1ctI3dRwMIgodhATPBzBQAsiLUJRDdnpnWiBjN3a9bLlXSiu3O6y1omsEyouQEAMHPczMMjEcvNvbg9rXmvGfm6O9GKdxrnuB9apUBqqLkBADBjm84+oHFrzsldDJrzam3x2LRSKTo8MbM5KicZiG5AYqi5MRArE6gSBgDLcuZ+lOSBzVvNyuusrxsRSH3qlaXe9crqbH+jie5++rxSv5x4HN3O38ClBNCFmhsAADOTlJpOoZEJtOV8uOTnmtWnNg1pXpEmrD1PYztUocYV3cXCapV1pa+2XaWRbSvn61jz+9WlaT1qiOYrAMXW3Bw6dIh69uxJZcuWFTUfmzZtyvM9Bw4coAYNGpCDgwP5+/vTihUryBRULG1aM+0CgHJVm7aDOi04RMuPSjeLtr2ttZjrifl7ONM/Y1pSh+qe2bp3c4+oiUH5mwuKP+cR2IDig5v4+HiqW7cuLV68OF/737lzh7p3707t2rWjc+fO0bhx42jYsGG0c+dOkpuzIyrBAEAZ3mjsS1e/7CLmesoL94hCszyYGlm/kbt27SqW/Fq2bBn5+fnRd999J9arV69OR44coQULFlBQUJCEJQUAkM/RkKfk6mRHtcq5SnaOOj6udCEshmysrWjOa3UkOw+AMZhVdUNwcDB17NhRZxsHNVyDk5Pk5GSxqMXGxkpaRgAAQ3oQnUgDf82cH0rKMWO42Sk1PYPsMO8TKIBZ9ZZ69OgReXrqtvnyOgcsiYmJet8ze/ZscnV11Sy+vr5GKi0AQOHGrBn+xyka/ecZsR4WmaDz2ui/Mrcb0tevZHbjRmADSmFWNTeFMXnyZJowYYJmnQMhBDgAYEqiE1JEsxPnrkTEJtPuKxFi+5ykVJ18lq0Xw2nrBcP1kNowqjmlpmWIsWkAlMSsghsvLy+KiMj8pVfjdRcXF3JyctL7Hu5VxQsAgCk6eSeS+v0ULMaNWfRGfUrXGuAua6LuL4duG/Tc3q6O5O2q/7MTwJyZVbNUYGAg7d27V2fb7t27xXYAAHO0eH+IeNx87qF4zMjQCm6yzKB9PiymUOfYPjazS3dWnDwMoESyBjdxcXGiSzcv6q7e/Pz+/fuaJqVBgwZp9h8xYgTdvn2bJk6cSNeuXaMlS5bQ2rVrafz48bL9DAAARZF1KoI1/4VqnvMr/X8+XuQLXN3bhfw9SmTbbmttVn/fAphHs9SpU6fEmDVq6tyYwYMHi8H5wsPDNYEO427gW7duFcHMokWLyMfHh3799Vd0AwcARQQ3jWbtpuS0lxNgatfaFJa6h5W+OhqMzwVKJWtw07Zt21x/efWNPszvOXv2rMQlAwAwjsexL4eqeBqXovPazwbMsela25tu7r1JFUoVo02jWhCn86B3FCiVWSUUAwAoxbO4ZGr69V5K08qxyeqHfZn5OIYwpp0/BXg6U7NK7uSGuZ1A4RDcAAAY2eZzD2jsamln89Y3V1T3Ot5GPSeAXJBNBgBgZDO3XMU1B5AQghsAACN7Gvcyz0aKSS971i0r2fEBzAGCGwAAiXHHCR6/Jik1nYb93ylJz9W3oQ81KF9S0nMAmDrk3AAASGzQ8pP05Hmy6Hr9390oSc5xdFJ7uvc0nhpVdKdzodGSnAPAXCC4AQCQ0NXwWDp886nk17hcSSexsNT0oo+PA2DO0CwFACChrosOG/36pme8HAgQwBIhuAEAMIC/Ttyn/dcey3Itf367oc76m00rkFsxOxrQxFeW8gDIDc1SAAAGaHr6bONFnekOOIn41aXHJLu2zg629Dw5TTzvXNNL5zX34vZ0amonTIwJFgs1NwAARfQoNinbtrOh0XT2vuESe2f0rKGzXsfXVTza2+j/GMeM32DJENwAABRRXFJmDYq2+Be1KobSKUvtzIJ+9WhI84q09cOWBj0PgBIguAEAKKJP1p/XPA+NTKDktHRKz2XOqMLgnlAdqnlo1j1cHGlGr5pUxdPZoOcBUALk3AAAFALn1CSnZZCjnQ0lpb7sndRq7n7xWLtcZrORIXzcuap4nPd6XWowc7eYBBMAcobgBgCgEN5beZp2X4mgY5Pa63394oOYIl9XdXKydqJw1m0AkB2apQAACoEDG/aaAXtEFbe3IR+3zIH4AKDwUHMDAFAAEbFJdOdpvGY9PCZ7T6nC+mNoU7KyIhq8/CRN6loN9wWgkBDcGIi1lRU52lnrtL0DgHJ0W3SYroTHSnb8I5+2Ix+3YuL5+emdydraSrJzASgdmqUMpJ5vSbo2syv9NriRoQ4JACaUPCxFYNOrblnNc/W8UAyBDUDRILgxMHtba50PRAAwfwbu1a0xql1lzXN8XAAYDpqlJPwQ5CYqJ3sbQ58CAIwsTaKJKEs62dOa95qRg50NamsADAjBjQTNU2oY/hxAuSMQG0KGSkVNK5WS5NgAlgzNUoa+oFa6H1wAYF64OVndpKx+bDhrjyTnwmcEgDRQcyNBrykAME9p6RnUZ8lRiopPpU41PGnLhXBq6W+YmpW5r9Whif+7oLPN1hp/XwJIAb9ZBqYd2+CvMgDzcuFBDF16EEsPohNpxbG79DQumTade2iQY/dr7Kt5XtbVkQYHViAvV0eDHBsAdKHmRsKaG7RKAZiX2MRUgx+zddUy9EF7f83zM/eiaMf41uTiaGfwcwFAJgQ3BoaaGwDzNeT3/wx+zGVvNaBi9pkftf/3TmNKTVfpDBkBAIaH4MbArMhK8rExAMBwTt+LovdXnqZutb0kuazatblWVlZkb4u8PACpIbgxMJ0R0xHcAJg89cSXfwTfk+T4jnYY6wrA2FA3KuFfaUgoBjAtJ+9E0ox/LlN8cholpaZLPop49zrekh4fAPRDzY2EOTfpyCgGMCn9fgoWj9wTytC4WSslTUV7rkZQS//S9FHnqlS/vJvBzwMAeUNwY2Dcps4BDsc1qLkBULbpPWrQl1uuiOdLBjYUNULBt55Ryyql0RwFICMENxKwsbKiNJWKJJqOBgDyEJuUSlsvhFOXml7kVtxesus1KLAC/Xc3khq8qKEp7mBLHWt44v4AyAzBjWR5NyrU3ADIZPzqc7T32mOas/0a7ZnQhso4O0iSX2NrY01L32po8OMCQNEguJGAGFE9nSgdfcEBZMGBDYtJTKXGX+2h8R2r0h/BhsmzsbW2ojT8bgOYNAQ3EjVLMeTcAJiGBXtuGOQ4VTxK0K7xrWnl8XtU3xfJwgCmCsGNBKxfDHaDmhsAZbGx5g4DVjQosKLcRQGAXGCcG4k+ABlqbgCM72jIU8mO/XZgBcmODQCGg+BGwmapdPSWAjCqPVciaOCvJyQ7/ptNykt2bAAwHAQ3EkCzFIBxpKZnUHRCinjOj8P+OGWwY/uVLp5tGzdJAYDpQ86NhSQUP4tLplIlHOQuBoBBdZp/kO4+S6C2AWXoQVSiwY578rMOFJWQSkELDxnsmABgPAhuJMy5MZWE4rX/hdLE/12g0e0q0ydB1eQuDkCRLDt4izacCaNVQ5uKwIYduP6kyFf11QblaMOZB+K5h4ujWBa/2YA8XByojo8r2dugohvAXOC3VYqLam1ac0txYMMW778lqvEfxSSJ9ZlbrlDQgkOUkJImcwkB8o8H5rsREUdNvt5rsMt2d053GtXWX+/El40rupODrQ2apADMCGpupGyWMpGaG21VpmwXj38Oa0q/Hbkjnm+7+Ij6NvSRuWQAOQuNTKBzodEU4OUs2WXy9yhBeya0plLF0XwLYO4Q3FhoQrF2jxI7GyRJgul6npRKrebul+z477WupHnu7yFd8AQAxoPgRsqu4CbSLJWXh9GZzVQApoBrPB/GJNKWC+HEv0mL94dIer6hLf0kPT4AGB+CGykH8TOTcW6+2XGNhrXyIzskTIKMgm89oy/+vUzXHj2X/Fwdq3uSs6MtlXCwJU8XR8nPBwDGheBGwuAmzVyiGyI6cvMpVfVypnIlneQuClioAb8cN8p5vn29LnLMABQOwY0UF/VFDUhaunk0S7F3Vvyn6TUCYCxj/jojmp+m9ahhlPMNaV4RgQ2ABUBwIwF7G/OruQEw9qCSPF4NBzbqYQmk1NK/tGh6bRvgIel5AMA0ILiR4qK+GOgmReaamzP3o+jVJccK9B6VSkXbLz2immVdqEKp7MPPAxTl/+Nbv56ghJR0qlymON16Em+0i7lqWFOjnQsA5IdB/CRgq665kXnmzIIGNmzj2Qc06s8z1GbeAbr71HhfPqBcPOcTjyjM/x85sGFSBjZzX6tDzSuXoioeJcR6owpukp0LAEwTam4kYGcCOTfqyQQLasLa85rnPX84Qhe/CDJgqcASvbr0GN2WuJbm6KT2FJOQSk/ikql1ldLUr7Gv6FJ++n6UqIUEAMuC4EaKi/qit1SqjDk39b7cXeRjPE/GtAyQfxxcJKenk4ezbtdqqQMbxr38svb048E0eeoEALA8aJaSQPDtZ+LxysNYKQ4PIIvktPQcpxQ5eOMJ1f1yFzX5aq9m7jLGc5kBABgbam4k8Dwps8bjzxP36atXaktxCgCjik9Oo8Zf7RFzO20c1ULnNQ54Bi8/qVlvNttwE1rmZUH/utSzTlmjnQ8AzINJ1NwsXryYKlasSI6OjtS0aVM6efLlB2VWK1asELPzai/8PlNUsVQxMlUnPuuQr/0qTtpKT+OSJS8PmLYTd56JZOCz96NFovzj5y9rZ4IWHjJKGUoWs6NFb9Sj/41sThtHNafbX3ejV+r7aMaVAgAwmZqbNWvW0IQJE2jZsmUisFm4cCEFBQXR9evXycND/5gULi4u4nU1DnBM0d1nCbKdu0H5knTmfnSOr/OQ82Pa+dOP+Zi35+3fTtL2sa0MXEIwdXHJafT2byeocw0vMWO2mv+LmeVZm6pl6ObjOMnLEjy5PXm7YvRsADCT4Gb+/Pk0fPhweuedd8Q6Bzlbt26l5cuX06RJk/S+h4MZLy8vI5fUfFx6EKM3sOnXyIfWngqjd1tkThQ4rmMV+u9uJJ24E5nr8a6GI3fIEvVZfJRCHseJ2pqccK6NoX3apRq5F7ejXnXL0d5rEaJJDIENAJhNcJOSkkKnT5+myZMna7ZZW1tTx44dKTg4OMf3xcXFUYUKFSgjI4MaNGhAX3/9NdWsWVPvvsnJyWJRi41V/hd1jx+OZNvWuKIbzepTm/o39qW6PiXFNq7OX/N+oKb5KTc8mmzZkk7Uqy7yGyzBxbAYEdhIrbq3i07wnHX6jx7IpwGAQpC1sfrp06eUnp5Onp6eOtt5/dGjR3rfExAQIGp1Nm/eTKtWrRIBTvPmzSksLEzv/rNnzyZXV1fN4uvrS5Zo9XuBZG9rTQ0ruBcqR2HO9mv04d9n6fS93Gt5wHzx6NT3nsWL2bl7/pg9QJYC/1fkAAcAQFHNUgUVGBgoFjUObKpXr04//fQTzZw5M9v+XCvEOT3aNTeWGOCoZyrPib2NNaXko9vujYg4ESCBcrp3D/rtJDWtVIoeRifS+tP6/0gwlJ/ebkgRsUk0ffNlzbZZfWrRa0uPUani9pKeGwAsh6zBTenSpcnGxoYiIiJ0tvN6fnNq7OzsqH79+hQSoj8x1sHBQSyWYsXRO9m2NfXLOxh5t6WfaHrKS9yLbu5gXjUyfpO3iSTzDVm6cf97PlzkXOWVd2UonIDsYGutCW5UKqKGFdxo30dtyMvVNHs9AoD5kbVZyt7enho2bEh7974cF4ObmXhdu3YmN9ysdfHiRfL29iZLx71bZvybfXbltwMr5Ple/uLLj6+2XS1U2UA+PF8YUyeZ33kaT63n7qfZ26/Sx+teTrchtcBKpcjRzkZv78ZKZUpQMXuzq0gGABMl+6cJNxkNHjyYGjVqRE2aNBFdwePj4zW9pwYNGkTlypUTuTPsyy+/pGbNmpG/vz9FR0fTvHnz6N69ezRs2DCydD/su6l3e9daeQd+nWp4ijFExq4+J0HJwNi+3XlddPP/d0xLCtcaMXjtqVCauP6CeP7TwduSloGTg3m04h/336SPOgWQm55mpxwGPAYAKBLZR7/q378/ffvttzR9+nSqV68enTt3jnbs2KFJMr5//z6Fh4dr9o+KihJdxznPplu3biKH5tixY1SjRg2ydCdzaFrIK9+G8V/TveuVoy0ftKSO1T1p9/jWEpQQjEU9fhEnBnNtiZo6sJHCtg9b0fQeur+H3NTEvfSyBjY8azd7q1l5ycoDAJbLSsUN8haEgyHuNRUTEyMGA5RCtWnbKSk1Q2/XVim1mLOPHkQn6myb1LUajWhTuVDHu/YolrosPJxt+6qhTallldKFLicYzoYzYTR/9w0a0rwilSphT+2redI/5x/StE2XNPt89UotmrLx5bqhONnZUGJqunh+fnpnci1mp/l/417cPtsEmtoSU9LpesRzquvjarKDcAKA+X5/y94spURTu9egqS++XG5GPKcVx+6Knii/v9NE0vNmDWxYPd/85dLoE+DprHf7W7+dMGrQBtyEdIs2n3tIfw1vSiWLZdaC7LsWQRPWZubMzNqacy5UUQIbvs+3n8SRj1sxqjr15cjEPep4049vNqDF+0PIx81JE9iwal55/9HgZG9TpP+bAAC5QXAjgW0XXzaj8RcST6DJDt14Qq2rliFjKuFQ+FvMf1GfmdaJGszcne01nl8Ic/oY3qm7kdR3WTA1q+Quxiaa9L8LImg9fPOpeH3JgVs0uWs1kTz+7opTJKU17zXTJPsyX3cnCo1MpHPTO2kCrNHt/CUtAwBAYSC4kUCpEi+7nv954p7m+aDlJ41e48GjChcFNy/ow/ML3ZjVVQwMCIULgHnZdTlCTGLKOSl9lx6jU/eixOvHb0fSwj03aPV/oTrv+/nQbbEYQ5MsQwgcntjeKOcFACgqfDNJYGyHl3/NpmXpDsLNU1Ko+8UunfU9E9rQznGtcwxOCoJ7UunDzRRcgwMFcyEsmkb9eYa2XAgXAye+suQopWeoNIGN2sI9+nu/SY2TfK98GYRcGAAwWwhuJODv8TJX5XmWQe+az9ln8PNlZKgoJjE1SxlKUICX/pyZgvplUCMq4+yQ7zwfyN3SA7qDJd6PTKDKn20zmcvGvZsw5gwAmDMENwoQm6Qb2EiBe+PoczTkmeTnVprtlx6Z1FgvdXxcaUATdMkGAOVAcCODLRceGvR4PN+T1HJqmnqelCpqHXhWcTRR6fcsLllco58P3aKYBOkD0YIIqukpauZmv1qbFvavJ8Y5AgAwdxjnRiL8ZZ8bQyUWXw2Ppa6LdMei4XFteHwbQ+NJFXMbrv+ToAD0nsni/rMEaj1vv2a9XEknozblLX6zgcjzUk+bMa1HDdF1u4a3CyWkpBus6RIAQGoY58aCZA1s2KddAiQ5V/fa3rkGN/N2Xqf3W1dCF/EXzodGU+/FR3WukTEDm3l961D3OplTbwxr5YcEYQCwGGiWkki/Rj65vp70YmRXKUg14isPvMZNGLnhLuKc4Kw2a8sVmr7Z8KPjmprQyARadypU1JLw6MBcc5c1sDGm1e81o9cb+WrWMQowAFgSBDcSWXsqTGf9fyOb66xHxL6czNCQtn7YUvLcmwmdqua6z47Lj+iTdecp5HEc/XrkDv0RfE/MRK1krebup0/WXxC94VYefzm2UUF0q+1VqPfxoH5qfG/WjQikZpUy524CALBEGMTPSBpWcNNZtzZA7Qrnc2RVs6wrSe2D9v5iPqOc8BgubN3pMJ2Rd5fsD6Fq3i40tKUfvb/yFJ2+F0XHJ3fQNGNxUHD/WTx91q26WdQ0/HbkjkiiPnjjSZGPVaFUMVoysKF4npqeQQeuP6Hhf2SOQDy/X13qU68cpWZkUO8fj1J1bxea2r26GMyPawDfb1NZLAAAkAnBjUT61CtLm85l9oo6PbWj3r/0eWqDwg6yx4O+aSeqGlNhAg+u1VDj4Gbn5QjxfNulR9SrblmKT07TTPbYvU5Zk593iOdbmrnlSpGOwflJfC32XH1MrzUsp9luZ2MtasiyJp07WNvQjnEvZ2uf3K16kc4PAKBUaJaSyPx+9cQ8UrP61NJMx7D2/UCdfT78+2yhj//Fv5fJXGk3yfE14InpgxYe0myLyzLwoangcqr99WK+sKJwcbIjDxdHerNpeXKwtSny8QAAIBNqbiRibW1Ff7zbJNe5eo6EZE6GWFCXHsSIPJacJjo0BmcHW3qeXLggpOnXe3XW150Ko7Col72I1pwKpX/OP6DpPWtSMTsbcS3ltvZUKE18UfvE+VOcS1RU9cubdu0UAIC5wjg3RpZ1/JvCjHeT0xg6xpyUk2sxTtyJpDd+Pi75uXhguVrl9OcScc6Jo52N5D+r32TDTI9QuUxx0ePs1pP4HAdGBACA7DDOjQVqmqVWyBh5N9wjhwMq7vpdScK5kXr8cIRCvuoqJiH99/xDMRN573rlaMTK06JnFlOXIz+1PDxKcN0vd4nE6I866x8TKCo+hexsrWnujmt6a8kK6vDEdnQuNJraBpQhZ0c7qlSmRJGPCQAA+qHmxsi4pqHatB06TVXc66lHHW+a2KWa+OLOTXhMIgXOzj75Jictq3N75HD6XiQ9eZ4iphg4cz/a6Od/o7GvSMzdM6E1lSyWe5J27x+P0PmwmBxrux7FJFGz2bpNZ4V15NN2IhDkkYkBAKDwUHNjwhyyBC8n70SKR87h4KVrLS9aMrBBjj2S9AU2TM7AhjWskFlz1KWWl+jKXGXKdrHOw/xfCY+V/Pyr/wsVj68tPUabRrcQtSPcnHQ1/Lnosj2le3Uq7mAjEnfVgY06l4ZraVr4lyZft2LkYGdNo//K7MpeFB+296eWVcqQj1uxIh8LAAAKBgnFRsZBS4dqHrT32uMcZ4zmbtPfvl43W9fvATnktxgz1yY/uCuz2vqRgVRj+k6jnZtzWWrP2EXjOlahhXtuarb/70zmmDsc+GhTJwkbYkTg4FvPaNHem/T1K7VFDygAAJAHmqVkkJ8EVQ5YIuNTaNDyE/RqfR9Ky8igr7ddy7bf0UntTbLJg5vfuAaHa1C4qUq77H6liytixOKZvWtSxxqe5O1qetcfAEBp0Cxl4vIzCB7XAgz4JbOm5tKDK2ZRY6ONezCpezG917oyDW1ZidS5vpwY3OzrvfQsPoXM2duBFeUuAgAA6IFB/EyUOrBRChtrKxHU8cLNVqendRLB2aUvgkQPopzs1BqRV27F7V92OV/2VuZUCQAAYHqQcyMT/kLn+YMK6+/hxhuwT0olHGxpxTsvBztMScsQIxjP2X6NxneqQv4eziII4u7bkQkpdOTmE0pISafZ27M30UnJ08WBjk3qIGb99nZ11MyHBQAApgc5NzLJ2iW8IP4Z04Lq+GB022dxyWJkY04Kvh7x3OD3qKyrIzWtVErMbs6jEufVTR8AAKSDnBszwPko3F34+30hVM3Lmf4c1lRMoplXojHPT4XA5mX3d152jn/ZdBUamSAmJdVHfb3Zojfq0crge3TqXpSYuLNRRTd6s0l5kQ/E01vw9ndaVMScTwAAZgg1NzLi7t1n7kdR7XKumuTbnKZWYL8PaUztqnkYsYTmafvFcBr5Z+ZYNRdmdCZHWxuys8nM94lJTKXElHTycnXU9FwrzCznAABgXKi5MaMk28YV8542wcfNiY582t4oZVKCrrW9qX8jX6rj60oujnY6r7k62YlFDYENAIDyoObGxFx+GEPdvz8iklbHdqhC4TFJNKa9v87AeAAAAJYmNjaWXF1dKSYmhlxcXHLdF72lTEzNsq4mPX4NAACAqUN1AAAAACgKghsAAABQFAQ3AAAAoCgIbgAAAEBRENwAAACAoiC4AQAAAEVBcAMAAACKguAGAAAAFAXBDQAAACgKghsAAABQFAQ3AAAAoCgIbgAAAEBRENwAAACAoiC4AQAAAEWxJQujUqnEY2xsrNxFAQAAgHxSf2+rv8dzY3HBzfPnz8Wjr6+v3EUBAACAQnyPu7q65rqPlSo/IZCCZGRk0MOHD8nZ2ZmsrKwMHlVy0BQaGkouLi4GPTYYFu6VecB9Mh+4V+Yj1ky/qzhc4cCmbNmyZG2de1aNxdXc8AXx8fGR9Bz8n8Wc/sNYMtwr84D7ZD5wr8yHixl+V+VVY6OGhGIAAABQFAQ3AAAAoCgIbgzIwcGBPv/8c/EIpg33yjzgPpkP3Cvz4WAB31UWl1AMAAAAyoaaGwAAAFAUBDcAAACgKAhuAAAAQFEQ3AAAAICiILgxkMWLF1PFihXJ0dGRmjZtSidPnjTUoYGIZsyYIUaU1l6qVaumuTZJSUk0evRoKlWqFJUoUYJee+01ioiI0Ll29+/fp+7du1OxYsXIw8ODPvnkE0pLS9PZ58CBA9SgQQPRi8Df359WrFiBe52HQ4cOUc+ePcWooXxfNm3apPM691mYPn06eXt7k5OTE3Xs2JFu3ryps09kZCQNHDhQDChWsmRJGjp0KMXFxensc+HCBWrVqpX4HePRVefOnZutLOvWrRP/L3if2rVr07Zt2wpcFku9T0OGDMn2O9alSxedfXCfjGP27NnUuHFjMZI+f1b16dOHrl+/rrOPKX3mJeWjLEbHvaWgaFavXq2yt7dXLV++XHX58mXV8OHDVSVLllRFRETg0hrI559/rqpZs6YqPDxcszx58kTz+ogRI1S+vr6qvXv3qk6dOqVq1qyZqnnz5prX09LSVLVq1VJ17NhRdfbsWdW2bdtUpUuXVk2ePFmzz+3bt1XFihVTTZgwQXXlyhXVDz/8oLKxsVHt2LED9zoXfC2nTJmi2rBhA/e8VG3cuFHn9Tlz5qhcXV1VmzZtUp0/f17Vq1cvlZ+fnyoxMVGzT5cuXVR169ZVHT9+XHX48GGVv7+/asCAAZrXY2JiVJ6enqqBAweqLl26pPr7779VTk5Oqp9++kmzz9GjR8X9mjt3rrh/U6dOVdnZ2akuXrxYoLJY6n0aPHiwuA/av2ORkZE6++A+GUdQUJDq999/F//Xz507p+rWrZuqfPnyqri4OJP8zBuRR1nkgODGAJo0aaIaPXq0Zj09PV1VtmxZ1ezZsw1xeHgR3PCXnz7R0dHiS2zdunWabVevXhUf4MHBwWKdf7Gtra1Vjx490uyzdOlSlYuLiyo5OVmsT5w4UQRQ2vr37y8+aHCv8yfrl2ZGRobKy8tLNW/ePJ375eDgIAIUxh+q/L7//vtPs8/27dtVVlZWqgcPHoj1JUuWqNzc3DT3in366aeqgIAAzXq/fv1U3bt31ylP06ZNVe+//36+y2IpcgpuevfuneN7cJ/k8/jxY3HPDh48aHKfedH5KIsc0CxVRCkpKXT69GlRva09fxWvBwcHF/XwoIWbD7hKvVKlSqIJg6tcGV//1NRUnXvATRPly5fX3AN+5GYKT09PzT5BQUFiArnLly9r9tE+hnof9TFwrwvuzp079OjRI53rynPDcNW29r3hpqhGjRpp9uH9+ffoxIkTmn1at25N9vb2OveGq+qjoqLydf/yUxZLx00U3HwREBBAI0eOpGfPnmlew32ST0xMjHh0d3c3uc+80/koixwQ3BTR06dPKT09Xec/EON1/iAFw+AvIG4L3rFjBy1dulR8UXH+Bc8Qy9eZv/T4CzKne8CP+u6R+rXc9uEPg8TERNzrQlBf29x+P/iRv1C12draig9yQ9w/7dfzKosl4/yaP/74g/bu3UvffPMNHTx4kLp27So+3xjukzwyMjJo3Lhx1KJFC6pVq5bmXpjKZ96jfJRFDhY3KziYJ/6QVatTp44IdipUqEBr164ViaEAUDRvvPGG5jn/xc+/Z5UrVxa1OR06dMDllQkn6l66dImOHDmCe1AAqLkpotKlS5ONjU22zHBe9/LyKurhIQf8V0LVqlUpJCREXGeuPo2Ojs7xHvCjvnukfi23fbgHDwdQuNcFp762uf1+8OPjx491XuceHdwzxxD3T/v1vMoCL3HzL/+f598x3Cd5jBkzhrZs2UL79+8nHx8fzXZT+szzykdZ5IDgpoi4Oq5hw4aiKle7GpHXAwMDi3p4yAF3E75165bo0svX387OTucecC4G5+So7wE/Xrx4UedLdPfu3eKXuEaNGpp9tI+h3kd9DNzrgvPz8xMfcNrXlau8OZdG+97wByO33avt27dP/B5xDZ16H+7KzG372veGc0Pc3Nzydf/yUxZ4KSwsTOTc8O8Y7pNxcc43BzYbN24Uvwv8f1ebKX3mNcxHWWQhWyqzgnBXOe5xsWLFCtGj4L333hNd5bSz1KFoPvroI9WBAwdUd+7cEV1+uXsjd2vkXgTqrojcVXLfvn2iK2JgYKBYsnaL7Ny5s+hayV0dy5Qpo7db5CeffCKy/RcvXqy3WyTuta7nz5+Lrqa88EfK/PnzxfN79+5pul/z78PmzZtVFy5cED1y9HUFr1+/vurEiROqI0eOqKpUqaLTFZx7ZHBX8Lffflt0j+X7wPcqa1dwW1tb1bfffivuH/ew09cVPK+yWOJ94tc+/vhj0buFf8f27NmjatCggbgPSUlJmmPgPhnHyJEjxZAF/Jmn3TU/ISFBs48pfeaNyKMsckBwYyA8PgDfXB4PgLvO8XgdYDjcPdHb21tc33Llyon1kJAQzev85TRq1CjRXZh/WV955RXxYaDt7t27qq5du4rxUTgw4oApNTVVZ5/9+/er6tWrJ85TqVIlMdYE7nXu+Jrxl2XWhbsWq7tgT5s2TQQn/CHZoUMH1fXr13WO8ezZMxHMlChRQnRVfeedd8QXrjYel6Zly5biGPx/gAOVrNauXauqWrWquH/cxXXr1q06r+enLJZ4n/hLk78E+cuPA8IKFSqI8Uyy/oGG+2Qc+u4TL9qfR6b0mZeYj7IYmxX/I1+9EQAAAIBhIecGAAAAFAXBDQAAACgKghsAAABQFAQ3AAAAoCgIbgAAAEBRENwAAACAoiC4AQAAAEVBcAMAkqtYsSItXLgw3/vzZI1WVlbZ5qsBAMgPBDcAoMEBRW7LjBkzCnW1/vvvP3rvvffyvX/z5s0pPDycXF1dJb87v/zyC9WtW5dKlCghJmStX78+zZ49W/P6kCFDqE+fPpKXAwAMx9aAxwIAM8cBhdqaNWto+vTpYhI8NQ4A1Hhw8/T0dLK1zftjpEyZMgUqB0/YZ4wZhZcvX07jxo2j77//ntq0aUPJycl04cIFunTpkuTnBgDpoOYGADQ4oFAvXGvCtTXq9WvXrpGzszNt375dzATs4OBAR44cEbOz9+7dmzw9PUXw07hxY9qzZ0+uzVJ83F9//ZVeeeUVKlasGFWpUoX++eefHJulVqxYIWpVdu7cSdWrVxfn6dKli04wlpaWRh9++KHYr1SpUvTpp5/S4MGDc6114XP269ePhg4dSv7+/lSzZk0aMGAAffXVV+J1rqn6v//7P9q8ebOm9orLxkJDQ8V7+Xzu7u7iGty9ezdbjc8XX3whgjuejXnEiBGUkpKi2Wf9+vVUu3ZtcnJyEmXu2LEjxcfH438kQBEhuAGAApk0aRLNmTOHrl69SnXq1KG4uDjq1q0b7d27l86ePSuCjp49e9L9+/dzPQ5/6XNwwDUl/P6BAwdSZGRkjvsnJCTQt99+SytXrqRDhw6J43/88cea17/55hv6888/6ffff6ejR49SbGwsbdq0KdcycNB2/Phxunfvnt7X+fhcRnUgxQs3maWmplJQUJAI9g4fPizOpw64tIMXviZ8nTgg+vvvv2nDhg3i52Z8LA6k3n33Xc0+r776qqgRA4AiknXaTgAwWTw7sKura7ZZpTdt2pTne3lGbp5JWI1nmV6wYIFmnY8zdepUzXpcXJzYtn37dp1zRUVFacrC69ozwS9evFjM7q3Gz+fNm6dZT0tLEzMZ9+7dO8dyPnz4UNWsWTNxbJ5NnGfIXrNmjSo9PV2zD2/LeoyVK1eqAgICxCzjasnJyWL25Z07d2re5+7uroqPj9fss3TpUjHzOR//9OnT4rw8czMAGBZqbgCgQBo1aqSzzjU3XMPBzUXcRMM1GFwTkVfNDdf6qBUvXlw02zx+/DjH/bn5qnLlypp1b29vzf4xMTEUERFBTZo00bxuY2Mjms9yw8cIDg6mixcv0tixY0XTFjdlcQ1MRkZGju87f/48hYSEiJob/nl54aappKQk0UynxonKXG61wMBAcb24SYtf69Chg2iWev3110Vic1RUVK7lBYD8QUIxABQIByLaOLDZvXu3aDLivBXOH+nbt69O84w+dnZ2Ouucz5JbQKFvf0M14dSqVUsso0aNEnkxrVq1ooMHD1K7du307s8BCgdO3AxW2ORpDr74uh07dox27dpFP/zwA02ZMoVOnDhBfn5+Rf6ZACwZam4AoEg434STZzk5mGshOI9FO7HWGDj5mROaucu5GvfkOnPmTIGPVaNGDfGoTuzlnlt8LG0NGjSgmzdvkoeHhwjotBft7utcw5OYmKhZ5/weruXx9fXVBGgtWrQQeTicr8Tn2rhxYyGuAABoQ3ADAEXCPZ04UfbcuXPiy/zNN9/MtQZGKh988IEYn4Z7NnH3dW5m4mYeDiByMnLkSJo5c6YI0DipmIOPQYMGidoXbkJS9/TipGc+5tOnT0UyMSc/ly5dWvSQ4oTiO3fuiIRg7q0VFhamOT7XXnFPrCtXrtC2bdvo888/pzFjxpC1tbWoofn666/p1KlTogmPr+GTJ09E8x4AFA2CGwAokvnz55Obm5voRcS9pLgXEddsGBt3/ebeRxyccGDCNSRcFkdHxxzfw12vOaDhnJeqVavSa6+9JvbnXk7cNZsNHz6cAgICRK4RBz0cCHEeDffYKl++vOjhxAEJBzGcc8O5Q2qcU8PBX+vWral///7Uq1cvzUCIvB8fg3uK8bmnTp1K3333HXXt2tUIVwtA2aw4q1juQgAAGBrXHnHQwV25uXbG2Lipjsfpyas7OgAYHhKKAUARuFmJE3PVIw3/+OOPormIm8kAwLKgWQoAFIHzWHgkYx4hmZN0uXs3j5SMHBYAy4NmKQAAAFAU1NwAAACAoiC4AQAAAEVBcAMAAACKguAGAAAAFAXBDQAAACgKghsAAABQFAQ3AAAAoCgIbgAAAEBRENwAAAAAKcn/AzKIK/95sWgRAAAAAElFTkSuQmCC",
-      "text/plain": [
-       "<Figure size 640x480 with 1 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "plt.plot(moving_average(critic_losses, 100))\n",
-    "plt.xlabel('Training Steps')\n",
-    "plt.ylabel('Critic Loss')\n",
-    "plt.title('Critic Loss over Time')\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 16,
-   "id": "22fa777d",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "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",
-      "text/plain": [
-       "<Figure size 640x480 with 1 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "plt.plot(moving_average(actor_losses, 100))\n",
-    "plt.xlabel('Training Steps')\n",
-    "plt.ylabel('Actor Loss')\n",
-    "plt.title('Actor Loss over Time')\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "207f9c1c",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def evaluate_policy(agent, env_mode = \"TRAIN_SHOOTING\", num_eval_rounds=10):\n",
-    "    if env_mode == \"TRAIN_SHOOTING\":\n",
-    "        mode = h_env.Mode.TRAIN_SHOOTING\n",
-    "    else:\n",
-    "        mode = h_env.Mode.TRAIN_DEFENSE\n",
-    "    \n",
-    "    eval_env = h_env.HockeyEnv(mode=mode)\n",
-    "\n",
-    "    mean_reward = 0.0\n",
-    "\n",
-    "    for _ in range(num_eval_rounds):\n",
-    "        (state, _), done = eval_env.reset(), False\n",
-    "        while not done:\n",
-    "            action = agent.act(state, deterministic=True)\n",
-    "            state, reward, done, trunc, _ = eval_env.step(action)\n",
-    "            mean_reward += reward\n",
-    "\n",
-    "    mean_reward /= num_eval_rounds\n",
-    "\n",
-    "    print(f\"Evaluation {num_eval_rounds} episodes: {mean_reward:.3f}\")\n",
-    "    eval_env.close()\n",
-    "    return mean_reward"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "c0b1727d",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Evaluation 100 episodes: 5.947\n"
-     ]
-    },
-    {
-     "data": {
-      "text/plain": [
-       "5.947087866128124"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "evaluate_policy(agent, num_eval_rounds=100)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "734c1ef2",
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "rl",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.10.19"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/src/rl_hockey/scripts/train_td3_hockey.py b/src/rl_hockey/scripts/train_td3_hockey.py
deleted file mode 100644
index 943ffb2..0000000
--- a/src/rl_hockey/scripts/train_td3_hockey.py
+++ /dev/null
@@ -1,242 +0,0 @@
-import numpy as np
-from tqdm import tqdm
-import matplotlib.pyplot as plt
-import hockey.hockey_env as h_env
-import datetime
-import random
-import os
-
-from rl_hockey.td3 import TD3
-
-
-def evaluate_policy(agent, easy=True, num_eval_rounds=10):
-    eval_env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)
-    if easy:
-        opponent = h_env.BasicOpponent(weak=True)
-    else:
-        opponent = h_env.BasicOpponent(weak=False)
-
-    mean_reward = 0.0
-    winners = []
-    print("-" * 20, "Evaluation", "-" * 20)
-
-    for _ in range(num_eval_rounds):
-        (state, _), done = eval_env.reset(), False
-        state2 = eval_env.obs_agent_two()
-        while not done:
-            action = agent.act(state, deterministic=True)
-            action2 = opponent.act(np.array(state2))
-            state, reward, done, trunc, info = eval_env.step(
-                np.hstack([action, action2])
-            )
-            state2 = eval_env.obs_agent_two()
-            mean_reward += reward
-
-        winners.append(info["winner"])
-
-    mean_reward /= num_eval_rounds
-    winrate = winners.count(1) / num_eval_rounds
-    print(f"Overall Scores, {num_eval_rounds} games:")
-    print(f"Mean reward: {mean_reward:.3f}")
-    print(f"Win rate: {winrate:.3f}")
-    eval_env.close()
-
-    return winrate, mean_reward
-
-
-def moving_average(data, window_size):
-    moving_averages = []
-    for i in range(len(data)):
-        window_start = max(0, i - window_size + 1)
-        window = data[window_start : i + 1]
-        moving_averages.append(sum(window) / len(window))
-
-    return moving_averages
-
-
-if __name__ == "__main__":
-
-    run_name = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
-    run_dir = f"/home/stud389/RL_CheungMaenzerAbraham_Hockey/results/td3/{run_name}"
-    os.makedirs(
-        run_dir,
-        exist_ok=True,
-    )
-    env = h_env.HockeyEnv(mode=h_env.Mode.NORMAL)
-
-    state_dim = env.observation_space.shape[0]
-    action_dim = env.action_space.shape[0] // 2
-    max_action = float(env.action_space.high.min())
-
-    max_episodes = 15000
-    updates_per_step = 1
-    warmup_steps = 50000
-    max_episode_steps = 200
-
-    new_config = {
-        "critic_lr": 1e-5,
-        "actor_lr": 1e-5,
-        "critic_dim": 256,
-        "actor_dim": 256,
-        "actor_n_layers": 2,
-        "critic_n_layers": 2,
-        "batch_size": 256,
-        "discount": 0.999,
-        "action_min": float(-max_action),
-        "action_max": float(max_action),
-        "policy_update_delay": 2,
-        "tau": 0.01,
-        "noise_type": "normal",
-        "exploration_noise": 0.15,
-        "policy_noise": 0.1,
-        "noise_clip": 0.5,
-        "target_network_update_steps": 1,
-        "verbose": True,
-        "prioritized_replay": False,
-    }
-
-    # agent = SAC(o_space.shape[0], action_dim=ac_space.shape[0], noise='pink', max_episode_steps=max_episode_steps)
-    agent = TD3(state_dim, action_dim=action_dim, **new_config)
-
-    critic_losses = []
-    actor_losses = []
-    rewards = []
-    steps = 0
-    gradient_steps = 0
-    evaluation_mean_rewards_strong = []
-    evaluation_winrates_strong = []
-
-    evaluation_mean_rewards_weak = []
-    evaluation_winrates_weak = []
-
-    # Evaluate untrained policy
-    print("Evaluating untrained policy")
-    weak_opponent_winrate, weak_opponentevaluation_mean_reward = evaluate_policy(
-        agent, easy=True, num_eval_rounds=10
-    )
-    strong_opponent_winrate, strong_evaluation_mean_reward = evaluate_policy(
-        agent, easy=False, num_eval_rounds=10
-    )
-
-    strong_opponent = h_env.BasicOpponent(weak=False)
-    weak_opponent = h_env.BasicOpponent(weak=True)
-    opponent_pool = [strong_opponent, weak_opponent]
-
-    pbar = tqdm(range(max_episodes), desc="TRAIN BABY TRAIN")
-    for i in pbar:
-        total_reward = 0
-        state, info = env.reset()
-        player2_state = env.obs_agent_two()
-        done = False
-
-        agent.on_episode_start(i)
-
-        episode_opponent = random.choice(opponent_pool)
-
-        for t in range(max_episode_steps):
-            if steps < warmup_steps:
-                # Use Strong Opponent to generate transitions
-
-                # Player 1 (warm up player, the one we are trying to learn from, aka the agent state)
-                action = env.action_space.sample()[:action_dim]
-
-                # Player 2, uses state 2
-                action2 = strong_opponent.act(np.array(player2_state))
-            else:
-                action = agent.act(state)  # Agent's action
-
-                # Player 2 action
-                action2 = episode_opponent.act(np.array(player2_state))
-
-            (next_state, reward, done, trunc, _) = env.step(
-                np.hstack([action, action2])
-            )
-            agent.store_transition((state, action, reward, next_state, done))
-
-            state = next_state
-            player2_state = env.obs_agent_two()  # Update player 2 state
-
-            steps += 1
-            total_reward += reward
-
-            if steps >= warmup_steps:
-                stats = agent.train(updates_per_step)
-
-                gradient_steps += updates_per_step
-                critic_losses.extend(stats["critic_loss"])
-                actor_losses.extend(stats["actor_loss"])
-
-            if done or trunc:
-                break
-
-        agent.on_episode_end(i)
-        rewards.append(total_reward)
-
-        if (i + 1) % 100 == 0 and steps >= warmup_steps:
-            weak_opponent_winrate, weak_opponent_evaluation_mean_reward = (
-                evaluate_policy(agent, easy=True, num_eval_rounds=10)
-            )
-            evaluation_mean_rewards_weak.append(weak_opponent_evaluation_mean_reward)
-            evaluation_winrates_weak.append(weak_opponent_winrate)
-
-            strong_winrate, strong_evaluation_mean_reward = evaluate_policy(
-                agent, easy=False, num_eval_rounds=10
-            )
-            evaluation_mean_rewards_strong.append(strong_evaluation_mean_reward)
-            evaluation_winrates_strong.append(strong_winrate)
-            agent.save(os.path.join(run_dir, "model.pt"))
-
-        pbar.set_postfix(
-            {
-                "total_reward": total_reward,
-                "episode_length": t,
-            }
-        )
-
-    agent.save(os.path.join(run_dir, "model.pt"))
-
-    fig, ax = plt.subplots()
-    ax.plot(moving_average(rewards, 10))
-    ax.set_xlabel("Episodes")
-    ax.set_ylabel("Total Reward")
-    ax.set_title("Total Reward per Episode")
-    fig.savefig(os.path.join(run_dir, "reward_plot.png"))
-    plt.close(fig)
-
-    fig, ax = plt.subplots()
-    ax.plot(moving_average(critic_losses, 100))
-    ax.set_xlabel("Training Steps")
-    ax.set_ylabel("Critic Loss")
-    ax.set_title("Critic Loss over Time")
-    fig.savefig(os.path.join(run_dir, "critic_loss_plot.png"))
-    plt.close(fig)
-
-    fig, ax = plt.subplots()
-    ax.plot(moving_average(actor_losses, 100))
-    ax.set_xlabel("Training Steps")
-    ax.set_ylabel("Actor Loss")
-    ax.set_title("Actor Loss over Time")
-    fig.savefig(os.path.join(run_dir, "actor_loss_plot.png"))
-    plt.close(fig)
-
-    fig, ax = plt.subplots()
-    episode_indices = np.arange(1, len(evaluation_mean_rewards_weak) + 1) * 100
-    ax.plot(episode_indices, evaluation_mean_rewards_weak, label="Weak")
-    ax.plot(episode_indices, evaluation_mean_rewards_strong, label="Strong")
-    ax.set_xlabel("Evaluation Number")
-    ax.set_ylabel("Mean Reward")
-    ax.set_title("Evaluation Mean Reward over Time")
-    ax.legend()
-    fig.savefig(os.path.join(run_dir, "evaluation_mean_reward_plot.png"))
-    plt.close(fig)
-
-    fig, ax = plt.subplots()
-    episode_indices = np.arange(1, len(evaluation_winrates_weak) + 1) * 100
-    ax.plot(episode_indices, evaluation_winrates_weak, label="Weak")
-    ax.plot(episode_indices, evaluation_winrates_strong, label="Strong")
-    ax.set_xlabel("Evaluation Number")
-    ax.set_ylabel("Win Rate")
-    ax.set_title("Evaluation Win Rate over Time")
-    ax.legend()
-    fig.savefig(os.path.join(run_dir, "evaluation_winrate_plot.png"))
-    plt.close(fig)
diff --git a/src/rl_hockey/td3/__init__.py b/src/rl_hockey/td3/__init__.py
deleted file mode 100644
index 4379649..0000000
--- a/src/rl_hockey/td3/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from rl_hockey.td3.td3 import TD3
diff --git a/src/rl_hockey/td3/models.py b/src/rl_hockey/td3/models.py
deleted file mode 100644
index b2cb2bb..0000000
--- a/src/rl_hockey/td3/models.py
+++ /dev/null
@@ -1,57 +0,0 @@
-import torch
-import torch.nn as nn
-
-
-class Actor(nn.Module):
-    def __init__(self, state_dim, action_dim, latent_dim=[256, 256], activation=nn.ReLU, max_action=1.0):
-        super(Actor, self).__init__()
-
-        self.net = nn.Sequential()
-        dim = state_dim
-        for ld in latent_dim:
-            self.net.append(nn.Linear(dim, ld))
-            self.net.append(activation())
-            dim = ld
-        self.net.append(nn.Linear(dim, action_dim))
-
-        self.max_action = max_action
-
-    def forward(self, state):
-        a = self.net(state)
-        return self.max_action * torch.tanh(a)
-
-
-class Critic(nn.Module):
-    def __init__(self, state_dim, action_dim, latent_dim=[256, 256], activation=nn.ReLU):
-        super(Critic, self).__init__()
-
-        # Q1 architecture
-        self.q1_net = nn.Sequential()
-        dim = state_dim + action_dim
-        for ld in latent_dim:
-            self.q1_net.append(nn.Linear(dim, ld))
-            self.q1_net.append(activation())
-            dim = ld
-        self.q1_net.append(nn.Linear(dim, 1))
-
-        # Q2 architecture
-        self.q2_net = nn.Sequential()
-        dim = state_dim + action_dim
-        for ld in latent_dim:
-            self.q2_net.append(nn.Linear(dim, ld))
-            self.q2_net.append(activation())
-            dim = ld
-        self.q2_net.append(nn.Linear(dim, 1))
-
-    def forward(self, state, action):
-        sa = torch.cat([state, action], 1)
-
-        q1 = self.q1_net(sa)
-        q2 = self.q2_net(sa)
-        return q1, q2
-
-    def Q1(self, state, action):
-        sa = torch.cat([state, action], 1)
-
-        q1 = self.q1_net(sa)
-        return q1
\ No newline at end of file
diff --git a/src/rl_hockey/td3/td3.py b/src/rl_hockey/td3/td3.py
deleted file mode 100644
index 5691da6..0000000
--- a/src/rl_hockey/td3/td3.py
+++ /dev/null
@@ -1,219 +0,0 @@
-import copy
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from rl_hockey.common import *
-from rl_hockey.td3.models import Actor, Critic
-import os
-
-
-DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-
-class TD3(Agent):
-    def __init__(
-        self,
-        state_dim,
-        action_dim,
-        **user_config
-    ):
-
-        self.config = {
-            "learning_rate": 3e-4,
-            "max_action": 1.0,
-            "discount": 0.99,
-            "tau": 0.005,
-            "expl_noise": 0.1,
-            "policy_noise": 0.2,
-            "noise_clip": 0.5,
-            "policy_freq": 2,
-            "batch_size": 256,
-            "latent_dim": [256, 256],
-            "activation": nn.ReLU,
-            "priority_replay": False,
-            "normalize_obs": False
-        }
-        self.config.update(user_config)
-        super().__init__(priority_replay=self.config["priority_replay"], normalize_obs=self.config["normalize_obs"])
-
-        self.actor = Actor(state_dim, action_dim, self.config["latent_dim"], self.config["activation"], self.config["max_action"]).to(DEVICE)
-        self.actor_target = copy.deepcopy(self.actor)
-        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.config["learning_rate"])
-
-        self.critic = Critic(state_dim, action_dim, self.config["latent_dim"], self.config["activation"]).to(DEVICE)
-        self.critic_target = copy.deepcopy(self.critic)
-        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.config["learning_rate"])
-
-        self.action_dim = action_dim
-        self.state_dim = state_dim
-        self.max_action = self.config["max_action"]
-        self.discount = self.config["discount"]
-        self.tau = self.config["tau"]
-        self.expl_noise = self.config["expl_noise"]
-        self.policy_noise = self.config["policy_noise"]
-        self.noise_clip = self.config["noise_clip"]
-        self.policy_freq = self.config["policy_freq"]
-        self.batch_size = self.config["batch_size"]
-        self.priority_replay = self.config["priority_replay"]
-
-        self.total_it = 0
-
-    def act(self, state, deterministic=False):
-        with torch.no_grad():
-            state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
-            action = self.actor(state).squeeze(0).cpu().numpy()
-            if not deterministic:
-                action += np.random.normal(
-                    0, self.max_action * self.expl_noise, size=self.action_dim
-                ).clip(-self.max_action, self.max_action)
-
-            return action
-        
-    def act_batch(self, states, deterministic=False):
-        """Process a batch of states at once (for vectorized environments)"""
-        with torch.no_grad():
-            states = torch.from_numpy(states).to(DEVICE)
-            batch_size = states.shape[0]
-            action = self.actor(states).squeeze(0).cpu().numpy()
-
-            if not deterministic:
-                action += np.random.normal(
-                    0, self.max_action * self.expl_noise, size=(batch_size, self.action_dim)
-                ).clip(-self.max_action, self.max_action)
-
-            return action
-
-
-    def evaluate(self, state):
-        with torch.no_grad():
-            state = torch.from_numpy(state).unsqueeze(0).to(DEVICE)
-
-            # 1. Get the deterministic action (No sampling, no log_prob)
-            action = self.actor(state)
-
-            # 2. Get the Q-values from both critics
-            q1, q2 = self.critic(state, action)
-
-            # 3. Take the conservative estimate (min)
-            value = torch.min(q1, q2)
-            return value.squeeze().item()
-
-
-    def train(self, steps):
-        critic_losses = []
-        actor_losses = []
-    
-        self.total_it += 1
-
-        # Sample replay buffer
-        if self.priority_replay:
-            (state, action, reward, next_state, done), tree_indices, importance_weights = self.buffer.sample(self.batch_size)
-            importance_weights = torch.from_numpy(importance_weights).to(dtype=torch.float32, device=DEVICE).unsqueeze(-1)
-        else:
-            state, action, reward, next_state, done = self.buffer.sample(self.batch_size)
-
-        state = torch.from_numpy(state).to(dtype=torch.float32, device=DEVICE)
-        action = torch.from_numpy(action).to(dtype=torch.float32, device=DEVICE)
-        reward = torch.from_numpy(reward).to(dtype=torch.float32, device=DEVICE)
-        next_state = torch.from_numpy(next_state).to(
-            dtype=torch.float32, device=DEVICE
-        )
-        done = torch.from_numpy(done).to(dtype=torch.float32, device=DEVICE)
-
-        with torch.no_grad():
-            next_action = self.actor_target(next_state)
-
-            # Compute the target Q value
-            target_Q1, target_Q2 = self.critic_target(next_state, next_action)
-            target_Q = torch.min(target_Q1, target_Q2)
-            target_Q = reward + (1-done) * self.discount * target_Q
-
-        # Get current Q estimates
-        current_Q1, current_Q2 = self.critic(state, action)
-
-        if self.priority_replay:
-            td_errors_q1 = torch.abs(current_Q1 - target_Q).detach().cpu().numpy()
-            td_errors_q2 = torch.abs(current_Q2 - target_Q).detach().cpu().numpy()
-            td_errors = td_errors_q1 + td_errors_q2
-
-            for i in range(self.batch_size):
-                idx = tree_indices[i]
-                self.buffer.update(idx, td_errors[i])
-
-            critic_loss = F.mse_loss(current_Q1, target_Q, reduction='none') + \
-                          F.mse_loss(current_Q2, target_Q, reduction='none')
-            critic_loss = (importance_weights * critic_loss).mean()
-
-        else:
-            # Compute critic loss
-            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(
-                current_Q2, target_Q
-            )
-    
-        critic_losses.append(critic_loss.item())
-
-        # Optimize the critic
-        self.critic_optimizer.zero_grad()
-        critic_loss.backward()
-        self.critic_optimizer.step()
-
-        # Delayed policy updates
-        if self.total_it % self.policy_freq == 0:
-
-            # Compute actor loss
-            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()
-            actor_losses.append(actor_loss.item())
-
-            # Optimize the actor
-            self.actor_optimizer.zero_grad()
-            actor_loss.backward()
-            self.actor_optimizer.step()
-
-            # Update the frozen target models
-            for param, target_param in zip(
-                self.critic.parameters(), self.critic_target.parameters()
-            ):
-                target_param.data.copy_(
-                    self.tau * param.data + (1 - self.tau) * target_param.data
-                )
-
-            for param, target_param in zip(
-                self.actor.parameters(), self.actor_target.parameters()
-            ):
-                target_param.data.copy_(
-                    self.tau * param.data + (1 - self.tau) * target_param.data
-                )
-
-        return {"critic_loss": critic_losses, "actor_loss": actor_losses}
-
-    def save(self, filepath):
-        if not os.path.exists(os.path.dirname(filepath)):
-            os.makedirs(os.path.dirname(filepath))
-
-        if not filepath.endswith(".pt"):
-            filepath += ".pt"
-
-        checkpoint = {
-            "actor": self.actor.state_dict(),
-            "actor_target": self.actor_target.state_dict(),
-            "critic": self.critic.state_dict(),
-            "critic_target": self.critic_target.state_dict(),
-            "actor_optimizer": self.actor_optimizer.state_dict(),
-            "critic_optimizer": self.critic_optimizer.state_dict(),
-        }
-
-        torch.save(checkpoint, filepath)
-
-    def load(self, filepath):
-        checkpoint = torch.load(filepath, map_location=DEVICE)
-
-        self.actor.load_state_dict(checkpoint["actor"])
-        self.actor_target.load_state_dict(checkpoint["actor_target"])
-        self.critic.load_state_dict(checkpoint["critic"])
-        self.critic_target.load_state_dict(checkpoint["critic_target"])
-        self.critic_optimizer.load_state_dict(checkpoint["critic_optimizer"])
-        self.actor_optimizer.load_state_dict(checkpoint["actor_optimizer"])
-
-    def on_episode_start(self, episode):
-        pass
\ No newline at end of file
diff --git a/resources/cluster/niklas/sync_to_cluster.sh b/sync_to_cluster.sh
old mode 100644
new mode 100755
similarity index 76%
rename from resources/cluster/niklas/sync_to_cluster.sh
rename to sync_to_cluster.sh
index a4609b8..11d639b
--- a/resources/cluster/niklas/sync_to_cluster.sh
+++ b/sync_to_cluster.sh
@@ -5,8 +5,7 @@
 SERVER="tcml-login1"
 # SSH config should handle the full hostname mapping
 REMOTE_DIR="~/RL_CheungMaenzerAbraham_Hockey"
-# Get project root directory (three level up from resources folder where this script is located)
-LOCAL_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/../../.." && pwd)"
+LOCAL_DIR="$(pwd)"
 
 echo "Syncing project to ${SERVER}:${REMOTE_DIR}..."
 echo ""
@@ -25,8 +24,6 @@ rsync -avz --progress \
     --exclude='*.out' \
     --exclude='*.err' \
     --exclude='models/' \
-    --exclude='runs/' \
-    --exclude='results/' \
     --exclude='singularity_build/' \
     --exclude='old/' \
     --exclude='.conda' \
@@ -41,8 +38,8 @@ if [ $? -eq 0 ]; then
     echo "Next steps:"
     echo "1. SSH to the cluster: ssh ${SERVER}"
     echo "2. Navigate to: cd ${REMOTE_DIR}"
-    echo "3. Run setup: bash resources/cluster_setup.sh"
-    echo "4. Submit the job: sbatch resources/train_single_run.sbatch"
+    echo "3. Run setup: bash cluster_setup.sh"
+    echo "4. Submit the job: sbatch train_single_run.sbatch"
 else
     echo ""
     echo "Sync failed. Please check the error messages above."
diff --git a/test/tdmpc2_test.py b/test/tdmpc2_test.py
deleted file mode 100644
index 7a466f4..0000000
--- a/test/tdmpc2_test.py
+++ /dev/null
@@ -1,100 +0,0 @@
-"""
-Comprehensive test suite for TD-MPC2 implementation
-Because the paper is very fucking complicated and I want to make sure I'm not doing anything wrong
-Checking all the stuff i atleast can check, mostly dim and shape of the tensors
-
-
-Niklas
-"""
-
-import sys
-import unittest
-
-
-class TestUtils(unittest.TestCase):
-    """Test the utility functions."""
-
-    def test_simnorm(self):
-        """Test the SimNorm layer."""
-        import torch
-
-        from rl_hockey.TD_MPC2.util import SimNorm
-
-        batch_size = 1
-        feature_dim = 10
-        simplex_dim = 5
-
-        simnorm = SimNorm(feature_dim, simplex_dim)
-
-        x = torch.randn(batch_size, feature_dim)
-        print(f"Input shape: {x.shape}")
-        print(f"Input: {x}")
-        y = simnorm(x)
-        print(f"Output shape: {y.shape}")
-        print(f"Output: {y}")
-        self.assertEqual(y.shape, (batch_size, feature_dim))
-
-    def test_fail_simnorm(self):
-        """Test the SimNorm layer with invalid group reshape input."""
-        import torch
-
-        from rl_hockey.TD_MPC2.util import SimNorm
-
-        batch_size = 1
-        feature_dim = 10
-        simplex_dim = 6
-
-        simnorm = SimNorm(feature_dim, simplex_dim)
-
-        x = torch.randn(batch_size, feature_dim)
-        print(f"Input shape: {x.shape}")
-        print(f"Input: {x}")
-
-        with self.assertRaises(ValueError):
-            _ = simnorm(x)
-
-
-def run_all_tests():
-    """Run all test suites and print a summary."""
-    loader = unittest.TestLoader()
-    suite = unittest.TestSuite()
-
-    # Add all test classes
-    test_classes = [
-        TestUtils,
-    ]
-
-    for test_class in test_classes:
-        tests = loader.loadTestsFromTestCase(test_class)
-        suite.addTests(tests)
-
-    runner = unittest.TextTestRunner(verbosity=2)
-    result = runner.run(suite)
-
-    print("\n" + "=" * 60)
-    print("TEST SUMMARY")
-    print("=" * 60)
-    print(f"Tests run: {result.testsRun}")
-    print(f"Failures: {len(result.failures)}")
-    print(f"Errors: {len(result.errors)}")
-    print(
-        f"Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%"
-    )
-    print("=" * 60)
-
-    if result.wasSuccessful():
-        print("\n✓ All tests passed! Implementation tested and verified.")
-    else:
-        print("\n✗ Some tests failed. Please check the output above for details.")
-
-    return result.wasSuccessful()
-
-
-if __name__ == "__main__":
-    print("=" * 60)
-    print("TD-MPC2 Implementation Test")
-    print("=" * 60)
-    print()
-
-    success = run_all_tests()
-    sys.exit(0 if success else 1)
diff --git a/resources/cluster/niklas/test_sbatch.sh b/test_sbatch.sh
old mode 100644
new mode 100755
similarity index 89%
rename from resources/cluster/niklas/test_sbatch.sh
rename to test_sbatch.sh
index cf66917..8bfb4cb
--- a/resources/cluster/niklas/test_sbatch.sh
+++ b/test_sbatch.sh
@@ -18,7 +18,7 @@ echo "✓ Project directory exists: $PROJECT_DIR"
 CONTAINER="$PROJECT_DIR/singularity_build/rl_hockey.simg"
 if [ ! -f "$CONTAINER" ]; then
     echo "WARNING: Container not found: $CONTAINER"
-    echo "  You need to build the container first: bash resources/cluster_setup.sh"
+    echo "  You need to build the container first: bash cluster_setup.sh"
 else
     echo "✓ Container exists: $CONTAINER"
 fi
@@ -42,7 +42,7 @@ echo "✓ Training script exists: $SCRIPT"
 # Display resource requirements from sbatch file
 echo ""
 echo "=== Resource Requirements ==="
-grep "^#SBATCH --" "$PROJECT_DIR/resources/train_single_run.sbatch" | grep -v "^#.*$" | while read line; do
+grep "^#SBATCH --" "$PROJECT_DIR/train_single_run.sbatch" | grep -v "^#.*$" | while read line; do
     echo "  $line"
 done
 
@@ -75,4 +75,4 @@ echo "  Parallel Environments: 24 (via NUM_ENVS)"
 echo ""
 echo "To submit the job, run:"
 echo "  cd $PROJECT_DIR"
-echo "  sbatch resources/train_single_run.sbatch"
+echo "  sbatch train_single_run.sbatch"
diff --git a/resources/cluster/jannik/train_single_run.sbatch b/train_single_run.sbatch
similarity index 96%
rename from resources/cluster/jannik/train_single_run.sbatch
rename to train_single_run.sbatch
index d2b8bbd..bca20a3 100644
--- a/resources/cluster/jannik/train_single_run.sbatch
+++ b/train_single_run.sbatch
@@ -24,7 +24,7 @@
 #SBATCH --mail-type=ALL
 # Email notifications: NONE, BEGIN, END, FAIL, REQUEUE, ALL
 
-#SBATCH --mail-user=jannik.maenzer@student.uni-tuebingen.de
+#SBATCH --mail-user=niklas-sebastian.abraham@student.uni-tuebingen.de
 # Your email address
 
 # Path to the Singularity container image
diff --git a/resources/cluster/niklas/verify_container.sh b/verify_container.sh
old mode 100644
new mode 100755
similarity index 100%
rename from resources/cluster/niklas/verify_container.sh
rename to verify_container.sh







































































[2026-01-27 13:43:02] [INFO] Episode 553: reward=8.12, shaped_reward=8.12, backprop_reward=8.12 | env=NORMAL | opponent=weighted_mixture | actor_loss=-6.908811 | critic_loss=0.546974 | grad_norm_critic=3.366219
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 5, Episode 553, step 45:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87269
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87270] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87270] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 6, Episode 553, step 87:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87270
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87271] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87271] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 7, Episode 553, step 72:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.145278
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.145278
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.072639
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.07263915240764618
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87271
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.07263915240764618
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87272] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87272] value=[-0.07263915]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 8, Episode 553, step 162:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.104937
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.104937
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.052468
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.05246840417385101
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87272
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.05246840417385101
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87273] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87273] value=[-0.0524684]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 9, Episode 553, step 58:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.125099
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.125099
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.062550
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.0625496432185173
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87273
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.0625496432185173
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87274] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87274] value=[-0.06254964]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 10, Episode 553, step 29:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.164815
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.164815
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.082408
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.08240751922130585
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87274
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.08240751922130585
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87275] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87275] value=[-0.08240752]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 11, Episode 553, step 109:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.039239
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.039239
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.019619
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.019619356840848923
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87275
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.019619356840848923
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87276] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87276] value=[-0.01961936]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 12, Episode 553, step 87:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.112950
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.112950
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.056475
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.05647515878081322
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87276
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.05647515878081322
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87277] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87277] value=[-0.05647516]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 13, Episode 553, step 97:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.037137
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.037137
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.018569
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.018568560481071472
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87277
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.018568560481071472
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87278] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87278] value=[-0.01856856]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 14, Episode 553, step 41:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.190598
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.190598
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.095299
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.09529893845319748
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87278
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.09529893845319748
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87279] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87279] value=[-0.09529894]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 15, Episode 553, step 142:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.070121
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.070121
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.035061
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.03506074473261833
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87279
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.03506074473261833
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87280] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87280] value=[-0.03506074]
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] batch_size=256, buffer.size=87280, use_torch_tensors=False
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Numpy mode - before indexing:
[2026-01-27 13:43:02] [INFO]   idx: shape=(256,), dtype=int64
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]   self.reward[0:5] values=[0.0, 0.0, 0.0, 0.0, 0.0]
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Numpy mode - after indexing:
[2026-01-27 13:43:02] [INFO]   state: shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: shape=(256, 4), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]     reward min=-5.064333, max=0.000000, mean=-0.063850
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[-0.10281115025281906, 0.0, -0.035246506333351135, -0.056651435792446136, -0.0411444716155529]
[2026-01-27 13:43:02] [INFO]   next_state: shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Final return values:
[2026-01-27 13:43:02] [INFO]   reward: shape=(256, 1), type=<class 'numpy.ndarray'>
[2026-01-27 13:43:02] [INFO]     reward stats: min=-5.064333, max=0.000000, mean=-0.063850
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] After buffer.sample():
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(256, 4), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.ndarray'>, shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]     reward min=-5.064333438873291, max=0.0, mean=-0.06385042518377304
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[[-0.10281115]
 [ 0.        ]
 [-0.03524651]
 [-0.05665144]
 [-0.04114447]]
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.ndarray'>, shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] After torch.from_numpy():
[2026-01-27 13:43:02] [INFO]   state: shape=torch.Size([256, 18]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]   action: shape=torch.Size([256, 4]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]   reward: shape=torch.Size([256, 1]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]     reward min=-5.064333, max=0.000000, mean=-0.063850
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[-0.10281115025281906, 0.0, -0.035246506333351135, -0.056651435792446136, -0.0411444716155529]
[2026-01-27 13:43:02] [INFO]   next_state: shape=torch.Size([256, 18]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]   done: shape=torch.Size([256, 1]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] Target calculation:
[2026-01-27 13:43:02] [INFO]   next_action: shape=torch.Size([256, 4]), dtype=torch.float32
[2026-01-27 13:43:02] [INFO]   next_log_prob: shape=torch.Size([256, 1]), dtype=torch.float32
[2026-01-27 13:43:02] [INFO]   q1: shape=torch.Size([256, 1]), dtype=torch.float32, min=2.490441, max=4.819449
[2026-01-27 13:43:02] [INFO]   q2: shape=torch.Size([256, 1]), dtype=torch.float32, min=2.390443, max=4.782768
[2026-01-27 13:43:02] [INFO]   next_value: shape=torch.Size([256, 1]), dtype=torch.float32, min=3.936468, max=6.619308
[2026-01-27 13:43:02] [INFO]   target: shape=torch.Size([256, 1]), dtype=torch.float32, min=-5.064333, max=6.553115, mean=5.231160
[2026-01-27 13:43:02] [INFO]     target[0:5]=[5.111048698425293, 5.734161853790283, 5.836941719055176, 5.737536907196045, 5.4881086349487305]
[2026-01-27 13:43:02] [INFO]     reward component: min=-5.064333, max=0.000000, mean=-0.063850
[2026-01-27 13:43:02] [INFO]     (1-done) component: min=0.000000, max=1.000000, mean=0.992188
[2026-01-27 13:43:02] [INFO]     discount=0.99, next_value component: min=3.897104, max=6.553115
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 0, Episode 553, step 145:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87280
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87281] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87281] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 1, Episode 553, step 54:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.151538
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.151538
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.075769
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.07576897740364075
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87281
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.07576897740364075
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87282] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87282] value=[-0.07576898]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 2, Episode 553, step 219:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87282
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87283] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87283] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 3, Episode 553, step 6:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87283
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87284] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87284] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 4, Episode 553, step 0:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87284
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87285] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87285] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 5, Episode 553, step 46:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87285
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87286] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87286] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 6, Episode 553, step 88:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87286
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87287] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87287] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 7, Episode 553, step 73:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.125593
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.125593
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.062797
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.06279654055833817
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87287
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.06279654055833817
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87288] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87288] value=[-0.06279654]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 8, Episode 553, step 163:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.100820
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.100820
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.050410
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.05040981248021126
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87288
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.05040981248021126
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87289] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87289] value=[-0.05040981]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 9, Episode 553, step 59:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.123857
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.123857
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.061928
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.061928410083055496
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87289
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.061928410083055496
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87290] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87290] value=[-0.06192841]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 10, Episode 553, step 30:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.159233
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.159233
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.079616
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.07961631566286087
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87290
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.07961631566286087
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87291] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87291] value=[-0.07961632]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 11, Episode 553, step 110:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.035507
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.035507
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.017754
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.01775369979441166
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87291
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.01775369979441166
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87292] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87292] value=[-0.0177537]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 12, Episode 553, step 88:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.111377
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.111377
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.055689
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.05568855628371239
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87292
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.05568855628371239
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87293] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87293] value=[-0.05568856]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 13, Episode 553, step 98:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.036391
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.036391
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.018195
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.0181952565908432
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87293
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.0181952565908432
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87294] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87294] value=[-0.01819526]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 14, Episode 553, step 42:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.190691
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.190691
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.095346
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.09534561634063721
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87294
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.09534561634063721
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87295] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87295] value=[-0.09534562]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 15, Episode 553, step 143:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.065277
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.065277
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.032638
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.03263835236430168
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87295
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.03263835236430168
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87296] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87296] value=[-0.03263835]
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] batch_size=256, buffer.size=87296, use_torch_tensors=False
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Numpy mode - before indexing:
[2026-01-27 13:43:02] [INFO]   idx: shape=(256,), dtype=int64
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]   self.reward[0:5] values=[0.0, 0.0, 0.0, 0.0, 0.0]
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Numpy mode - after indexing:
[2026-01-27 13:43:02] [INFO]   state: shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: shape=(256, 4), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]     reward min=-5.077090, max=0.000000, mean=-0.057333
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[-0.009068974293768406, -0.048002902418375015, -0.07639171183109283, -0.06632957607507706, -0.010889080353081226]
[2026-01-27 13:43:02] [INFO]   next_state: shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Final return values:
[2026-01-27 13:43:02] [INFO]   reward: shape=(256, 1), type=<class 'numpy.ndarray'>
[2026-01-27 13:43:02] [INFO]     reward stats: min=-5.077090, max=0.000000, mean=-0.057333
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] After buffer.sample():
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(256, 4), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.ndarray'>, shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]     reward min=-5.077089786529541, max=0.0, mean=-0.05733261629939079
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[[-0.00906897]
 [-0.0480029 ]
 [-0.07639171]
 [-0.06632958]
 [-0.01088908]]
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.ndarray'>, shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] After torch.from_numpy():
[2026-01-27 13:43:02] [INFO]   state: shape=torch.Size([256, 18]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]   action: shape=torch.Size([256, 4]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]   reward: shape=torch.Size([256, 1]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]     reward min=-5.077090, max=0.000000, mean=-0.057333
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[-0.009068974293768406, -0.048002902418375015, -0.07639171183109283, -0.06632957607507706, -0.010889080353081226]
[2026-01-27 13:43:02] [INFO]   next_state: shape=torch.Size([256, 18]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO]   done: shape=torch.Size([256, 1]), dtype=torch.float32, device=cuda:0
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] Target calculation:
[2026-01-27 13:43:02] [INFO]   next_action: shape=torch.Size([256, 4]), dtype=torch.float32
[2026-01-27 13:43:02] [INFO]   next_log_prob: shape=torch.Size([256, 1]), dtype=torch.float32
[2026-01-27 13:43:02] [INFO]   q1: shape=torch.Size([256, 1]), dtype=torch.float32, min=2.392688, max=5.163049
[2026-01-27 13:43:02] [INFO]   q2: shape=torch.Size([256, 1]), dtype=torch.float32, min=2.259016, max=5.453281
[2026-01-27 13:43:02] [INFO]   next_value: shape=torch.Size([256, 1]), dtype=torch.float32, min=3.826872, max=7.036590
[2026-01-27 13:43:02] [INFO]   target: shape=torch.Size([256, 1]), dtype=torch.float32, min=-5.077090, max=6.966224, mean=5.193127
[2026-01-27 13:43:02] [INFO]     target[0:5]=[5.871426105499268, 5.1086039543151855, 5.31938362121582, 5.198299884796143, 5.157888889312744]
[2026-01-27 13:43:02] [INFO]     reward component: min=-5.077090, max=0.000000, mean=-0.057333
[2026-01-27 13:43:02] [INFO]     (1-done) component: min=0.000000, max=1.000000, mean=0.996094
[2026-01-27 13:43:02] [INFO]     discount=0.99, next_value component: min=3.788604, max=6.966224
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 0, Episode 553, step 146:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87296
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87297] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87297] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 1, Episode 553, step 55:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87297
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87298] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87298] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 2, Episode 553, step 220:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87298
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87299] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87299] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 3, Episode 553, step 7:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87299
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87300] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87300] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 4, Episode 553, step 1:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87300
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87301] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87301] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 5, Episode 553, step 47:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87301
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87302] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87302] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 6, Episode 553, step 89:
[2026-01-27 13:43:02] [INFO]   raw reward from env: 0.000000
[2026-01-27 13:43:02] [INFO]   shaped_reward: 0.000000
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): 0.000000
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=0.0
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87302
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=0.0
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87303] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87303] value=[0.]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 7, Episode 553, step 74:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.106605
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.106605
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.053302
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.053302377462387085
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87303
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.053302377462387085
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87304] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87304] value=[-0.05330238]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 8, Episode 553, step 164:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.096844
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.096844
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.048422
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.048421867191791534
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87304
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.048421867191791534
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87305] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87305] value=[-0.04842187]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 9, Episode 553, step 60:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.123517
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.123517
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.061758
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.061758484691381454
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87305
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.061758484691381454
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87306] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87306] value=[-0.06175848]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 10, Episode 553, step 31:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.153571
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.153571
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.076785
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.0767853781580925
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87306
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.0767853781580925
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87307] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87307] value=[-0.07678538]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 11, Episode 553, step 111:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.031534
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.031534
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.015767
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.015766968950629234
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87307
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.015766968950629234
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87308] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87308] value=[-0.01576697]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 12, Episode 553, step 89:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.109482
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.109482
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.054741
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.05474085733294487
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87308
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.05474085733294487
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87309] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87309] value=[-0.05474086]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 13, Episode 553, step 99:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.034894
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.034894
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.017447
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.01744675263762474
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87309
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.01744675263762474
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87310] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87310] value=[-0.01744675]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 14, Episode 553, step 43:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.193313
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.193313
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.096657
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.09665651619434357
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87310
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.09665651619434357
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87311] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87311] value=[-0.09665652]
[2026-01-27 13:43:02] [INFO] [TRAIN REWARD CALC] Env 15, Episode 553, step 144:
[2026-01-27 13:43:02] [INFO]   raw reward from env: -0.059356
[2026-01-27 13:43:02] [INFO]   shaped_reward: -0.059356
[2026-01-27 13:43:02] [INFO]   reward_scale: 0.500000
[2026-01-27 13:43:02] [INFO]   scaled_reward (stored in buffer): -0.029678
[2026-01-27 13:43:02] [INFO]   done: False, winner: 0
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Input transition:
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(4,), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), dtype=float32, value=-0.02967781573534012
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(18,), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.bool'>, shape=(), dtype=bool, value=False
[2026-01-27 13:43:02] [INFO]   buffer.use_torch_tensors=False, buffer.device=cpu, buffer.size=87311
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] Numpy mode - storing directly:
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.float32'>, shape=(), value=-0.02967781573534012
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), self.reward[87312] shape=(1,)
[2026-01-27 13:43:02] [INFO] [BUFFER STORE] After numpy assignment: self.reward[87312] value=[-0.02967782]
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] batch_size=256, buffer.size=87312, use_torch_tensors=False
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Numpy mode - before indexing:
[2026-01-27 13:43:02] [INFO]   idx: shape=(256,), dtype=int64
[2026-01-27 13:43:02] [INFO]   self.reward shape=(1000000, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]   self.reward[0:5] values=[0.0, 0.0, 0.0, 0.0, 0.0]
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Numpy mode - after indexing:
[2026-01-27 13:43:02] [INFO]   state: shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: shape=(256, 4), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]     reward min=-0.137207, max=0.000000, mean=-0.036132
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[-0.03372897207736969, -0.086960569024086, -0.1287176012992859, -0.09659312665462494, 0.0]
[2026-01-27 13:43:02] [INFO]   next_state: shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO] [BUFFER SAMPLE] Final return values:
[2026-01-27 13:43:02] [INFO]   reward: shape=(256, 1), type=<class 'numpy.ndarray'>
[2026-01-27 13:43:02] [INFO]     reward stats: min=-0.137207, max=0.000000, mean=-0.036132
[2026-01-27 13:43:02] [INFO] [SAC TRAIN STEP 0] After buffer.sample():
[2026-01-27 13:43:02] [INFO]   state: type=<class 'numpy.ndarray'>, shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   action: type=<class 'numpy.ndarray'>, shape=(256, 4), dtype=float32
[2026-01-27 13:43:02] [INFO]   reward: type=<class 'numpy.ndarray'>, shape=(256, 1), dtype=float32
[2026-01-27 13:43:02] [INFO]     reward min=-0.13720709085464478, max=0.0, mean=-0.03613200783729553
[2026-01-27 13:43:02] [INFO]     reward[0:5]=[[-0.03372897]
 [-0.08696057]
 [-0.1287176 ]
 [-0.09659313]
 [ 0.        ]]
[2026-01-27 13:43:02] [INFO]   next_state: type=<class 'numpy.ndarray'>, shape=(256, 18), dtype=float32
[2026-01-27 13:43:02] [INFO]   done: type=<class 'numpy.ndarray'>, shape=(256, 1), dtype=float32
