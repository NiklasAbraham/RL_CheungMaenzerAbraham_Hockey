{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d831ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef02867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rl_hockey.sac import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pendulum-v1'\n",
    "# env_name = 'LunarLanderContinuous-v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "env = gym.wrappers.RescaleAction(env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "o_space = env.observation_space\n",
    "ac_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c848d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100\n",
    "max_episode_steps = 500\n",
    "updates_per_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(o_space.shape[0], action_dim=ac_space.shape[0], noise='pink', max_episode_steps=max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a02ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_losses = []\n",
    "actor_losses = []\n",
    "rewards = []\n",
    "gradient_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(max_episodes), desc=env_name)\n",
    "for i in pbar:    \n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    agent.on_episode_start(i)\n",
    "\n",
    "    for t in range(max_episode_steps):\n",
    "        done = False\n",
    "        action = agent.act(state)\n",
    "        (next_state, reward, done, trunc, _) = env.step(action)\n",
    "        agent.store_transition((state, action, reward, next_state, done))            \n",
    "        state = next_state\n",
    "\n",
    "        stats = agent.train(updates_per_step)\n",
    "\n",
    "        gradient_steps += updates_per_step\n",
    "        total_reward += reward\n",
    "        critic_losses.extend(stats['critic_loss'])\n",
    "        actor_losses.extend(stats['actor_loss'])\n",
    "\n",
    "        if done or trunc:\n",
    "            break\n",
    "\n",
    "    agent.on_episode_end(i)\n",
    "\n",
    "    rewards.append(total_reward)    \n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'total_reward': total_reward\n",
    "    })\n",
    "\n",
    "agent.save(f'../../../models/sac/{env_name}_{gradient_steps//1000}k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    return [sum(data[max(0, i - window_size + 1):i + 1]) / (min(i + 1, window_size)) for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e393d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(rewards, 10))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(critic_losses, 100))\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Critic Loss')\n",
    "plt.title('Critic Loss over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(actor_losses, 100))\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Actor Loss')\n",
    "plt.title('Actor Loss over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode='human')\n",
    "env = gym.wrappers.RescaleAction(env, min_action=-1.0, max_action=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "state, _ = env.reset()\n",
    "for t in range(max_episode_steps):\n",
    "    done = False\n",
    "    action = agent.act(state, deterministic=True)\n",
    "    (next_state, reward, done, trunc, _) = env.step(action)\n",
    "    state = next_state\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    if done or trunc:\n",
    "        break\n",
    "\n",
    "print(f'total_reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-hockey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
